{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6aaabb-7c4e-4656-abe1-734c8b5633fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nequip\n",
    "import wandb\n",
    "import allegro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e55e4c-7e6a-4768-b436-07b8a5d2ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchirag17\u001b[0m (\u001b[33mchirag17-indian-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ec2310-a86b-4623-bd20-33ea0c29d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.size'] = 30\n",
    "\n",
    "def parse_lammps_rdf(rdffile):\n",
    "    \"\"\"Parse the RDF file written by LAMMPS\n",
    "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
    "    \"\"\"\n",
    "    with open(rdffile, 'r') as rdfout:\n",
    "        rdfs = []; buffer = []\n",
    "        for line in rdfout:\n",
    "            values = line.split()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elif len(values) == 2:\n",
    "                nbins = values[1]\n",
    "            else:\n",
    "                buffer.append([float(values[1]), float(values[2])])\n",
    "                if len(buffer) == int(nbins):\n",
    "                    frame = np.transpose(np.array(buffer))\n",
    "                    rdfs.append(frame)\n",
    "                    buffer = []\n",
    "    return rdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b6c2c7-cc84-41d5-b9be-ba41bb0ca0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapi-1000.extxyz\n"
     ]
    }
   ],
   "source": [
    "!ls temp_data1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de28c63-585b-46c3-9a25-c5b317897427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/train.py:180: UserWarning: default_dtype=float32 but we strongly recommend float64\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchirag17\u001b[0m (\u001b[33mchirag17-indian-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/chirag/wandb/run-20241017_100858-XOhzIuKlXbG9fFRpewmXK3DGBQlzCPLVltLvtD1Pp6A\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33matT1000\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial/runs/XOhzIuKlXbG9fFRpewmXK3DGBQlzCPLVltLvtD1Pp6A\u001b[0m\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  datas = [torch.load(d) for d in datas]\n",
      "Loaded data: Batch(atomic_numbers=[48000, 1], batch=[48000], cell=[500, 3, 3], edge_cell_shift=[1206536, 3], edge_index=[2, 1206536], forces=[48000, 3], free_energy=[500], pbc=[500, 3], pos=[48000, 3], ptr=[501], total_energy=[500, 1])\n",
      "    processed data size: ~34.08 MB\n",
      "Cached processed data to disk\n",
      "Done!\n",
      "Successfully loaded the data set of type ASEDataset(500)...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Replace string dataset_per_atom_total_energy_mean to -608.9110717773438\n",
      "Atomic outputs are scaled by: [Pb, I, N, C, H: None], shifted by [Pb, I, N, C, H: -608.911072].\n",
      "Replace string dataset_forces_rms to 1.230722427368164\n",
      "Initially outputs are globally scaled by: 1.230722427368164, total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Equivariance test passed; equivariance errors:\n",
      "   Errors are in real units, where relevant.\n",
      "   Please note that the large scale of the typical\n",
      "   shifts to the (atomic) energy can cause\n",
      "   catastrophic cancellation and give incorrectly\n",
      "   the equivariance error as zero for those fields.\n",
      "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
      "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_embedding             -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_cutoff                -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_features              -> max error=4.321e-07\n",
      "   edge permutation equivariance of field edge_energy                -> max error=1.788e-07\n",
      "   node permutation equivariance of field atomic_energy              -> max error=0.000e+00\n",
      "   edge & node permutation invariance for field total_energy         -> max error=0.000e+00\n",
      "   node permutation equivariance of field forces                     -> max error=2.548e-06\n",
      "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=7.335e-07\n",
      "   (parity_k=0, did_translate=False, field=cell                )     -> max error=2.374e-07\n",
      "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=1.439e-05\n",
      "   (parity_k=0, did_translate=False, field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=9.007e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=1.800e-05\n",
      "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=6.748e-06\n",
      "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=6.104e-05\n",
      "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=forces              )     -> max error=2.900e-05\n",
      "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=9.523e-07\n",
      "   (parity_k=0, did_translate=True , field=cell                )     -> max error=1.972e-07\n",
      "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=1.204e-05\n",
      "   (parity_k=0, did_translate=True , field=edge_cutoff         )     -> max error=7.629e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=5.904e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=1.308e-05\n",
      "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=6.616e-06\n",
      "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=6.104e-05\n",
      "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=forces              )     -> max error=3.820e-05\n",
      "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=7.985e-07\n",
      "   (parity_k=1, did_translate=False, field=cell                )     -> max error=2.794e-07\n",
      "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=1.372e-05\n",
      "   (parity_k=1, did_translate=False, field=edge_cutoff         )     -> max error=7.629e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.955e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=1.377e-05\n",
      "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=4.981e-06\n",
      "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=6.104e-05\n",
      "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=forces              )     -> max error=3.104e-05\n",
      "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=8.768e-07\n",
      "   (parity_k=1, did_translate=True , field=cell                )     -> max error=4.735e-07\n",
      "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=1.085e-05\n",
      "   (parity_k=1, did_translate=True , field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=8.875e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=1.335e-05\n",
      "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=7.391e-06\n",
      "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=6.104e-05\n",
      "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=forces              )     -> max error=3.876e-05\n",
      "Number of weights: 37608\n",
      "Number of trainable weights: 37608\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      0     2        0.786        0.783      0.00377        0.795         1.09         5.93       0.0618\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Initial Validation          0    0.217    0.002        0.774      0.00671        0.781        0.776         1.08         8.72       0.0908\n",
      "Wall time: 0.2178795020000166\n",
      "! Best model        0    0.781\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1    10        0.925        0.923      0.00148        0.742         1.18         4.54       0.0473\n",
      "      1    20        0.416        0.411      0.00539         0.55        0.789         8.67       0.0903\n",
      "      1    30        0.786        0.785     0.000708        0.743         1.09         3.14       0.0328\n",
      "      1    40        0.508        0.503      0.00479        0.568        0.873         8.18       0.0852\n",
      "      1    50        0.508        0.508       0.0007        0.616        0.877         3.12       0.0326\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1     2        0.438        0.436      0.00186        0.579        0.813         4.42        0.046\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               1    3.066    0.002        0.895      0.00377        0.899        0.676         1.16         5.85        0.061\n",
      "! Validation          1    3.066    0.002        0.482      0.00215        0.484        0.581        0.854         5.09       0.0531\n",
      "Wall time: 3.066720299999986\n",
      "! Best model        1    0.484\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      2    10        0.445        0.443      0.00147        0.573        0.819         4.53       0.0472\n",
      "      2    20        0.621        0.621     0.000624        0.618         0.97         2.95       0.0308\n",
      "      2    30        0.586        0.573        0.013        0.603        0.931         13.5        0.141\n",
      "      2    40        0.546        0.541      0.00477        0.561        0.905         8.16        0.085\n",
      "      2    50         1.13         1.12       0.0106        0.672          1.3         12.2        0.127\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      2     2        0.364        0.361      0.00267        0.527         0.74          5.3       0.0552\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               2    5.968    0.002        0.772      0.00585        0.778        0.596         1.08         7.55       0.0787\n",
      "! Validation          2    5.968    0.002        0.423      0.00179        0.425        0.536        0.801         4.39       0.0457\n",
      "Wall time: 5.968851089999987\n",
      "! Best model        2    0.425\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      3    10        0.525        0.524     0.000242        0.569        0.891         1.84       0.0191\n",
      "      3    20        0.509        0.509     0.000202         0.56        0.878         1.68       0.0175\n",
      "      3    30        0.657        0.656      0.00143        0.636        0.997         4.46       0.0465\n",
      "      3    40        0.456        0.455     0.000504        0.497         0.83         2.65       0.0276\n",
      "      3    50        0.515         0.51      0.00483         0.54        0.879         8.21       0.0855\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      3     2        0.299        0.297       0.0023        0.468         0.67         4.96       0.0517\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               3    8.706    0.002        0.698      0.00477        0.703        0.549         1.03         6.29       0.0655\n",
      "! Validation          3    8.706    0.002        0.372      0.00159        0.374        0.487        0.751         4.18       0.0435\n",
      "Wall time: 8.70636646700001\n",
      "! Best model        3    0.374\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      4    10        0.508        0.508     8.61e-05        0.541        0.877          1.1       0.0114\n",
      "      4    20        0.253        0.252      0.00111        0.446        0.618         3.94        0.041\n",
      "      4    30        0.588        0.587      0.00151        0.599        0.943         4.59       0.0478\n",
      "      4    40        0.211        0.206      0.00448        0.399        0.559         7.91       0.0824\n",
      "      4    50         0.43         0.43      5.3e-05        0.464        0.807        0.859      0.00895\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      4     2        0.256        0.253      0.00214        0.427        0.619         4.53       0.0472\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               4   11.490    0.002        0.601      0.00307        0.604        0.499        0.954         5.33       0.0555\n",
      "! Validation          4   11.490    0.002        0.315       0.0012        0.317        0.447        0.691         3.09       0.0321\n",
      "Wall time: 11.49094106299998\n",
      "! Best model        4    0.317\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      5    10        0.336        0.336     0.000239        0.471        0.713         1.82        0.019\n",
      "      5    20        0.366        0.366     7.44e-05        0.468        0.745         1.02       0.0107\n",
      "      5    30        0.369        0.367      0.00201        0.489        0.746         5.29       0.0551\n",
      "      5    40         0.33        0.329      0.00122        0.499        0.706         4.13        0.043\n",
      "      5    50        0.211        0.208      0.00275        0.413        0.561         6.19       0.0645\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      5     2        0.219        0.218      0.00117        0.391        0.574         3.45       0.0359\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               5   14.651    0.002        0.515      0.00384        0.519        0.456        0.883         5.97       0.0622\n",
      "! Validation          5   14.651    0.002        0.269     0.000784         0.27        0.414        0.639         2.76       0.0288\n",
      "Wall time: 14.651826835000008\n",
      "! Best model        5    0.270\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      6    10        0.327        0.326      0.00167        0.461        0.702         4.83       0.0503\n",
      "      6    20        0.351        0.345       0.0057        0.464        0.723         8.92       0.0929\n",
      "      6    30         0.25         0.25     0.000194        0.408        0.615         1.64       0.0171\n",
      "      6    40        0.265        0.264      0.00132        0.412        0.632          4.3       0.0448\n",
      "      6    50        0.238        0.232      0.00647        0.407        0.592          9.5        0.099\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      6     2        0.194        0.193     0.000718        0.367        0.541         2.69       0.0281\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               6   17.567    0.002        0.459      0.00359        0.462        0.439        0.833         5.72       0.0596\n",
      "! Validation          6   17.567    0.002        0.241     0.000531        0.241        0.394        0.604         2.18       0.0228\n",
      "Wall time: 17.567129029\n",
      "! Best model        6    0.241\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      7    10        0.204        0.202       0.0016         0.41        0.553         4.72       0.0492\n",
      "      7    20        0.201        0.201     0.000144        0.378        0.552         1.42       0.0148\n",
      "      7    30         0.22        0.218      0.00249        0.365        0.574          5.9       0.0615\n",
      "      7    40        0.242        0.239      0.00289        0.383        0.602         6.35       0.0662\n",
      "      7    50         0.26        0.258      0.00253        0.417        0.625         5.94       0.0618\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      7     2        0.181        0.181     0.000667        0.353        0.523         2.55       0.0266\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               7   20.469    0.002        0.408       0.0019         0.41        0.421        0.786         4.22       0.0439\n",
      "! Validation          7   20.469    0.002        0.226     0.000465        0.226        0.381        0.585         2.02        0.021\n",
      "Wall time: 20.46999975099999\n",
      "! Best model        7    0.226\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      8    10        0.152         0.15      0.00196        0.351        0.476         5.22       0.0544\n",
      "      8    20        0.137        0.136     0.000939        0.333        0.454         3.62       0.0377\n",
      "      8    30        0.311         0.31      0.00118        0.425        0.685         4.06       0.0423\n",
      "      8    40        0.328        0.327      0.00134        0.452        0.703         4.32        0.045\n",
      "      8    50        0.211         0.21     0.000252        0.387        0.565         1.88       0.0195\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      8     2         0.17         0.17     0.000534        0.344        0.507         2.35       0.0245\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               8   23.131    0.002        0.378      0.00439        0.382        0.411        0.756         6.02       0.0627\n",
      "! Validation          8   23.131    0.002        0.216     0.000424        0.216        0.373        0.571         1.96       0.0205\n",
      "Wall time: 23.131350365000003\n",
      "! Best model        8    0.216\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      9    10        0.129        0.117       0.0123        0.316        0.421         13.1        0.137\n",
      "      9    20         0.26         0.26     0.000225        0.415        0.627         1.77       0.0185\n",
      "      9    30         2.34         2.34      0.00332        0.564         1.88         6.81        0.071\n",
      "      9    40            3            3     0.000407         0.69         2.13         2.39       0.0249\n",
      "      9    50         0.19         0.19     2.34e-05        0.362        0.536         0.57      0.00594\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      9     2        0.162        0.161     0.000495        0.337        0.494         2.31       0.0241\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               9   25.801    0.002        0.348      0.00262         0.35        0.403        0.726         4.79       0.0499\n",
      "! Validation          9   25.801    0.002        0.207     0.000467        0.207        0.366         0.56         2.05       0.0213\n",
      "Wall time: 25.80169986599998\n",
      "! Best model        9    0.207\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     10    10        0.243        0.243     2.78e-06        0.388        0.606        0.195      0.00203\n",
      "     10    20         0.32        0.319      0.00144        0.456        0.695         4.49       0.0468\n",
      "     10    30        0.234         0.23      0.00354          0.4        0.591         7.03       0.0732\n",
      "     10    40        0.206        0.205     0.000305        0.391        0.558         2.06       0.0215\n",
      "     10    50        0.263        0.263     6.76e-05        0.402        0.631        0.973       0.0101\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     10     2        0.156        0.156     0.000565        0.332        0.486         2.46       0.0257\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              10   28.382    0.002        0.322        0.002        0.324        0.399        0.698         4.23       0.0441\n",
      "! Validation         10   28.382    0.002        0.203     0.000463        0.203        0.362        0.554         2.08       0.0217\n",
      "Wall time: 28.382950779999987\n",
      "! Best model       10    0.203\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     11    10        0.204        0.203     0.000723         0.37        0.555         3.18       0.0331\n",
      "     11    20       0.0162      0.00621      0.00997       0.0646        0.097         11.8        0.123\n",
      "     11    30        0.174        0.171        0.003        0.339        0.509         6.47       0.0674\n",
      "     11    40        0.351        0.351     2.92e-06        0.474        0.729        0.203      0.00212\n",
      "     11    50        0.247        0.247      5.3e-05        0.419        0.612        0.863      0.00899\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     11     2         0.15         0.15     0.000558        0.327        0.476         2.46       0.0256\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              11   31.006    0.002          0.3      0.00216        0.302        0.388        0.674         4.12       0.0429\n",
      "! Validation         11   31.006    0.002        0.197     0.000506        0.198        0.357        0.547         2.13       0.0222\n",
      "Wall time: 31.006860786000004\n",
      "! Best model       11    0.198\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     12    10        0.247        0.247     8.01e-07        0.389        0.612        0.105       0.0011\n",
      "     12    20        0.278        0.277     0.000131        0.441        0.648         1.35       0.0141\n",
      "     12    30        0.159        0.158     0.000846        0.334        0.489         3.44       0.0358\n",
      "     12    40        0.303        0.302      0.00128        0.422        0.676         4.23       0.0441\n",
      "     12    50       0.0819       0.0816      0.00028        0.241        0.352         1.98       0.0206\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     12     2        0.146        0.145     0.000627        0.322        0.469         2.64       0.0275\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              12   33.642    0.002        0.276      0.00177        0.278        0.379        0.647         4.04        0.042\n",
      "! Validation         12   33.642    0.002        0.195     0.000565        0.195        0.353        0.543          2.3        0.024\n",
      "Wall time: 33.64236384999998\n",
      "! Best model       12    0.195\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     13    10       0.0117      0.00821      0.00346       0.0838        0.112         6.95       0.0724\n",
      "     13    20        0.157        0.157     0.000663        0.338        0.487         3.05       0.0317\n",
      "     13    30        0.277        0.277     0.000334        0.405        0.647         2.16       0.0225\n",
      "     13    40        0.172        0.171      0.00128        0.342        0.509         4.22       0.0439\n",
      "     13    50        0.246        0.245      0.00102          0.4        0.609         3.77       0.0393\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     13     2        0.139        0.138     0.000666        0.314        0.458         2.67       0.0278\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              13   37.397    0.002        0.268      0.00202         0.27        0.371        0.637         4.15       0.0432\n",
      "! Validation         13   37.397    0.002        0.188     0.000666        0.189        0.346        0.534         2.57       0.0267\n",
      "Wall time: 37.397720745000015\n",
      "! Best model       13    0.189\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     14    10        0.234        0.224      0.00915         0.38        0.583         11.3        0.118\n",
      "     14    20        0.127        0.124      0.00348        0.322        0.433         6.97       0.0726\n",
      "     14    30        0.197        0.197     0.000555        0.378        0.546         2.78        0.029\n",
      "     14    40         0.15        0.146      0.00331        0.352        0.471         6.79       0.0708\n",
      "     14    50         0.27        0.266      0.00357        0.412        0.635         7.06       0.0735\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     14     2        0.136        0.136     0.000686        0.311        0.453         2.76       0.0287\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              14   39.906    0.002        0.255      0.00428        0.259        0.368        0.621         6.62        0.069\n",
      "! Validation         14   39.906    0.002        0.187     0.000624        0.187        0.344        0.532         2.44       0.0254\n",
      "Wall time: 39.906758543000024\n",
      "! Best model       14    0.187\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     15    10        0.244        0.241      0.00252        0.385        0.605         5.93       0.0618\n",
      "     15    20        0.156        0.154      0.00169        0.314        0.483         4.86       0.0507\n",
      "     15    30        0.101       0.0999      0.00131        0.294        0.389         4.28       0.0446\n",
      "     15    40        0.131        0.131     1.46e-05        0.281        0.445        0.449      0.00468\n",
      "     15    50        0.294        0.294     0.000225        0.428        0.667         1.77       0.0185\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     15     2        0.132        0.132     0.000707        0.306        0.446         2.79       0.0291\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              15   42.346    0.002         0.24      0.00187        0.242        0.358        0.603          4.2       0.0438\n",
      "! Validation         15   42.346    0.002        0.184     0.000667        0.185        0.341        0.529         2.58       0.0268\n",
      "Wall time: 42.34661901099997\n",
      "! Best model       15    0.185\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     16    10        0.128        0.126       0.0014        0.281        0.437         4.41        0.046\n",
      "     16    20        0.147        0.146     0.000614        0.328         0.47         2.93       0.0305\n",
      "     16    30        0.272        0.272     5.25e-05        0.404        0.642        0.855      0.00891\n",
      "     16    40        0.223        0.223     0.000101        0.328        0.581         1.19       0.0124\n",
      "     16    50        0.211         0.21     0.000284        0.366        0.564         1.99       0.0208\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     16     2        0.127        0.127      0.00073        0.301        0.438         2.85       0.0297\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              16   44.855    0.002        0.234       0.0014        0.236        0.354        0.596         3.42       0.0356\n",
      "! Validation         16   44.855    0.002        0.182     0.000661        0.183        0.338        0.525         2.55       0.0266\n",
      "Wall time: 44.85589997699998\n",
      "! Best model       16    0.183\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     17    10        0.206        0.206     0.000527        0.362        0.559         2.71       0.0282\n",
      "     17    20         0.15        0.149      0.00132        0.316        0.474          4.3       0.0448\n",
      "     17    30        0.249        0.248     7.16e-05        0.423        0.613            1       0.0104\n",
      "     17    40       0.0995       0.0995     1.25e-05        0.288        0.388        0.418      0.00435\n",
      "     17    50        0.281        0.274      0.00675        0.404        0.644         9.71        0.101\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     17     2        0.123        0.122     0.000759        0.296         0.43          2.9       0.0302\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              17   47.342    0.002        0.226       0.0015        0.228        0.351        0.586         3.68       0.0384\n",
      "! Validation         17   47.342    0.002        0.179     0.000715         0.18        0.335         0.52          2.7       0.0281\n",
      "Wall time: 47.342617820999976\n",
      "! Best model       17    0.180\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     18    10        0.113        0.113     0.000109        0.291        0.413         1.24       0.0129\n",
      "     18    20        0.245        0.244     0.000723        0.396        0.608         3.18       0.0331\n",
      "     18    30        0.241         0.24      0.00122        0.375        0.602         4.12        0.043\n",
      "     18    40        0.228        0.226      0.00123        0.404        0.586         4.14       0.0431\n",
      "     18    50       0.0788       0.0717      0.00709        0.241         0.33         9.95        0.104\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     18     2         0.12        0.119     0.000772        0.292        0.424         2.97       0.0309\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              18   49.857    0.002        0.216      0.00169        0.218        0.341        0.573         3.75       0.0391\n",
      "! Validation         18   49.857    0.002        0.177     0.000672        0.177        0.332        0.517         2.59        0.027\n",
      "Wall time: 49.857774533\n",
      "! Best model       18    0.177\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     19    10        0.253        0.251      0.00119        0.422        0.617         4.07       0.0424\n",
      "     19    20        0.116         0.11      0.00511        0.285        0.409         8.45        0.088\n",
      "     19    30        0.296        0.294      0.00102        0.405        0.668         3.77       0.0393\n",
      "     19    40        0.135        0.134     0.000818        0.324         0.45         3.38       0.0352\n",
      "     19    50         0.19        0.188      0.00194         0.35        0.534         5.21       0.0542\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     19     2        0.115        0.115     0.000759        0.287        0.417         2.93       0.0305\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              19   52.388    0.002         0.21      0.00109        0.211        0.338        0.563         3.35       0.0349\n",
      "! Validation         19   52.388    0.002        0.174     0.000668        0.175         0.33        0.514         2.58       0.0269\n",
      "Wall time: 52.38888324300001\n",
      "! Best model       19    0.175\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     20    10          0.1          0.1     2.31e-05        0.281        0.389        0.566       0.0059\n",
      "     20    20        0.235        0.235     7.65e-05        0.386        0.597         1.03       0.0107\n",
      "     20    30         0.16        0.156      0.00433        0.329        0.486         7.78        0.081\n",
      "     20    40        0.121        0.121     0.000253          0.3        0.427         1.88       0.0196\n",
      "     20    50         0.24         0.24     0.000534        0.362        0.602         2.73       0.0284\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     20     2        0.112        0.111     0.000791        0.282         0.41         3.02       0.0315\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              20   54.779    0.002        0.205      0.00224        0.207        0.334        0.558         4.45       0.0463\n",
      "! Validation         20   54.779    0.002        0.171     0.000662        0.172        0.326        0.509         2.58       0.0268\n",
      "Wall time: 54.779243764\n",
      "! Best model       20    0.172\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     21    10       0.0553       0.0545     0.000865        0.216        0.287         3.48       0.0362\n",
      "     21    20        0.249        0.248      0.00124        0.407        0.612         4.17       0.0434\n",
      "     21    30        0.146        0.146     4.14e-08        0.314        0.471       0.0273     0.000285\n",
      "     21    40        0.239        0.239     0.000229        0.413        0.602         1.79       0.0186\n",
      "     21    50        0.184        0.184     3.88e-05        0.346        0.528        0.734      0.00765\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     21     2        0.108        0.107     0.000749        0.278        0.403         2.92       0.0304\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              21   57.247    0.002        0.197       0.0022        0.199        0.328        0.546         4.62       0.0481\n",
      "! Validation         21   57.247    0.002        0.169      0.00065        0.169        0.323        0.505         2.56       0.0267\n",
      "Wall time: 57.247170228000016\n",
      "! Best model       21    0.169\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     22    10        0.242        0.241     0.000504        0.424        0.605         2.65       0.0276\n",
      "     22    20        0.251        0.248      0.00264        0.384        0.613         6.07       0.0632\n",
      "     22    30        0.119         0.11      0.00846        0.274        0.408         10.9        0.113\n",
      "     22    40        0.225        0.225     0.000369        0.406        0.584         2.27       0.0236\n",
      "     22    50        0.245        0.243      0.00239        0.417        0.607         5.77       0.0601\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     22     2        0.105        0.104     0.000753        0.273        0.397         2.94       0.0306\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              22   60.904    0.002        0.191      0.00162        0.193        0.325        0.538         3.81       0.0397\n",
      "! Validation         22   60.904    0.002        0.166     0.000656        0.166        0.319        0.501          2.6       0.0271\n",
      "Wall time: 60.90505778300002\n",
      "! Best model       22    0.166\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     23    10         1.58         1.58      0.00019        0.547         1.55         1.63        0.017\n",
      "     23    20        0.204        0.203     0.000612        0.373        0.555         2.93       0.0305\n",
      "     23    30        0.232        0.232     6.69e-05        0.392        0.593        0.965       0.0101\n",
      "     23    40       0.0626       0.0623     0.000293        0.225        0.307         2.02       0.0211\n",
      "     23    50        0.259        0.258     0.000197        0.389        0.626         1.66       0.0173\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     23     2        0.102        0.101     0.000743        0.269        0.391         2.92       0.0304\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              23   64.491    0.002        0.187      0.00147        0.189        0.323        0.533         3.58       0.0373\n",
      "! Validation         23   64.491    0.002        0.164     0.000634        0.164        0.316        0.498         2.53       0.0263\n",
      "Wall time: 64.49172071700002\n",
      "! Best model       23    0.164\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     24    10         0.14         0.14     0.000366         0.31         0.46         2.26       0.0236\n",
      "     24    20       0.0987       0.0987     8.76e-07        0.272        0.387        0.109      0.00114\n",
      "     24    30         0.28         0.27      0.00981        0.373        0.639         11.7        0.122\n",
      "     24    40        0.214        0.214     4.08e-05        0.386        0.569        0.754      0.00785\n",
      "     24    50        0.214        0.213     0.000713        0.394        0.568         3.15       0.0328\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     24     2       0.0984       0.0977     0.000722        0.265        0.385         2.87       0.0299\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              24   68.250    0.002         0.18      0.00135        0.181        0.314        0.522          3.4       0.0355\n",
      "! Validation         24   68.250    0.002        0.161     0.000612        0.162        0.313        0.494         2.47       0.0257\n",
      "Wall time: 68.250997229\n",
      "! Best model       24    0.162\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     25    10        0.189        0.189     6.16e-06        0.349        0.536        0.293      0.00305\n",
      "     25    20        0.234        0.233      0.00108        0.374        0.594         3.89       0.0405\n",
      "     25    30        0.157        0.156     0.000723        0.285        0.487         3.18       0.0331\n",
      "     25    40        0.164        0.164     6.16e-06         0.32        0.498        0.293      0.00305\n",
      "     25    50        0.157        0.154      0.00354        0.313        0.482         7.03       0.0732\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     25     2       0.0959       0.0951     0.000736        0.261         0.38          2.9       0.0302\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              25   71.955    0.002        0.173      0.00142        0.175        0.311        0.512         3.61       0.0376\n",
      "! Validation         25   71.955    0.002        0.159     0.000644        0.159         0.31         0.49         2.59       0.0269\n",
      "Wall time: 71.95549804199999\n",
      "! Best model       25    0.159\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     26    10         1.41         1.41     0.000508        0.538         1.46         2.66       0.0278\n",
      "     26    20       0.0864       0.0853      0.00113        0.249        0.359         3.97       0.0413\n",
      "     26    30        0.213        0.213     0.000419        0.347        0.568         2.42       0.0252\n",
      "     26    40        0.223         0.22      0.00246        0.369        0.578         5.86        0.061\n",
      "     26    50         0.12        0.115      0.00561        0.296        0.417         8.84       0.0921\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     26     2       0.0935       0.0928     0.000733        0.258        0.375          2.9       0.0302\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              26   75.691    0.002        0.169      0.00152        0.171        0.306        0.506         3.68       0.0383\n",
      "! Validation         26   75.691    0.002        0.157     0.000624        0.157        0.308        0.487         2.53       0.0264\n",
      "Wall time: 75.69150123700001\n",
      "! Best model       26    0.157\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     27    10         0.21        0.208      0.00134        0.343        0.562         4.33       0.0451\n",
      "     27    20       0.0847       0.0847     1.86e-05        0.277        0.358        0.508      0.00529\n",
      "     27    30       0.0881       0.0881        2e-07        0.263        0.365       0.0547      0.00057\n",
      "     27    40        0.167        0.163      0.00376        0.329        0.497         7.24       0.0754\n",
      "     27    50        0.124        0.122      0.00164        0.273         0.43         4.79       0.0498\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     27     2       0.0913       0.0906      0.00072        0.256         0.37         2.87       0.0299\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              27   79.445    0.002        0.168      0.00125        0.169        0.309        0.504         3.41       0.0356\n",
      "! Validation         27   79.445    0.002        0.155      0.00061        0.155        0.305        0.484          2.5        0.026\n",
      "Wall time: 79.445961501\n",
      "! Best model       27    0.155\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     28    10       0.0734       0.0716      0.00186        0.248        0.329         5.09       0.0531\n",
      "     28    20       0.0944       0.0928      0.00161        0.274        0.375         4.73       0.0493\n",
      "     28    30       0.0978       0.0975     0.000296        0.274        0.384         2.04       0.0212\n",
      "     28    40       0.0771        0.077     0.000141        0.256        0.341          1.4       0.0146\n",
      "     28    50      0.00556      0.00387      0.00169       0.0557       0.0765         4.86       0.0506\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     28     2       0.0894       0.0887     0.000733        0.253        0.367         2.89       0.0301\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              28   83.234    0.002        0.167       0.0013        0.168        0.314        0.503         3.52       0.0366\n",
      "! Validation         28   83.234    0.002        0.153     0.000625        0.153        0.302        0.481         2.55       0.0265\n",
      "Wall time: 83.234729915\n",
      "! Best model       28    0.153\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     29    10        0.178        0.177     0.000274        0.353        0.518         1.96       0.0204\n",
      "     29    20        0.112        0.098       0.0135        0.277        0.385         13.7        0.143\n",
      "     29    30        0.202        0.202        2e-07        0.356        0.552       0.0547      0.00057\n",
      "     29    40        0.171        0.171     8.53e-05        0.336        0.509         1.09       0.0114\n",
      "     29    50       0.0819        0.077      0.00487         0.25        0.342         8.25       0.0859\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     29     2       0.0876       0.0869      0.00075        0.251        0.363         2.93       0.0305\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              29   87.016    0.002        0.159      0.00184         0.16        0.301         0.49         3.95       0.0411\n",
      "! Validation         29   87.016    0.002         0.15     0.000623         0.15        0.299        0.476         2.55       0.0266\n",
      "Wall time: 87.016413736\n",
      "! Best model       29    0.150\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     30    10        0.199        0.198     0.000334        0.331        0.548         2.16       0.0225\n",
      "     30    20       0.0708       0.0685      0.00232        0.226        0.322          5.7       0.0593\n",
      "     30    30         1.05         1.05     0.000159        0.502         1.26         1.49       0.0155\n",
      "     30    40        0.195        0.195     0.000212        0.369        0.543         1.72       0.0179\n",
      "     30    50        0.178        0.178     1.97e-05        0.325         0.52        0.523      0.00545\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     30     2       0.0857        0.085     0.000726        0.248        0.359         2.87       0.0299\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              30   91.042    0.002        0.154      0.00105        0.155        0.298        0.483         3.02       0.0315\n",
      "! Validation         30   91.042    0.002        0.148     0.000612        0.148        0.297        0.473         2.52       0.0263\n",
      "Wall time: 91.04236167800002\n",
      "! Best model       30    0.148\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     31    10        0.105        0.105     5.07e-05        0.284        0.399        0.844      0.00879\n",
      "     31    20        0.197        0.197     0.000322        0.329        0.546         2.12       0.0221\n",
      "     31    30       0.0741       0.0728      0.00127        0.234        0.332         4.21       0.0439\n",
      "     31    40         0.24        0.238      0.00211        0.372        0.601         5.43       0.0565\n",
      "     31    50         0.18         0.18     9.86e-05        0.364        0.522         1.18       0.0122\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     31     2       0.0847       0.0839     0.000753        0.246        0.357         2.93       0.0305\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              31   94.694    0.002        0.148      0.00184         0.15        0.293        0.474         4.13        0.043\n",
      "! Validation         31   94.694    0.002        0.146     0.000631        0.147        0.294         0.47         2.58       0.0268\n",
      "Wall time: 94.69480726699999\n",
      "! Best model       31    0.147\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     32    10        0.159        0.158     0.000449        0.336        0.489          2.5       0.0261\n",
      "     32    20        0.218        0.216      0.00174        0.404        0.572         4.93       0.0514\n",
      "     32    30       0.0833       0.0826     0.000697        0.254        0.354         3.12       0.0325\n",
      "     32    40        0.115        0.114      0.00149        0.289        0.415         4.57       0.0476\n",
      "     32    50        0.191        0.191     0.000198        0.336        0.538         1.66       0.0173\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     32     2       0.0832       0.0825     0.000746        0.245        0.353         2.91       0.0303\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              32   98.989    0.002         0.15      0.00144        0.151        0.303        0.477         3.76       0.0392\n",
      "! Validation         32   98.989    0.002        0.145     0.000619        0.146        0.293        0.469         2.54       0.0264\n",
      "Wall time: 98.989952996\n",
      "! Best model       32    0.146\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     33    10        0.161         0.16     0.000695        0.327        0.493         3.12       0.0325\n",
      "     33    20        0.171         0.17     0.000474        0.315        0.508         2.57       0.0268\n",
      "     33    30        0.174        0.171      0.00337        0.332        0.508         6.86       0.0715\n",
      "     33    40        0.146        0.146     0.000298        0.299         0.47         2.04       0.0212\n",
      "     33    50        0.113        0.107      0.00551        0.287        0.403         8.77       0.0913\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     33     2       0.0819       0.0811     0.000775        0.242         0.35         2.98        0.031\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              33  102.054    0.002        0.145      0.00319        0.148        0.293        0.468         5.41       0.0563\n",
      "! Validation         33  102.054    0.002        0.143     0.000663        0.144         0.29        0.466         2.69        0.028\n",
      "Wall time: 102.056213324\n",
      "! Best model       33    0.144\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     34    10        0.164        0.159      0.00428        0.333        0.491         7.73       0.0805\n",
      "     34    20        0.155        0.153      0.00233        0.335        0.481         5.71       0.0594\n",
      "     34    30        0.242         0.24      0.00184         0.36        0.603         5.06       0.0527\n",
      "     34    40        0.124        0.123      0.00153        0.291        0.431         4.62       0.0482\n",
      "     34    50       0.0725       0.0697      0.00276        0.249        0.325         6.21       0.0647\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     34     2        0.081       0.0803     0.000789        0.242        0.349            3       0.0312\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              34  106.229    0.002        0.143      0.00261        0.145        0.297        0.465            5       0.0521\n",
      "! Validation         34  106.229    0.002        0.143     0.000668        0.143        0.289        0.465         2.69        0.028\n",
      "Wall time: 106.230123429\n",
      "! Best model       34    0.143\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     35    10       0.0878       0.0865      0.00127        0.258        0.362         4.22       0.0439\n",
      "     35    20        0.153        0.152     0.000446        0.316         0.48          2.5        0.026\n",
      "     35    30        0.751         0.75     0.000897        0.446         1.07         3.54       0.0369\n",
      "     35    40        0.154        0.154     3.98e-06        0.315        0.482        0.234      0.00244\n",
      "     35    50        0.142         0.14      0.00192        0.312        0.461         5.17       0.0539\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     35     2       0.0793       0.0785     0.000825        0.239        0.345         3.08       0.0321\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              35  109.399    0.002        0.141      0.00166        0.143        0.294        0.462          3.8       0.0396\n",
      "! Validation         35  109.399    0.002         0.14     0.000704        0.141        0.286        0.461          2.8       0.0292\n",
      "Wall time: 109.40038847\n",
      "! Best model       35    0.141\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     36    10       0.0812       0.0735      0.00767        0.239        0.334         10.3        0.108\n",
      "     36    20        0.141        0.141     0.000116        0.305        0.462         1.27       0.0133\n",
      "     36    30        0.157        0.157     0.000666        0.328        0.487         3.05       0.0317\n",
      "     36    40        0.233        0.233     0.000245        0.375        0.594         1.85       0.0193\n",
      "     36    50        0.069       0.0654      0.00359        0.239        0.315         7.08       0.0738\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     36     2       0.0787       0.0779     0.000833        0.238        0.343          3.1       0.0323\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              36  113.185    0.002        0.137      0.00177        0.139        0.291        0.455         4.26       0.0444\n",
      "! Validation         36  113.185    0.002        0.139     0.000697         0.14        0.285        0.459         2.79        0.029\n",
      "Wall time: 113.18585349900002\n",
      "! Best model       36    0.140\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     37    10        0.183        0.183     1.69e-05        0.329        0.526        0.488      0.00509\n",
      "     37    20         0.16        0.157      0.00321        0.327        0.487          6.7       0.0697\n",
      "     37    30        0.172        0.169      0.00304        0.346        0.505         6.52       0.0679\n",
      "     37    40        0.104        0.103      0.00106        0.284        0.395         3.85       0.0401\n",
      "     37    50        0.092       0.0914     0.000632         0.26        0.372         2.97        0.031\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     37     2       0.0778       0.0769     0.000832        0.237        0.341         3.09       0.0322\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              37  117.074    0.002        0.134      0.00204        0.136        0.288        0.451         4.16       0.0434\n",
      "! Validation         37  117.074    0.002        0.138     0.000717        0.139        0.283        0.458         2.84       0.0296\n",
      "Wall time: 117.07479190700002\n",
      "! Best model       37    0.139\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     38    10        0.554        0.553      0.00133         0.43        0.915         4.31       0.0449\n",
      "     38    20       0.0649       0.0639     0.000944        0.234        0.311         3.63       0.0378\n",
      "     38    30        0.196        0.195       0.0014        0.349        0.543         4.42       0.0461\n",
      "     38    40        0.069       0.0685     0.000525         0.23        0.322         2.71       0.0282\n",
      "     38    50        0.117        0.117      9.7e-05        0.275         0.42         1.16       0.0121\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     38     2        0.077       0.0761      0.00087        0.235         0.34         3.17        0.033\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              38  120.115    0.002        0.131      0.00196        0.133        0.284        0.445         3.85       0.0401\n",
      "! Validation         38  120.115    0.002        0.137     0.000725        0.138        0.282        0.456         2.86       0.0297\n",
      "Wall time: 120.11604897799998\n",
      "! Best model       38    0.138\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     39    10        0.156        0.154      0.00137        0.334        0.484         4.38       0.0456\n",
      "     39    20       0.0601        0.057      0.00313        0.225        0.294         6.61       0.0688\n",
      "     39    30       0.0902       0.0824      0.00777        0.247        0.353         10.4        0.108\n",
      "     39    40       0.0613       0.0611     0.000251        0.213        0.304         1.87       0.0195\n",
      "     39    50       0.0541       0.0494      0.00471        0.206        0.274         8.11       0.0844\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     39     2       0.0762       0.0753     0.000911        0.234        0.338         3.26        0.034\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              39  123.203    0.002        0.129      0.00182        0.131        0.285        0.442         4.12       0.0429\n",
      "! Validation         39  123.203    0.002        0.136     0.000774        0.137        0.279        0.454            3       0.0312\n",
      "Wall time: 123.20431280000003\n",
      "! Best model       39    0.137\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     40    10        0.046       0.0449      0.00118        0.193        0.261         4.06       0.0423\n",
      "     40    20        0.151         0.15     0.000353        0.312        0.477         2.22       0.0231\n",
      "     40    30       0.0644       0.0609      0.00349         0.23        0.304         6.98       0.0727\n",
      "     40    40       0.0709       0.0691      0.00178        0.237        0.324         4.98       0.0519\n",
      "     40    50        0.114         0.11      0.00372        0.272        0.408          7.2        0.075\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     40     2       0.0751       0.0742     0.000892        0.232        0.335         3.22       0.0335\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              40  126.195    0.002        0.128      0.00178        0.129        0.284         0.44         4.12       0.0429\n",
      "! Validation         40  126.195    0.002        0.134     0.000761        0.135        0.277        0.451         2.97       0.0309\n",
      "Wall time: 126.19572917300002\n",
      "! Best model       40    0.135\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     41    10        0.165        0.164     0.000313        0.326        0.499         2.09       0.0218\n",
      "     41    20        0.131        0.131     3.06e-06        0.308        0.445        0.207      0.00216\n",
      "     41    30       0.0565       0.0494      0.00716        0.178        0.274           10        0.104\n",
      "     41    40        0.158        0.157     0.000546        0.313        0.488         2.76       0.0288\n",
      "     41    50        0.162        0.158      0.00457        0.303        0.489         7.99       0.0833\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     41     2       0.0743       0.0734     0.000883        0.231        0.333         3.19       0.0332\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              41  129.319    0.002        0.125      0.00242        0.128        0.284        0.436         4.76       0.0496\n",
      "! Validation         41  129.319    0.002        0.133     0.000778        0.134        0.275        0.449            3       0.0313\n",
      "Wall time: 129.319576734\n",
      "! Best model       41    0.134\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     42    10       0.0969       0.0965     0.000462        0.261        0.382         2.54       0.0264\n",
      "     42    20        0.163         0.16      0.00314        0.339        0.493         6.62        0.069\n",
      "     42    30        0.101       0.0985      0.00289         0.25        0.386         6.35       0.0661\n",
      "     42    40        0.192        0.191     0.000781         0.37        0.538          3.3       0.0344\n",
      "     42    50        0.162        0.157      0.00429        0.305        0.488         7.74       0.0806\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     42     2       0.0736       0.0727     0.000892         0.23        0.332         3.21       0.0334\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              42  132.704    0.002        0.125      0.00273        0.128        0.282        0.435         5.02       0.0523\n",
      "! Validation         42  132.704    0.002        0.132     0.000782        0.133        0.274        0.447         3.01       0.0313\n",
      "Wall time: 132.704343091\n",
      "! Best model       42    0.133\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     43    10       0.0618       0.0613     0.000432        0.234        0.305         2.46       0.0256\n",
      "     43    20        0.041       0.0342       0.0068        0.155        0.228         9.75        0.102\n",
      "     43    30        0.212        0.212     2.88e-05        0.346        0.567        0.633      0.00659\n",
      "     43    40        0.073       0.0707      0.00226        0.255        0.327         5.62       0.0586\n",
      "     43    50        0.146        0.145      0.00075        0.304        0.469         3.23       0.0337\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     43     2       0.0727       0.0717     0.000929        0.228         0.33         3.29       0.0342\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              43  136.244    0.002        0.121      0.00164        0.123        0.278        0.429         3.85       0.0401\n",
      "! Validation         43  136.244    0.002        0.131     0.000774        0.132        0.272        0.445         2.99       0.0311\n",
      "Wall time: 136.244873369\n",
      "! Best model       43    0.132\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     44    10       0.0976       0.0973     0.000312        0.263        0.384         2.09       0.0217\n",
      "     44    20        0.155        0.155     7.43e-06        0.301        0.484        0.324      0.00338\n",
      "     44    30        0.064       0.0626      0.00142        0.216        0.308         4.45       0.0464\n",
      "     44    40        0.121        0.121        7e-06         0.29        0.429        0.312      0.00326\n",
      "     44    50        0.123        0.122     0.000655        0.274         0.43         3.02       0.0315\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     44     2       0.0722       0.0712      0.00094        0.227        0.328          3.3       0.0344\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              44  140.413    0.002        0.116      0.00211        0.119        0.273         0.42         4.65       0.0484\n",
      "! Validation         44  140.413    0.002         0.13      0.00079        0.131        0.271        0.444         3.03       0.0315\n",
      "Wall time: 140.41353458999998\n",
      "! Best model       44    0.131\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     45    10       0.0472       0.0353       0.0119        0.174        0.231         12.9        0.134\n",
      "     45    20        0.181        0.177      0.00353        0.344        0.518         7.03       0.0732\n",
      "     45    30       0.0361       0.0354     0.000772        0.162        0.231         3.29       0.0342\n",
      "     45    40        0.152        0.151     0.000422          0.3        0.478         2.43       0.0253\n",
      "     45    50        0.215        0.215     2.31e-05         0.37        0.571        0.566       0.0059\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     45     2       0.0716       0.0707     0.000946        0.227        0.327         3.32       0.0346\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              45  143.701    0.002        0.116      0.00202        0.118        0.273        0.419         4.58       0.0477\n",
      "! Validation         45  143.701    0.002        0.129     0.000809         0.13        0.269        0.442         3.08       0.0321\n",
      "Wall time: 143.70179742599998\n",
      "! Best model       45    0.130\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     46    10       0.0525       0.0484      0.00404        0.196        0.271         7.52       0.0783\n",
      "     46    20        0.159        0.155      0.00463         0.31        0.484         8.04       0.0837\n",
      "     46    30       0.0647       0.0645     0.000261         0.22        0.313         1.91       0.0199\n",
      "     46    40        0.147        0.147     8.53e-05        0.307        0.471         1.09       0.0114\n",
      "     46    50        0.105        0.104     0.000894        0.279        0.397         3.53       0.0368\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     46     2       0.0708       0.0699     0.000976        0.225        0.325         3.38       0.0352\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              46  146.996    0.002        0.116     0.000931        0.117        0.275        0.419         3.09       0.0322\n",
      "! Validation         46  146.996    0.002        0.128     0.000807        0.129        0.269        0.441         3.06       0.0319\n",
      "Wall time: 146.99728362399998\n",
      "! Best model       46    0.129\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     47    10       0.0662       0.0662     5.96e-06        0.245        0.317        0.285      0.00297\n",
      "     47    20        0.159        0.147       0.0117        0.299        0.472         12.8        0.133\n",
      "     47    30        0.175        0.174      0.00162        0.343        0.513         4.76       0.0496\n",
      "     47    40         0.17         0.17       0.0001        0.332        0.507         1.18       0.0123\n",
      "     47    50       0.0858       0.0761      0.00974        0.251         0.34         11.7        0.121\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     47     2       0.0702       0.0693     0.000981        0.224        0.324         3.39       0.0353\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              47  150.563    0.002        0.119      0.00284        0.122        0.281        0.425         5.12       0.0534\n",
      "! Validation         47  150.563    0.002        0.127     0.000803        0.128        0.267        0.439         3.06       0.0318\n",
      "Wall time: 150.56348478800004\n",
      "! Best model       47    0.128\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     48    10        0.148        0.147     0.000676        0.311        0.472         3.07        0.032\n",
      "     48    20       0.0695       0.0663      0.00321        0.244        0.317          6.7       0.0697\n",
      "     48    30        0.138        0.138     2.88e-05        0.296        0.457        0.637      0.00663\n",
      "     48    40        0.123        0.123     6.96e-05         0.28        0.431        0.984       0.0103\n",
      "     48    50        0.178        0.175      0.00319        0.327        0.514         6.68       0.0696\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     48     2       0.0696       0.0686      0.00098        0.223        0.322         3.39       0.0353\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              48  153.970    0.002        0.114      0.00218        0.117        0.274        0.416         4.66       0.0486\n",
      "! Validation         48  153.970    0.002        0.127     0.000821        0.127        0.266        0.438         3.11       0.0323\n",
      "Wall time: 153.971253127\n",
      "! Best model       48    0.127\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     49    10        0.149        0.149     0.000429         0.32        0.475         2.45       0.0255\n",
      "     49    20       0.0945       0.0867      0.00782        0.234        0.362         10.4        0.109\n",
      "     49    30        0.079       0.0773       0.0017        0.263        0.342         4.87       0.0507\n",
      "     49    40        0.188        0.188     4.14e-08        0.351        0.533       0.0273     0.000285\n",
      "     49    50       0.0846       0.0842     0.000389        0.262        0.357         2.33       0.0243\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     49     2       0.0689       0.0679      0.00098        0.222        0.321         3.38       0.0352\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              49  157.344    0.002        0.113      0.00151        0.114        0.273        0.413         3.82       0.0398\n",
      "! Validation         49  157.344    0.002        0.125     0.000785        0.126        0.265        0.436         3.01       0.0313\n",
      "Wall time: 157.34438035099998\n",
      "! Best model       49    0.126\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     50    10        0.124        0.123     0.000693        0.292        0.432         3.11       0.0324\n",
      "     50    20       0.0756       0.0669      0.00867        0.225        0.318           11        0.115\n",
      "     50    30       0.0837       0.0807      0.00305        0.257         0.35         6.52        0.068\n",
      "     50    40        0.182        0.181     0.000842        0.344        0.524         3.43       0.0357\n",
      "     50    50        0.151        0.148      0.00341         0.31        0.473          6.9       0.0719\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     50     2       0.0685       0.0675     0.000984        0.221         0.32         3.39       0.0353\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              50  160.640    0.002        0.112      0.00317        0.115        0.271        0.412          5.4       0.0563\n",
      "! Validation         50  160.640    0.002        0.124     0.000817        0.125        0.263        0.434         3.09       0.0322\n",
      "Wall time: 160.640917506\n",
      "! Best model       50    0.125\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     51    10       0.0541       0.0516      0.00253         0.21        0.279         5.94       0.0619\n",
      "     51    20       0.0706       0.0693      0.00127         0.23        0.324         4.21       0.0439\n",
      "     51    30        0.158        0.158     1.49e-05        0.323        0.489        0.453      0.00472\n",
      "     51    40       0.0934       0.0929      0.00051        0.258        0.375         2.67       0.0278\n",
      "     51    50        0.143         0.14       0.0031        0.285         0.46         6.58       0.0685\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     51     2       0.0683       0.0673      0.00104        0.221        0.319          3.5       0.0364\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              51  164.398    0.002        0.107      0.00168        0.108        0.265        0.402         3.96       0.0412\n",
      "! Validation         51  164.398    0.002        0.124     0.000867        0.125        0.263        0.433         3.21       0.0334\n",
      "Wall time: 164.39852393899997\n",
      "! Best model       51    0.125\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     52    10       0.0525       0.0525     2.15e-05        0.213        0.282        0.551      0.00574\n",
      "     52    20       0.0622        0.062     0.000136        0.218        0.306         1.38       0.0144\n",
      "     52    30        0.136        0.135     0.000732        0.296        0.452          3.2       0.0333\n",
      "     52    40        0.118        0.115      0.00371        0.264        0.417         7.19       0.0749\n",
      "     52    50       0.0962        0.096     0.000274        0.253        0.381         1.95       0.0203\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     52     2       0.0677       0.0667      0.00103         0.22        0.318         3.48       0.0363\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              52  167.541    0.002        0.105      0.00246        0.108        0.262        0.399         4.72       0.0492\n",
      "! Validation         52  167.541    0.002        0.123     0.000839        0.124        0.262        0.432         3.14       0.0327\n",
      "Wall time: 167.54099741399995\n",
      "! Best model       52    0.124\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     53    10       0.0619       0.0618     7.65e-05        0.214        0.306         1.03       0.0107\n",
      "     53    20       0.0449       0.0439     0.000959        0.194        0.258         3.66       0.0381\n",
      "     53    30        0.132        0.131     0.000977         0.31        0.446         3.69       0.0385\n",
      "     53    40        0.177        0.176     0.000641        0.308        0.517         2.99       0.0312\n",
      "     53    50       0.0665        0.061      0.00551        0.234        0.304         8.78       0.0914\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     53     2       0.0672       0.0662      0.00108        0.219        0.317         3.57       0.0372\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              53  171.137    0.002        0.106      0.00292        0.109        0.264          0.4          5.4       0.0563\n",
      "! Validation         53  171.137    0.002        0.122     0.000869        0.123         0.26         0.43          3.2       0.0334\n",
      "Wall time: 171.138107692\n",
      "! Best model       53    0.123\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     54    10       0.0577       0.0509      0.00672        0.216        0.278         9.69        0.101\n",
      "     54    20       0.0937       0.0933     0.000377        0.262        0.376          2.3       0.0239\n",
      "     54    30        0.147        0.144      0.00225        0.329        0.468         5.61       0.0584\n",
      "     54    40        0.154        0.154     6.36e-06        0.348        0.483        0.297      0.00309\n",
      "     54    50        0.158        0.158     0.000557        0.321        0.489         2.79       0.0291\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     54     2        0.067        0.066      0.00104        0.219        0.316         3.49       0.0363\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              54  174.750    0.002        0.114       0.0016        0.115         0.28        0.415         3.82       0.0398\n",
      "! Validation         54  174.750    0.002        0.122     0.000866        0.123         0.26        0.429          3.2       0.0334\n",
      "Wall time: 174.75106304800005\n",
      "! Best model       54    0.123\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     55    10        0.123        0.121      0.00133        0.301        0.429         4.31       0.0449\n",
      "     55    20        0.143        0.143     3.78e-05        0.292        0.465        0.723      0.00753\n",
      "     55    30       0.0695       0.0688     0.000687        0.228        0.323          3.1       0.0323\n",
      "     55    40       0.0641        0.064     0.000137        0.216        0.311         1.39       0.0144\n",
      "     55    50       0.0556       0.0549      0.00067        0.221        0.288         3.06       0.0319\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     55     2       0.0666       0.0655      0.00108        0.218        0.315         3.58       0.0373\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              55  178.119    0.002        0.107      0.00291         0.11        0.266        0.402         5.29       0.0551\n",
      "! Validation         55  178.119    0.002        0.121      0.00089        0.122        0.259        0.429         3.26       0.0339\n",
      "Wall time: 178.11964503099995\n",
      "! Best model       55    0.122\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     56    10        0.133        0.131      0.00244        0.276        0.445         5.83       0.0608\n",
      "     56    20        0.108        0.105      0.00342        0.273        0.398         6.91        0.072\n",
      "     56    30        0.143        0.134       0.0097        0.314         0.45         11.6        0.121\n",
      "     56    40       0.0657       0.0621      0.00366        0.235        0.307         7.14       0.0744\n",
      "     56    50       0.0999       0.0928       0.0071        0.263        0.375         9.95        0.104\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     56     2       0.0661        0.065      0.00108        0.217        0.314         3.57       0.0372\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              56  182.406    0.002        0.102      0.00284        0.105        0.261        0.394         5.22       0.0544\n",
      "! Validation         56  182.406    0.002        0.121     0.000897        0.122        0.258        0.428         3.27       0.0341\n",
      "Wall time: 182.40685682099996\n",
      "! Best model       56    0.122\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     57    10        0.133        0.132      0.00136        0.309        0.447         4.36       0.0454\n",
      "     57    20        0.063        0.063     1.59e-06        0.219        0.309        0.152      0.00159\n",
      "     57    30        0.157        0.154      0.00314        0.325        0.483         6.62       0.0689\n",
      "     57    40        0.057       0.0463       0.0107        0.196        0.265         12.2        0.127\n",
      "     57    50        0.185        0.183      0.00204        0.328        0.527         5.34       0.0557\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     57     2       0.0656       0.0645       0.0011        0.215        0.313         3.61       0.0376\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              57  185.729    0.002        0.103      0.00305        0.106        0.259        0.394         5.25       0.0547\n",
      "! Validation         57  185.729    0.002         0.12      0.00092        0.121        0.256        0.427         3.33       0.0347\n",
      "Wall time: 185.72964921\n",
      "! Best model       57    0.121\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     58    10        0.159        0.159     0.000237        0.313        0.491         1.82       0.0189\n",
      "     58    20       0.0543       0.0538     0.000422        0.211        0.286         2.43       0.0253\n",
      "     58    30       0.0643       0.0638     0.000443        0.217        0.311         2.48       0.0259\n",
      "     58    40        0.098       0.0979     0.000142        0.261        0.385         1.41       0.0146\n",
      "     58    50        0.184        0.184     0.000427        0.306        0.527         2.44       0.0254\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     58     2       0.0653       0.0642       0.0011        0.215        0.312         3.61       0.0376\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              58  188.969    0.002       0.0993      0.00184        0.101        0.256        0.388         4.44       0.0462\n",
      "! Validation         58  188.969    0.002         0.12     0.000906        0.121        0.256        0.426         3.29       0.0343\n",
      "Wall time: 188.96947311800005\n",
      "! Best model       58    0.121\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     59    10        0.147        0.143      0.00433        0.307        0.465         7.78        0.081\n",
      "     59    20       0.0583       0.0491      0.00914        0.203        0.273         11.3        0.118\n",
      "     59    30       0.0565       0.0563     0.000233        0.219        0.292          1.8       0.0188\n",
      "     59    40       0.0844       0.0829      0.00153        0.262        0.354         4.62       0.0481\n",
      "     59    50       0.0434       0.0429     0.000424        0.198        0.255         2.43       0.0253\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     59     2       0.0649       0.0638       0.0011        0.215        0.311         3.61       0.0376\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              59  192.713    0.002        0.101      0.00197        0.103         0.26         0.39         4.26       0.0444\n",
      "! Validation         59  192.713    0.002        0.119     0.000897         0.12        0.255        0.425         3.26        0.034\n",
      "Wall time: 192.71432411799998\n",
      "! Best model       59    0.120\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     60    10        0.145        0.141      0.00328        0.305        0.463         6.77       0.0705\n",
      "     60    20        0.125        0.123       0.0027          0.3        0.431         6.14       0.0639\n",
      "     60    30       0.0628       0.0623     0.000553        0.221        0.307         2.78       0.0289\n",
      "     60    40        0.137         0.13      0.00646        0.315        0.444          9.5       0.0989\n",
      "     60    50       0.0718       0.0691      0.00278        0.229        0.323         6.23       0.0649\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     60     2       0.0644       0.0633      0.00111        0.214         0.31         3.62       0.0378\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              60  196.937    0.002        0.101       0.0037        0.105        0.262        0.392         5.86        0.061\n",
      "! Validation         60  196.937    0.002        0.119     0.000875        0.119        0.254        0.424         3.21       0.0334\n",
      "Wall time: 196.93777018400004\n",
      "! Best model       60    0.119\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     61    10        0.163        0.163     4.96e-05        0.318        0.497        0.828      0.00863\n",
      "     61    20        0.136        0.128      0.00776        0.312         0.44         10.4        0.108\n",
      "     61    30        0.109        0.103      0.00624        0.259        0.394         9.34       0.0972\n",
      "     61    40         0.12         0.12     7.21e-06        0.288        0.427        0.316       0.0033\n",
      "     61    50        0.132        0.129      0.00249        0.294        0.443          5.9       0.0614\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     61     2       0.0643       0.0632      0.00118        0.213        0.309         3.75       0.0391\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              61  201.139    0.002       0.0988      0.00553        0.104        0.257        0.387         7.31       0.0761\n",
      "! Validation         61  201.139    0.002        0.118     0.000917        0.119        0.253        0.423          3.3       0.0343\n",
      "Wall time: 201.13981182499998\n",
      "! Best model       61    0.119\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     62    10        0.119        0.118     0.000219        0.274        0.423         1.75       0.0182\n",
      "     62    20       0.0629       0.0626       0.0003        0.239        0.308         2.05       0.0214\n",
      "     62    30        0.122        0.119      0.00323        0.285        0.424         6.71       0.0699\n",
      "     62    40        0.131        0.131     8.61e-05        0.303        0.445         1.09       0.0114\n",
      "     62    50        0.116        0.114      0.00204        0.297        0.416         5.34       0.0557\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     62     2        0.064       0.0629      0.00117        0.213        0.309         3.74       0.0389\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              62  204.401    0.002        0.102       0.0022        0.105        0.264        0.394         4.39       0.0458\n",
      "! Validation         62  204.401    0.002        0.118     0.000926        0.119        0.253        0.422         3.32       0.0346\n",
      "Wall time: 204.40173415199996\n",
      "! Best model       62    0.119\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     63    10       0.0734       0.0661      0.00727        0.222        0.316         10.1        0.105\n",
      "     63    20        0.103       0.0978      0.00469        0.266        0.385         8.09       0.0843\n",
      "     63    30       0.0452       0.0452     3.34e-05        0.198        0.262         0.68      0.00708\n",
      "     63    40       0.0688       0.0656       0.0032        0.226        0.315         6.68       0.0696\n",
      "     63    50        0.139        0.138      0.00163        0.299        0.456         4.76       0.0496\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     63     2       0.0637       0.0625      0.00115        0.212        0.308         3.71       0.0386\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              63  207.862    0.002       0.0975      0.00441        0.102        0.255        0.384         6.58       0.0686\n",
      "! Validation         63  207.862    0.002        0.117      0.00091        0.118        0.252        0.421         3.29       0.0343\n",
      "Wall time: 207.86293744799997\n",
      "! Best model       63    0.118\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     64    10       0.0401       0.0383      0.00184        0.186        0.241         5.06       0.0527\n",
      "     64    20       0.0163       0.0121      0.00412       0.0895        0.136         7.58        0.079\n",
      "     64    30        0.139        0.139     8.61e-05        0.306        0.458         1.09       0.0114\n",
      "     64    40       0.0792       0.0787     0.000574        0.258        0.345         2.83       0.0295\n",
      "     64    50        0.049        0.048     0.000946        0.202         0.27         3.63       0.0378\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     64     2       0.0633       0.0622      0.00118        0.211        0.307         3.74        0.039\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              64  212.042    0.002        0.097      0.00156       0.0985        0.257        0.383         3.81       0.0397\n",
      "! Validation         64  212.042    0.002        0.117     0.000902        0.118        0.251         0.42         3.26       0.0339\n",
      "Wall time: 212.04323991400003\n",
      "! Best model       64    0.118\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     65    10       0.0595       0.0565      0.00296        0.208        0.293         6.43        0.067\n",
      "     65    20        0.114        0.112      0.00214        0.298        0.412         5.47        0.057\n",
      "     65    30       0.0816       0.0749      0.00678        0.249        0.337         9.73        0.101\n",
      "     65    40       0.0484       0.0474     0.000921          0.2        0.268         3.59       0.0374\n",
      "     65    50        0.115        0.114     0.000697         0.29        0.416         3.12       0.0325\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     65     2       0.0631       0.0619      0.00122        0.211        0.306         3.81       0.0397\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              65  216.654    0.002       0.0954      0.00225       0.0976        0.254         0.38         4.37       0.0456\n",
      "! Validation         65  216.654    0.002        0.116     0.000927        0.117         0.25         0.42          3.3       0.0344\n",
      "Wall time: 216.65564448900005\n",
      "! Best model       65    0.117\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     66    10        0.146        0.126       0.0197        0.291        0.437         16.6        0.173\n",
      "     66    20       0.0635       0.0594      0.00406        0.211          0.3         7.53       0.0784\n",
      "     66    30        0.165        0.161      0.00432        0.312        0.494         7.77       0.0809\n",
      "     66    40        0.115        0.115     1.66e-09        0.264        0.417      0.00781     8.14e-05\n",
      "     66    50       0.0517       0.0449      0.00683        0.193        0.261         9.77        0.102\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     66     2        0.063       0.0617      0.00121         0.21        0.306         3.79       0.0394\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              66  221.042    0.002       0.0965      0.00346       0.0999        0.254        0.382         5.83       0.0607\n",
      "! Validation         66  221.042    0.002        0.116     0.000919        0.117         0.25        0.419         3.28       0.0342\n",
      "Wall time: 221.04339900500003\n",
      "! Best model       66    0.117\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     67    10        0.138        0.138     0.000527        0.301        0.457         2.71       0.0282\n",
      "     67    20        0.042       0.0375      0.00443        0.177        0.238         7.86       0.0819\n",
      "     67    30        0.134        0.131      0.00361        0.278        0.445         7.09       0.0739\n",
      "     67    40        0.087       0.0851      0.00197        0.245        0.359         5.25       0.0546\n",
      "     67    50       0.0963       0.0962     3.73e-05         0.27        0.382        0.723      0.00753\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     67     2       0.0626       0.0615      0.00117         0.21        0.305         3.74        0.039\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              67  225.341    0.002        0.094      0.00185       0.0959        0.252        0.377         4.32        0.045\n",
      "! Validation         67  225.341    0.002        0.115      0.00094        0.116        0.249        0.418         3.36       0.0349\n",
      "Wall time: 225.342625316\n",
      "! Best model       67    0.116\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     68    10        0.046       0.0441      0.00191        0.188        0.258         5.16       0.0538\n",
      "     68    20        0.169        0.168       0.0007         0.31        0.505         3.12       0.0326\n",
      "     68    30       0.0387       0.0387     3.11e-05         0.18        0.242        0.656      0.00684\n",
      "     68    40        0.122        0.121     0.000666        0.276        0.429         3.05       0.0318\n",
      "     68    50       0.0795       0.0793     0.000253         0.24        0.346         1.88       0.0196\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     68     2       0.0622       0.0611      0.00118        0.209        0.304         3.75       0.0391\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              68  229.500    0.002       0.0905       0.0016       0.0921        0.245         0.37         3.75        0.039\n",
      "! Validation         68  229.500    0.002        0.115      0.00092        0.116        0.248        0.418          3.3       0.0344\n",
      "Wall time: 229.50120618800003\n",
      "! Best model       68    0.116\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     69    10        0.077       0.0758      0.00112        0.236        0.339         3.96       0.0412\n",
      "     69    20       0.0505       0.0443      0.00619        0.194        0.259         9.29       0.0968\n",
      "     69    30       0.0978       0.0948        0.003        0.251        0.379         6.48       0.0675\n",
      "     69    40       0.0451       0.0442     0.000849        0.195        0.259         3.44       0.0358\n",
      "     69    50       0.0133      0.00654      0.00675       0.0595       0.0995         9.71        0.101\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     69     2       0.0622        0.061      0.00118        0.209        0.304         3.74        0.039\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              69  233.623    0.002       0.0914      0.00266       0.0941        0.247        0.372            5       0.0521\n",
      "! Validation         69  233.623    0.002        0.115     0.000906        0.115        0.248        0.417         3.26        0.034\n",
      "Wall time: 233.62433340499996\n",
      "! Best model       69    0.115\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     70    10        0.109        0.109     0.000219        0.264        0.406         1.75       0.0182\n",
      "     70    20        0.053        0.045      0.00794        0.199        0.261         10.5         0.11\n",
      "     70    30        0.105        0.104     0.000553        0.285        0.397         2.78        0.029\n",
      "     70    40       0.0926       0.0912      0.00133        0.259        0.372         4.32        0.045\n",
      "     70    50        0.129        0.126      0.00293        0.277        0.437         6.39       0.0666\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     70     2       0.0618       0.0607      0.00115        0.208        0.303          3.7       0.0386\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              70  236.472    0.002       0.0904      0.00364        0.094        0.247         0.37         5.73       0.0597\n",
      "! Validation         70  236.472    0.002        0.115     0.000926        0.115        0.247        0.417         3.32       0.0346\n",
      "Wall time: 236.473413181\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     71    10        0.132        0.129       0.0025        0.285        0.443         5.91       0.0616\n",
      "     71    20        0.111        0.111      0.00031        0.289         0.41         2.08       0.0217\n",
      "     71    30       0.0927       0.0889      0.00385        0.257        0.367         7.33       0.0764\n",
      "     71    40        0.149        0.143      0.00569        0.321        0.466         8.91       0.0928\n",
      "     71    50       0.0689       0.0683     0.000622        0.227        0.322         2.95       0.0307\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     71     2       0.0614       0.0602      0.00115        0.207        0.302          3.7       0.0385\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              71  239.492    0.002       0.0929      0.00376       0.0967        0.252        0.375         6.31       0.0657\n",
      "! Validation         71  239.492    0.002        0.114     0.000926        0.115        0.246        0.416         3.33       0.0346\n",
      "Wall time: 239.49298204300004\n",
      "! Best model       71    0.115\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     72    10        0.121        0.119      0.00203        0.298        0.425         5.33       0.0555\n",
      "     72    20        0.123        0.122      0.00122        0.289        0.429         4.12       0.0429\n",
      "     72    30        0.114        0.111      0.00288        0.299         0.41         6.35       0.0661\n",
      "     72    40        0.051       0.0508      0.00015        0.191        0.277         1.45       0.0151\n",
      "     72    50       0.0843       0.0733       0.0109         0.22        0.333         12.3        0.129\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     72     2       0.0612         0.06      0.00118        0.207        0.302         3.75       0.0391\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              72  243.628    0.002       0.0906      0.00272       0.0934        0.246        0.371         4.89        0.051\n",
      "! Validation         72  243.628    0.002        0.113     0.000935        0.114        0.246        0.415         3.34       0.0348\n",
      "Wall time: 243.62915395\n",
      "! Best model       72    0.114\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     73    10       0.0803       0.0689       0.0114        0.237        0.323         12.6        0.132\n",
      "     73    20        0.096       0.0959     3.93e-05        0.273        0.381        0.738      0.00769\n",
      "     73    30       0.0671       0.0668     0.000302         0.22        0.318         2.05       0.0214\n",
      "     73    40         0.13         0.13     0.000133        0.295        0.443         1.36       0.0142\n",
      "     73    50      0.00971      0.00459      0.00512       0.0508       0.0834         8.45        0.088\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     73     2       0.0609       0.0597      0.00118        0.206        0.301         3.75        0.039\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              73  247.438    0.002       0.0901      0.00294       0.0931        0.248        0.369         4.99        0.052\n",
      "! Validation         73  247.438    0.002        0.113     0.000905        0.114        0.245        0.414         3.25       0.0339\n",
      "Wall time: 247.43858451800003\n",
      "! Best model       73    0.114\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     74    10       0.0772       0.0757       0.0015        0.234        0.339         4.58       0.0477\n",
      "     74    20        0.103        0.103     0.000218        0.284        0.394         1.75       0.0182\n",
      "     74    30        0.102          0.1      0.00164        0.256         0.39         4.79       0.0499\n",
      "     74    40        0.122        0.118      0.00326         0.27        0.423         6.74       0.0702\n",
      "     74    50       0.0152       0.0079      0.00729       0.0751        0.109         10.1        0.105\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     74     2       0.0609       0.0597      0.00118        0.206        0.301         3.74       0.0389\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              74  251.461    0.002       0.0902      0.00382        0.094        0.248         0.37         5.77       0.0601\n",
      "! Validation         74  251.461    0.002        0.113     0.000904        0.114        0.245        0.413         3.26       0.0339\n",
      "Wall time: 251.46365204299997\n",
      "! Best model       74    0.114\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     75    10       0.0744       0.0725      0.00194        0.242        0.331         5.21       0.0542\n",
      "     75    20       0.0915       0.0897       0.0018        0.257        0.369         5.01       0.0522\n",
      "     75    30        0.169        0.169     0.000761        0.326        0.505         3.26       0.0339\n",
      "     75    40        0.129        0.127      0.00234        0.299        0.438         5.71       0.0595\n",
      "     75    50        0.106       0.0851       0.0211        0.237        0.359         17.2        0.179\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     75     2       0.0606       0.0595      0.00113        0.205          0.3         3.68       0.0383\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              75  254.901    0.002       0.0899      0.00381       0.0937        0.249        0.369         5.64       0.0587\n",
      "! Validation         75  254.901    0.002        0.112     0.000926        0.113        0.244        0.412         3.34       0.0347\n",
      "Wall time: 254.90144146100005\n",
      "! Best model       75    0.113\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     76    10       0.0454       0.0451     0.000281        0.194        0.261         1.98       0.0207\n",
      "     76    20        0.108        0.104      0.00404        0.263        0.397         7.52       0.0783\n",
      "     76    30        0.105        0.104     0.000551        0.257        0.397         2.77       0.0289\n",
      "     76    40        0.151        0.149      0.00133        0.319        0.476          4.3       0.0448\n",
      "     76    50       0.0687        0.068      0.00073        0.227        0.321         3.19       0.0332\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     76     2       0.0604       0.0592      0.00115        0.204        0.299         3.69       0.0385\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              76  258.773    0.002       0.0897      0.00224        0.092        0.246        0.369         4.58       0.0477\n",
      "! Validation         76  258.773    0.002        0.112     0.000898        0.113        0.243        0.412         3.25       0.0339\n",
      "Wall time: 258.77384773800003\n",
      "! Best model       76    0.113\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     77    10        0.076       0.0728      0.00319         0.25        0.332         6.67       0.0695\n",
      "     77    20        0.108        0.107     0.000839        0.293        0.403         3.42       0.0356\n",
      "     77    30        0.157        0.152      0.00516        0.291        0.479         8.48       0.0884\n",
      "     77    40       0.0602       0.0561      0.00408        0.205        0.292         7.55       0.0786\n",
      "     77    50         0.13        0.129     0.000777        0.303        0.442          3.3       0.0343\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     77     2       0.0602        0.059      0.00117        0.204        0.299         3.71       0.0387\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              77  262.900    0.002       0.0877      0.00294       0.0906        0.243        0.364         5.34       0.0557\n",
      "! Validation         77  262.900    0.002        0.112     0.000895        0.113        0.243        0.412         3.23       0.0337\n",
      "Wall time: 262.90139860999994\n",
      "! Best model       77    0.113\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     78    10       0.0888       0.0885     0.000298        0.266        0.366         2.04       0.0212\n",
      "     78    20        0.119        0.118      0.00168        0.285        0.422         4.84       0.0505\n",
      "     78    30       0.0721       0.0716     0.000448        0.213        0.329          2.5       0.0261\n",
      "     78    40        0.102        0.102     0.000689        0.288        0.392          3.1       0.0323\n",
      "     78    50       0.0963       0.0943      0.00202        0.254        0.378         5.31       0.0553\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     78     2         0.06       0.0588      0.00113        0.204        0.299         3.67       0.0382\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              78  265.638    0.002        0.087       0.0027       0.0897        0.242        0.363         4.81       0.0501\n",
      "! Validation         78  265.638    0.002        0.112     0.000919        0.113        0.243        0.411         3.31       0.0345\n",
      "Wall time: 265.63836293900005\n",
      "! Best model       78    0.113\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     79    10       0.0383       0.0311      0.00718        0.154        0.217           10        0.104\n",
      "     79    20        0.109        0.109     0.000632        0.289        0.406         2.97        0.031\n",
      "     79    30       0.0462       0.0413      0.00489        0.189         0.25         8.26        0.086\n",
      "     79    40        0.154        0.154     2.52e-06          0.3        0.484        0.188      0.00195\n",
      "     79    50        0.104        0.101      0.00236         0.26        0.392         5.74       0.0598\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     79     2       0.0598       0.0587      0.00114        0.204        0.298         3.68       0.0383\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              79  268.562    0.002       0.0864      0.00195       0.0883         0.24        0.362         4.35       0.0454\n",
      "! Validation         79  268.562    0.002        0.111     0.000913        0.112        0.242        0.411         3.29       0.0343\n",
      "Wall time: 268.56306703499996\n",
      "! Best model       79    0.112\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     80    10        0.047        0.047        2e-07          0.2        0.267       0.0547      0.00057\n",
      "     80    20       0.0712       0.0668      0.00438        0.219        0.318         7.82       0.0815\n",
      "     80    30       0.0633       0.0489       0.0144        0.209        0.272         14.2        0.148\n",
      "     80    40        0.119        0.119     3.98e-06        0.282        0.424        0.234      0.00244\n",
      "     80    50        0.118        0.116      0.00164        0.263        0.419         4.78       0.0498\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     80     2       0.0599       0.0586      0.00122        0.203        0.298          3.8       0.0396\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              80  272.171    0.002        0.085      0.00207       0.0871         0.24        0.359         4.12       0.0429\n",
      "! Validation         80  272.171    0.002        0.111     0.000912        0.112        0.242         0.41         3.25       0.0339\n",
      "Wall time: 272.17217897999996\n",
      "! Best model       80    0.112\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     81    10        0.115        0.108      0.00675        0.261        0.405         9.71        0.101\n",
      "     81    20        0.111         0.11      0.00154         0.28        0.407         4.63       0.0482\n",
      "     81    30       0.0928       0.0909      0.00191        0.253        0.371         5.16       0.0538\n",
      "     81    40        0.167        0.151       0.0163        0.293        0.478         15.1        0.157\n",
      "     81    50       0.0865       0.0833      0.00315        0.256        0.355         6.64       0.0691\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     81     2       0.0596       0.0584       0.0012        0.202        0.297         3.78       0.0394\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              81  274.640    0.002        0.085      0.00406       0.0891        0.241        0.359         6.44       0.0671\n",
      "! Validation         81  274.640    0.002        0.111     0.000936        0.112        0.241        0.409         3.33       0.0347\n",
      "Wall time: 274.64057749299997\n",
      "! Best model       81    0.112\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     82    10        0.102        0.101     0.000158        0.266        0.392         1.48       0.0155\n",
      "     82    20        0.154        0.154     0.000358        0.304        0.482         2.23       0.0233\n",
      "     82    30        0.088       0.0878     0.000171        0.246        0.365         1.54       0.0161\n",
      "     82    40        0.089       0.0841      0.00486        0.233        0.357         8.24       0.0858\n",
      "     82    50        0.121        0.121     0.000302        0.282        0.428         2.05       0.0214\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     82     2       0.0593       0.0581      0.00121        0.202        0.297         3.82       0.0398\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              82  278.139    0.002       0.0851      0.00294        0.088        0.242        0.359         4.93       0.0513\n",
      "! Validation         82  278.139    0.002         0.11     0.000963        0.111         0.24        0.409          3.4       0.0354\n",
      "Wall time: 278.14027765400004\n",
      "! Best model       82    0.111\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     83    10        0.105        0.105     4.14e-08         0.29        0.398       0.0234     0.000244\n",
      "     83    20       0.0744       0.0732      0.00119        0.218        0.333         4.07       0.0424\n",
      "     83    30        0.105        0.102      0.00311        0.269        0.393         6.59       0.0686\n",
      "     83    40        0.113        0.112     0.000565        0.273        0.413         2.81       0.0293\n",
      "     83    50        0.105        0.105     5.48e-05        0.268        0.399        0.875      0.00911\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     83     2       0.0592        0.058       0.0012        0.202        0.296         3.81       0.0397\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              83  280.577    0.002       0.0862      0.00215       0.0883        0.243        0.361         4.39       0.0458\n",
      "! Validation         83  280.577    0.002         0.11     0.000968        0.111         0.24        0.408         3.42       0.0356\n",
      "Wall time: 280.57750752\n",
      "! Best model       83    0.111\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     84    10       0.0584       0.0584     5.98e-07        0.208        0.297       0.0938     0.000977\n",
      "     84    20       0.0848       0.0848     2.23e-05        0.246        0.358        0.559      0.00582\n",
      "     84    30         0.11        0.108      0.00179        0.255        0.405            5       0.0521\n",
      "     84    40        0.156        0.136       0.0202        0.317        0.453         16.8        0.175\n",
      "     84    50       0.0722       0.0687      0.00351        0.235        0.323            7       0.0729\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     84     2       0.0592        0.058      0.00121        0.201        0.296         3.81       0.0397\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              84  283.583    0.002       0.0866      0.00513       0.0917        0.244        0.362         6.82       0.0711\n",
      "! Validation         84  283.583    0.002         0.11     0.000963        0.111         0.24        0.408          3.4       0.0354\n",
      "Wall time: 283.58443281800004\n",
      "! Best model       84    0.111\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     85    10        0.121        0.108       0.0128        0.268        0.405         13.3        0.139\n",
      "     85    20       0.0515       0.0515     2.31e-05        0.217        0.279         0.57      0.00594\n",
      "     85    30       0.0908       0.0904     0.000364        0.259         0.37         2.25       0.0235\n",
      "     85    40       0.0599       0.0579      0.00199        0.223        0.296         5.27       0.0549\n",
      "     85    50       0.0409       0.0309      0.00995        0.159        0.216         11.8        0.123\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     85     2       0.0589       0.0577      0.00123        0.201        0.296         3.85       0.0401\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              85  286.506    0.002       0.0874      0.00328       0.0906        0.246        0.364         5.18        0.054\n",
      "! Validation         85  286.506    0.002        0.109     0.000955         0.11        0.239        0.407         3.37       0.0351\n",
      "Wall time: 286.50666537\n",
      "! Best model       85    0.110\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     86    10       0.0354        0.035     0.000444        0.155         0.23         2.49        0.026\n",
      "     86    20        0.081       0.0806     0.000377        0.245        0.349         2.29       0.0239\n",
      "     86    30        0.137        0.136      0.00119        0.315        0.453         4.07       0.0424\n",
      "     86    40       0.0498        0.041       0.0088        0.188        0.249         11.1        0.115\n",
      "     86    50       0.0556        0.042       0.0136         0.19        0.252         13.8        0.143\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     86     2       0.0587       0.0575      0.00122        0.201        0.295         3.81       0.0397\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              86  290.005    0.002       0.0932      0.00277        0.096        0.258        0.376         5.16       0.0538\n",
      "! Validation         86  290.005    0.002        0.109     0.000937         0.11        0.239        0.406         3.33       0.0347\n",
      "Wall time: 290.00522132500004\n",
      "! Best model       86    0.110\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     87    10        0.134        0.125      0.00809        0.287        0.436         10.6        0.111\n",
      "     87    20        0.107       0.0987      0.00824        0.278        0.387         10.7        0.112\n",
      "     87    30        0.107        0.104      0.00345        0.284        0.396         6.95       0.0723\n",
      "     87    40        0.103        0.102      0.00156        0.255        0.393         4.67       0.0487\n",
      "     87    50       0.0855       0.0811      0.00438        0.254         0.35         7.82       0.0814\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     87     2       0.0584       0.0572      0.00116          0.2        0.294         3.71       0.0387\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              87  294.609    0.002       0.0836      0.00328       0.0868        0.238        0.356         5.56       0.0579\n",
      "! Validation         87  294.609    0.002        0.109     0.000904        0.109        0.238        0.405         3.27       0.0341\n",
      "Wall time: 294.609814611\n",
      "! Best model       87    0.109\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     88    10        0.102       0.0999      0.00204        0.254        0.389         5.34       0.0556\n",
      "     88    20       0.0517       0.0497      0.00204        0.211        0.274         5.33       0.0555\n",
      "     88    30       0.0844       0.0727       0.0118        0.234        0.332         12.8        0.133\n",
      "     88    40        0.098       0.0933      0.00467        0.279        0.376         8.07       0.0841\n",
      "     88    50        0.113        0.108      0.00494        0.265        0.405          8.3       0.0865\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     88     2       0.0581        0.057       0.0011          0.2        0.294         3.64       0.0379\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              88  298.613    0.002       0.0828      0.00312        0.086        0.238        0.354         5.33       0.0555\n",
      "! Validation         88  298.613    0.002        0.108     0.000927        0.109        0.238        0.405         3.34       0.0348\n",
      "Wall time: 298.613473255\n",
      "! Best model       88    0.109\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     89    10       0.0433       0.0413      0.00192        0.186         0.25         5.18        0.054\n",
      "     89    20        0.051       0.0505      0.00054        0.193        0.276         2.75       0.0286\n",
      "     89    30        0.151         0.15     0.000807        0.292        0.477         3.36        0.035\n",
      "     89    40       0.0994       0.0994     1.22e-05        0.273        0.388        0.414      0.00431\n",
      "     89    50       0.0974       0.0958      0.00155        0.273        0.381         4.65       0.0485\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     89     2       0.0577       0.0566      0.00115        0.199        0.293         3.72       0.0387\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              89  302.232    0.002       0.0826      0.00288       0.0855        0.237        0.354         5.04       0.0525\n",
      "! Validation         89  302.232    0.002        0.108     0.000927        0.109        0.238        0.404         3.33       0.0347\n",
      "Wall time: 302.232373546\n",
      "! Best model       89    0.109\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     90    10        0.111         0.11      0.00129        0.288        0.408         4.24       0.0442\n",
      "     90    20       0.0483       0.0437      0.00458        0.198        0.257            8       0.0833\n",
      "     90    30       0.0454       0.0375      0.00793        0.174        0.238         10.5         0.11\n",
      "     90    40       0.0867       0.0857      0.00101        0.249         0.36         3.75       0.0391\n",
      "     90    50       0.0635       0.0603       0.0032        0.238        0.302         6.68       0.0696\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     90     2       0.0576       0.0564      0.00117        0.199        0.292         3.74       0.0389\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              90  305.596    0.002       0.0842        0.002       0.0862        0.242        0.357         4.59       0.0478\n",
      "! Validation         90  305.596    0.002        0.108     0.000915        0.109        0.237        0.404         3.29       0.0343\n",
      "Wall time: 305.59735885000003\n",
      "! Best model       90    0.109\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     91    10       0.0763       0.0741      0.00222        0.229        0.335         5.56       0.0579\n",
      "     91    20       0.0356       0.0299       0.0057        0.159        0.213         8.92       0.0929\n",
      "     91    30        0.141        0.141     0.000653        0.284        0.461         3.02       0.0315\n",
      "     91    40        0.102        0.101     0.000292         0.28        0.392         2.02        0.021\n",
      "     91    50        0.103        0.103     0.000777         0.28        0.394         3.29       0.0343\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     91     2       0.0574       0.0563      0.00112        0.198        0.292         3.67       0.0382\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              91  309.203    0.002       0.0813      0.00179       0.0831        0.236        0.351         4.29       0.0447\n",
      "! Validation         91  309.203    0.002        0.107     0.000921        0.108        0.237        0.403         3.32       0.0346\n",
      "Wall time: 309.203594871\n",
      "! Best model       91    0.108\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     92    10       0.0403       0.0355       0.0048        0.176        0.232         8.18       0.0852\n",
      "     92    20       0.0972       0.0972     9.22e-05        0.256        0.384         1.13       0.0118\n",
      "     92    30          0.1       0.0989      0.00126        0.276        0.387          4.2       0.0437\n",
      "     92    40       0.0443         0.04      0.00433        0.186        0.246         7.77       0.0809\n",
      "     92    50       0.0406       0.0305       0.0101        0.164        0.215         11.9        0.124\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     92     2       0.0575       0.0563      0.00116        0.198        0.292         3.72       0.0388\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              92  312.939    0.002       0.0815      0.00213       0.0836        0.237        0.351         4.62       0.0481\n",
      "! Validation         92  312.939    0.002        0.107     0.000922        0.108        0.237        0.402         3.31       0.0345\n",
      "Wall time: 312.94078639799994\n",
      "! Best model       92    0.108\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     93    10        0.107        0.104      0.00293        0.284        0.397          6.4       0.0667\n",
      "     93    20       0.0279       0.0201      0.00778        0.118        0.174         10.4        0.109\n",
      "     93    30        0.123        0.118      0.00458        0.307        0.424            8       0.0833\n",
      "     93    40        0.118        0.118     1.11e-05        0.291        0.423        0.391      0.00407\n",
      "     93    50       0.0513       0.0493      0.00197        0.205        0.273         5.25       0.0547\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     93     2       0.0572       0.0561      0.00112        0.198        0.291         3.66       0.0382\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              93  316.012    0.002       0.0848      0.00267       0.0875        0.242        0.358          5.1       0.0532\n",
      "! Validation         93  316.012    0.002        0.107     0.000928        0.108        0.236        0.402         3.35       0.0349\n",
      "Wall time: 316.012175432\n",
      "! Best model       93    0.108\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     94    10        0.135        0.131      0.00433        0.296        0.446         7.78       0.0811\n",
      "     94    20       0.0996       0.0995     0.000123        0.262        0.388         1.31       0.0137\n",
      "     94    30       0.0586       0.0548      0.00374        0.198        0.288         7.23       0.0753\n",
      "     94    40        0.125        0.118       0.0064        0.261        0.423         9.45       0.0985\n",
      "     94    50       0.0997       0.0979      0.00177         0.27        0.385         4.97       0.0518\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     94     2       0.0569       0.0558      0.00114        0.197        0.291         3.69       0.0384\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              94  319.716    0.002       0.0818       0.0032        0.085        0.236        0.352         5.48       0.0571\n",
      "! Validation         94  319.716    0.002        0.107     0.000887        0.107        0.236        0.402         3.23       0.0336\n",
      "Wall time: 319.717390382\n",
      "! Best model       94    0.107\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     95    10       0.0477       0.0451      0.00255        0.192        0.261         5.96       0.0621\n",
      "     95    20       0.0802       0.0776      0.00255        0.231        0.343         5.97       0.0622\n",
      "     95    30       0.0428       0.0427     0.000161        0.185        0.254          1.5       0.0156\n",
      "     95    40        0.103        0.101      0.00134        0.261        0.392         4.33       0.0451\n",
      "     95    50       0.0561       0.0539      0.00229        0.202        0.286         5.65       0.0589\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     95     2       0.0569       0.0558      0.00111        0.197        0.291         3.65        0.038\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              95  323.073    0.002       0.0824      0.00192       0.0843        0.239        0.353         4.06       0.0423\n",
      "! Validation         95  323.073    0.002        0.106     0.000905        0.107        0.235        0.401         3.28       0.0342\n",
      "Wall time: 323.07377398600005\n",
      "! Best model       95    0.107\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     96    10        0.071        0.071     5.61e-05        0.228        0.328        0.883       0.0092\n",
      "     96    20       0.0448       0.0445      0.00021        0.194         0.26         1.71       0.0178\n",
      "     96    30       0.0518       0.0515     0.000302        0.199        0.279         2.05       0.0214\n",
      "     96    40       0.0957       0.0956     5.61e-05        0.269        0.381        0.883       0.0092\n",
      "     96    50       0.0388       0.0291      0.00968         0.16         0.21         11.6        0.121\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     96     2       0.0568       0.0557      0.00116        0.197         0.29         3.71       0.0387\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              96  326.120    0.002       0.0802      0.00242       0.0826        0.235        0.349         4.74       0.0494\n",
      "! Validation         96  326.120    0.002        0.106     0.000905        0.107        0.235          0.4         3.26        0.034\n",
      "Wall time: 326.12148293700005\n",
      "! Best model       96    0.107\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     97    10       0.0994       0.0975      0.00197         0.25        0.384         5.24       0.0546\n",
      "     97    20        0.107        0.101      0.00573        0.264        0.391         8.95       0.0932\n",
      "     97    30       0.0638       0.0635     0.000318        0.227         0.31         2.11       0.0219\n",
      "     97    40        0.106        0.103      0.00315        0.273        0.394         6.63       0.0691\n",
      "     97    50       0.0838       0.0832     0.000602        0.242        0.355          2.9       0.0302\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     97     2       0.0565       0.0554      0.00113        0.196         0.29         3.68       0.0383\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              97  329.099    0.002       0.0782      0.00243       0.0806         0.23        0.344         5.05       0.0526\n",
      "! Validation         97  329.099    0.002        0.105     0.000927        0.106        0.234        0.399         3.33       0.0347\n",
      "Wall time: 329.10414417\n",
      "! Best model       97    0.106\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     98    10       0.0782       0.0778     0.000422        0.236        0.343         2.43       0.0253\n",
      "     98    20       0.0697       0.0692     0.000486        0.237        0.324          2.6       0.0271\n",
      "     98    30        0.054       0.0478      0.00618        0.201        0.269         9.29       0.0968\n",
      "     98    40       0.0461       0.0378      0.00832        0.179        0.239         10.8        0.112\n",
      "     98    50        0.063       0.0584      0.00458        0.226        0.297            8       0.0833\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     98     2       0.0565       0.0554      0.00111        0.196         0.29         3.64        0.038\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              98  332.646    0.002       0.0816      0.00289       0.0845        0.239        0.352         5.06       0.0527\n",
      "! Validation         98  332.646    0.002        0.105     0.000898        0.106        0.234        0.399         3.27        0.034\n",
      "Wall time: 332.647567984\n",
      "! Best model       98    0.106\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     99    10        0.159        0.157      0.00154        0.317        0.488         4.64       0.0483\n",
      "     99    20       0.0911       0.0911     6.69e-05         0.25        0.371        0.965       0.0101\n",
      "     99    30        0.152        0.152     3.98e-06        0.307        0.479        0.238      0.00248\n",
      "     99    40       0.0949       0.0904      0.00451        0.249         0.37         7.94       0.0827\n",
      "     99    50        0.129        0.129     0.000163        0.291        0.442         1.51       0.0157\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     99     2       0.0564       0.0552      0.00114        0.196        0.289         3.69       0.0384\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              99  336.750    0.002       0.0819      0.00298       0.0849        0.241        0.352         5.51       0.0574\n",
      "! Validation         99  336.750    0.002        0.105     0.000914        0.106        0.234        0.399          3.3       0.0344\n",
      "Wall time: 336.751333986\n",
      "! Best model       99    0.106\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "    100    10        0.105        0.103      0.00239        0.269        0.394         5.78       0.0602\n",
      "    100    20        0.103        0.103     0.000202        0.283        0.394         1.68       0.0175\n",
      "    100    30        0.105        0.105     0.000302        0.258        0.399         2.05       0.0214\n",
      "    100    40       0.0931       0.0927     0.000429        0.255        0.375         2.45       0.0255\n",
      "    100    50        0.104       0.0981       0.0061        0.253        0.385         9.23       0.0962\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "    100     2       0.0561        0.055      0.00113        0.196        0.289         3.66       0.0382\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train             100  340.320    0.002       0.0812      0.00231       0.0835        0.237        0.351          4.6       0.0479\n",
      "! Validation        100  340.320    0.002        0.105     0.000881        0.106        0.234        0.398         3.21       0.0334\n",
      "Wall time: 340.320187676\n",
      "! Best model      100    0.106\n",
      "! Stop training: max epochs\n",
      "Wall time: 340.328558953\n",
      "Cumulative wall time: 340.328558953\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./results1000\n",
    "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27720fb-b1c6-409e-981a-a0185203babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-11.8'\n",
      "Using device: cpu\n",
      "Loading dataset... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
      "    loading dataset took 0.0340s\n",
      "    loaded dataset of size 500 and sampled --n-data=2 frames\n",
      "    benchmark frames statistics:\n",
      "         number of atoms: 96\n",
      "         number of types: 5\n",
      "          avg. num edges: 2420.0\n",
      "         avg. neigh/atom: 25.20833396911621\n",
      "Building model... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "    building model took 0.3706s\n",
      "    model has 37608 weights\n",
      "    model has 37608 trainable weights\n",
      "    model weights and buffers take 0.14 MB\n",
      "Compile...\n",
      "    compilation took 0.2089s\n",
      "Warmup...\n",
      "    6 calls of warmup took 0.5132s\n",
      "Benchmarking...\n"
     ]
    }
   ],
   "source": [
    "!nequip-benchmark allegro/configs/tutorial.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3deb5ca1-ac25-49d8-a285-ccac3910932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trainer = torch.load(\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
      "    loaded model\n",
      "Loading original dataset...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
      "Loaded dataset specified in config.yaml.\n",
      "Using origial training dataset (110 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 50 frames.\n",
      "Starting...\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "  2%|‚ñâ                                           | 1/50 [00:00<00:08,  5.53it/s]\n",
      "  4%|‚ñà‚ñä                                          | 2/50 [00:00<00:17,  2.80it/s]\n",
      "  6%|‚ñà‚ñà‚ñã                                         | 3/50 [00:00<00:15,  3.01it/s]\n",
      "f_mae = 0.0742 | f_rmse = 0.1016 | e_mae = 0.1096 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0748 | f_rmse = 0.1011 | e_mae = 0.1012 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0704 | f_rmse = 0.0957 | e_mae = 0.1024 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0670 | f_rmse = 0.0912 | e_mae = 0.1006 | e/N_mae = 0.0016\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 8/50 [00:01<00:03, 10.55it/s]\n",
      "f_mae = 0.0660 | f_rmse = 0.0895 | e_mae = 0.1005 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0648 | f_rmse = 0.0878 | e_mae = 0.1101 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0636 | f_rmse = 0.0860 | e_mae = 0.1082 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0636 | f_rmse = 0.0858 | e_mae = 0.1162 | e/N_mae = 0.0018\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 13/50 [00:01<00:02, 17.36it/s]\n",
      "f_mae = 0.0625 | f_rmse = 0.0837 | e_mae = 0.1016 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0622 | f_rmse = 0.0830 | e_mae = 0.1013 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0614 | f_rmse = 0.0818 | e_mae = 0.0999 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0615 | f_rmse = 0.0818 | e_mae = 0.0962 | e/N_mae = 0.0015\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 18/50 [00:01<00:01, 23.14it/s]\n",
      "f_mae = 0.0602 | f_rmse = 0.0800 | e_mae = 0.0940 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0791 | e_mae = 0.0927 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0591 | f_rmse = 0.0784 | e_mae = 0.0898 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0590 | f_rmse = 0.0780 | e_mae = 0.0881 | e/N_mae = 0.0014\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 23/50 [00:01<00:00, 28.96it/s]\n",
      "f_mae = 0.0597 | f_rmse = 0.0790 | e_mae = 0.0946 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0786 | e_mae = 0.0937 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0599 | f_rmse = 0.0788 | e_mae = 0.0987 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0600 | f_rmse = 0.0788 | e_mae = 0.0974 | e/N_mae = 0.0015\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 28/50 [00:01<00:00, 33.80it/s]\n",
      "f_mae = 0.0598 | f_rmse = 0.0784 | e_mae = 0.1002 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0783 | e_mae = 0.1018 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0598 | f_rmse = 0.0783 | e_mae = 0.1035 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0780 | e_mae = 0.1042 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0779 | e_mae = 0.1030 | e/N_mae = 0.0016\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 34/50 [00:01<00:00, 39.13it/s]\n",
      "f_mae = 0.0598 | f_rmse = 0.0779 | e_mae = 0.0987 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0777 | e_mae = 0.1000 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0778 | e_mae = 0.0995 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0776 | e_mae = 0.0977 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0776 | e_mae = 0.0953 | e/N_mae = 0.0015\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 40/50 [00:01<00:00, 42.69it/s]\n",
      "f_mae = 0.0593 | f_rmse = 0.0771 | e_mae = 0.0931 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0592 | f_rmse = 0.0769 | e_mae = 0.0911 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0590 | f_rmse = 0.0766 | e_mae = 0.0900 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0589 | f_rmse = 0.0764 | e_mae = 0.0884 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0588 | f_rmse = 0.0762 | e_mae = 0.0880 | e/N_mae = 0.0014\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 46/50 [00:01<00:00, 45.18it/s]\n",
      "f_mae = 0.0587 | f_rmse = 0.0760 | e_mae = 0.0851 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0587 | f_rmse = 0.0759 | e_mae = 0.0834 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0586 | f_rmse = 0.0757 | e_mae = 0.0834 | e/N_mae = 0.0013\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 25.73it/s]\n",
      "f_mae = 0.0586 | f_rmse = 0.0757 | e_mae = 0.0844 | e/N_mae = 0.0013\n",
      "\n",
      "--- Final result: ---\n",
      "               f_mae =  0.058591           \n",
      "              f_rmse =  0.075679           \n",
      "               e_mae =  0.084355           \n",
      "             e/N_mae =  0.001318           \n",
      "               f_mae =  0.058591           \n",
      "              f_rmse =  0.075679           \n",
      "               e_mae =  0.084355           \n",
      "             e/N_mae =  0.001318           \n"
     ]
    }
   ],
   "source": [
    "!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bafb171a-54f9-481f-aa20-eb89fdbd707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "INFO:root:Loading best_model.pth from training session...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "INFO:root:Compiled & optimized model.\n",
      "model1000K.pth\tsi-deployed.pth\n"
     ]
    }
   ],
   "source": [
    "!nequip-deploy build --train-dir results/silicon-tutorial/si model1000K.pth\n",
    "!ls *pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fcae3-c291-400b-9dc4-4077cfb4f91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nequip_tt",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
