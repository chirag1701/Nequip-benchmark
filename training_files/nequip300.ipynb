{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6aaabb-7c4e-4656-abe1-734c8b5633fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nequip\n",
    "import wandb\n",
    "import allegro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e55e4c-7e6a-4768-b436-07b8a5d2ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchirag17\u001b[0m (\u001b[33mchirag17-indian-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ec2310-a86b-4623-bd20-33ea0c29d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.size'] = 30\n",
    "\n",
    "def parse_lammps_rdf(rdffile):\n",
    "    \"\"\"Parse the RDF file written by LAMMPS\n",
    "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
    "    \"\"\"\n",
    "    with open(rdffile, 'r') as rdfout:\n",
    "        rdfs = []; buffer = []\n",
    "        for line in rdfout:\n",
    "            values = line.split()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elif len(values) == 2:\n",
    "                nbins = values[1]\n",
    "            else:\n",
    "                buffer.append([float(values[1]), float(values[2])])\n",
    "                if len(buffer) == int(nbins):\n",
    "                    frame = np.transpose(np.array(buffer))\n",
    "                    rdfs.append(frame)\n",
    "                    buffer = []\n",
    "    return rdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b6c2c7-cc84-41d5-b9be-ba41bb0ca0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapi-300.extxyz\n"
     ]
    }
   ],
   "source": [
    "!ls temp_data300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de28c63-585b-46c3-9a25-c5b317897427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/train.py:180: UserWarning: default_dtype=float32 but we strongly recommend float64\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchirag17\u001b[0m (\u001b[33mchirag17-indian-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/chirag/wandb/run-20241017_102515-RRaAE9w7AkUDitWbcjkN5EK6tvnzZULoQ8aa47kO_yM\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33matT300\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial/runs/RRaAE9w7AkUDitWbcjkN5EK6tvnzZULoQ8aa47kO_yM\u001b[0m\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  datas = [torch.load(d) for d in datas]\n",
      "Loaded data: Batch(atomic_numbers=[48000, 1], batch=[48000], cell=[500, 3, 3], edge_cell_shift=[1205598, 3], edge_index=[2, 1205598], forces=[48000, 3], free_energy=[500], pbc=[500, 3], pos=[48000, 3], ptr=[501], total_energy=[500, 1])\n",
      "    processed data size: ~34.05 MB\n",
      "Cached processed data to disk\n",
      "Done!\n",
      "Successfully loaded the data set of type ASEDataset(500)...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Replace string dataset_per_atom_total_energy_mean to -609.0789794921875\n",
      "Atomic outputs are scaled by: [Pb, I, N, C, H: None], shifted by [Pb, I, N, C, H: -609.078979].\n",
      "Replace string dataset_forces_rms to 0.6629341840744019\n",
      "Initially outputs are globally scaled by: 0.6629341840744019, total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Equivariance test passed; equivariance errors:\n",
      "   Errors are in real units, where relevant.\n",
      "   Please note that the large scale of the typical\n",
      "   shifts to the (atomic) energy can cause\n",
      "   catastrophic cancellation and give incorrectly\n",
      "   the equivariance error as zero for those fields.\n",
      "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
      "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_embedding             -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_cutoff                -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_features              -> max error=3.576e-07\n",
      "   edge permutation equivariance of field edge_energy                -> max error=8.941e-08\n",
      "   node permutation equivariance of field atomic_energy              -> max error=0.000e+00\n",
      "   edge & node permutation invariance for field total_energy         -> max error=3.906e-03\n",
      "   node permutation equivariance of field forces                     -> max error=1.997e-06\n",
      "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=5.329e-07\n",
      "   (parity_k=0, did_translate=False, field=cell                )     -> max error=4.638e-07\n",
      "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=1.436e-05\n",
      "   (parity_k=0, did_translate=False, field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=8.562e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=2.086e-05\n",
      "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=4.143e-06\n",
      "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=forces              )     -> max error=7.243e-06\n",
      "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=4.757e-07\n",
      "   (parity_k=0, did_translate=True , field=cell                )     -> max error=4.570e-07\n",
      "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=1.130e-05\n",
      "   (parity_k=0, did_translate=True , field=edge_cutoff         )     -> max error=6.676e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=2.406e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=7.629e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=2.397e-06\n",
      "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=forces              )     -> max error=1.203e-05\n",
      "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=4.815e-07\n",
      "   (parity_k=1, did_translate=False, field=cell                )     -> max error=4.739e-07\n",
      "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=1.035e-05\n",
      "   (parity_k=1, did_translate=False, field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.787e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=5.603e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=2.056e-06\n",
      "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=forces              )     -> max error=1.636e-05\n",
      "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=9.102e-07\n",
      "   (parity_k=1, did_translate=True , field=cell                )     -> max error=4.708e-07\n",
      "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=1.017e-05\n",
      "   (parity_k=1, did_translate=True , field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=4.931e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=1.311e-05\n",
      "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=2.414e-06\n",
      "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=6.104e-05\n",
      "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=forces              )     -> max error=6.455e-06\n",
      "Number of weights: 37608\n",
      "Number of trainable weights: 37608\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      0     2          1.1          1.1      0.00268        0.507        0.695         3.24       0.0337\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Initial Validation          0    0.227    0.002         1.14      0.00294         1.14        0.507        0.708         3.41       0.0355\n",
      "Wall time: 0.22718644299993684\n",
      "! Best model        0    1.144\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1    10        0.955        0.954      0.00149        0.447        0.647         2.45       0.0256\n",
      "      1    20        0.911        0.909      0.00104        0.433        0.632         2.05       0.0214\n",
      "      1    30        0.764        0.762      0.00161        0.415        0.579         2.55       0.0266\n",
      "      1    40        0.564        0.563      0.00104        0.345        0.498         2.05       0.0214\n",
      "      1    50        0.405        0.405     1.59e-05        0.302        0.422         0.25       0.0026\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1     2        0.457        0.456     0.000111         0.33        0.448        0.521      0.00543\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               1    3.874    0.002        0.718      0.00102        0.719        0.391        0.562         1.65       0.0172\n",
      "! Validation          1    3.874    0.002        0.469     0.000113        0.469        0.328        0.454        0.586       0.0061\n",
      "Wall time: 3.875138178000043\n",
      "! Best model        1    0.469\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      2    10        0.418        0.412      0.00604        0.299        0.426         4.95       0.0516\n",
      "      2    20        0.246        0.246     8.01e-05        0.248        0.329        0.566       0.0059\n",
      "      2    30        0.292        0.288      0.00367        0.268        0.356         3.86       0.0402\n",
      "      2    40        0.284        0.284     0.000105        0.244        0.353        0.648      0.00675\n",
      "      2    50        0.296        0.296     1.17e-05        0.255        0.361        0.219      0.00228\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      2     2        0.302        0.302     0.000149        0.258        0.364        0.653       0.0068\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               2    6.385    0.002        0.319      0.00149         0.32        0.268        0.374         1.92         0.02\n",
      "! Validation          2    6.385    0.002        0.298     8.03e-05        0.298        0.255        0.362        0.423      0.00441\n",
      "Wall time: 6.3855699329999425\n",
      "! Best model        2    0.298\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      3    10        0.255        0.255     1.59e-05        0.247        0.335        0.254      0.00264\n",
      "      3    20        0.274        0.274     0.000476        0.257        0.347         1.39       0.0145\n",
      "      3    30        0.252        0.251     0.000966        0.239        0.332         1.98       0.0206\n",
      "      3    40         0.24        0.236      0.00443        0.226        0.322         4.24       0.0441\n",
      "      3    50        0.228        0.227     0.000757        0.233        0.316         1.75       0.0182\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      3     2        0.279        0.279     0.000101        0.246         0.35        0.596      0.00621\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               3    8.896    0.002        0.271      0.00119        0.272        0.243        0.345         1.73       0.0181\n",
      "! Validation          3    8.896    0.002        0.274     6.35e-05        0.274        0.243        0.347         0.44      0.00458\n",
      "Wall time: 8.896275351999975\n",
      "! Best model        3    0.274\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      4    10        0.223        0.223     8.91e-05        0.236        0.313        0.602      0.00627\n",
      "      4    20        0.201        0.201     9.86e-05        0.217        0.297        0.633      0.00659\n",
      "      4    30        0.261        0.261     1.66e-07        0.227        0.339       0.0273     0.000285\n",
      "      4    40        0.205        0.203      0.00127         0.21        0.299         2.27       0.0236\n",
      "      4    50        0.209        0.209     0.000388        0.216        0.303         1.25       0.0131\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      4     2        0.252        0.252      9.6e-05        0.233        0.333        0.541      0.00564\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               4   11.442    0.002        0.247      0.00135        0.248        0.231         0.33         1.98       0.0206\n",
      "! Validation          4   11.442    0.002        0.249     8.93e-05        0.249        0.231        0.331        0.534      0.00557\n",
      "Wall time: 11.442888824999955\n",
      "! Best model        4    0.249\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      5    10        0.337        0.337     0.000124        0.257        0.385        0.711      0.00741\n",
      "      5    20        0.265        0.265     0.000739        0.244        0.341         1.73        0.018\n",
      "      5    30        0.224        0.224     0.000115         0.23        0.314        0.684      0.00712\n",
      "      5    40        0.171        0.171     0.000404          0.2        0.274         1.28       0.0133\n",
      "      5    50        0.281        0.281      0.00027        0.251        0.351         1.05       0.0109\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      5     2        0.221        0.221     8.44e-05         0.22        0.312        0.556      0.00579\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               5   14.030    0.002        0.216     0.000673        0.216        0.216        0.308         1.36       0.0141\n",
      "! Validation          5   14.030    0.002        0.218     5.69e-05        0.218        0.217         0.31        0.425      0.00443\n",
      "Wall time: 14.030574074000015\n",
      "! Best model        5    0.218\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      6    10        0.262        0.261      0.00116        0.235        0.339         2.16       0.0225\n",
      "      6    20        0.166        0.166     1.06e-05        0.201         0.27        0.207      0.00216\n",
      "      6    30        0.168        0.164      0.00461        0.178        0.268         4.32        0.045\n",
      "      6    40        0.147        0.146     8.91e-05        0.182        0.254        0.602      0.00627\n",
      "      6    50        0.165        0.164     0.000519        0.197        0.269         1.45       0.0151\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      6     2        0.183        0.183     7.73e-05        0.204        0.284        0.545      0.00567\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               6   16.425    0.002        0.178      0.00126        0.179        0.199        0.279         1.89       0.0197\n",
      "! Validation          6   16.425    0.002        0.181     4.75e-05        0.181        0.201        0.282        0.377      0.00393\n",
      "Wall time: 16.425358322999955\n",
      "! Best model        6    0.181\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      7    10        0.166        0.165      0.00124        0.205        0.269         2.24       0.0233\n",
      "      7    20        0.177        0.171      0.00565        0.209        0.274         4.79       0.0498\n",
      "      7    30        0.173        0.169      0.00437          0.2        0.273         4.21       0.0438\n",
      "      7    40        0.121        0.118      0.00265        0.166        0.228         3.28       0.0341\n",
      "      7    50        0.168        0.167     0.000497        0.207        0.271         1.42       0.0148\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      7     2         0.15         0.15     7.09e-05        0.188        0.257         0.52      0.00541\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               7   19.341    0.002        0.149       0.0021        0.151        0.187        0.256         2.45       0.0255\n",
      "! Validation          7   19.341    0.002        0.149     4.53e-05        0.149        0.186        0.256        0.367      0.00382\n",
      "Wall time: 19.341528613000037\n",
      "! Best model        7    0.149\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      8    10        0.103        0.103     0.000161        0.161        0.212        0.809      0.00842\n",
      "      8    20        0.122        0.121      0.00135        0.165        0.231         2.33       0.0243\n",
      "      8    30        0.118        0.117      0.00127        0.171        0.227         2.27       0.0236\n",
      "      8    40         0.13        0.127      0.00288        0.182        0.236         3.41       0.0355\n",
      "      8    50        0.127        0.127     2.08e-05        0.177        0.237        0.289      0.00301\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      8     2        0.129        0.129     5.74e-05        0.176        0.238        0.462      0.00481\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               8   21.938    0.002        0.127      0.00204        0.129        0.174        0.236         2.51       0.0261\n",
      "! Validation          8   21.938    0.002        0.128     3.83e-05        0.128        0.175        0.237         0.33      0.00344\n",
      "Wall time: 21.93852620100006\n",
      "! Best model        8    0.128\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      9    10        0.122        0.122     0.000596        0.168        0.231         1.55       0.0162\n",
      "      9    20        0.115        0.115     1.66e-07        0.169        0.225       0.0234     0.000244\n",
      "      9    30        0.156        0.144       0.0124        0.189        0.252         7.08       0.0737\n",
      "      9    40        0.101        0.097      0.00426        0.158        0.207         4.16       0.0433\n",
      "      9    50        0.162         0.16      0.00145        0.194        0.266         2.43       0.0253\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      9     2        0.115        0.115     6.53e-05        0.167        0.224        0.408      0.00425\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               9   24.780    0.002        0.114      0.00204        0.116        0.166        0.224         2.32       0.0242\n",
      "! Validation          9   24.780    0.002        0.114     4.99e-05        0.115        0.166        0.224         0.37      0.00385\n",
      "Wall time: 24.780595682000012\n",
      "! Best model        9    0.115\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     10    10        0.121        0.115      0.00636        0.171        0.225         5.08       0.0529\n",
      "     10    20        0.127        0.125      0.00238        0.179        0.234          3.1       0.0323\n",
      "     10    30        0.124        0.123     0.000912        0.177        0.232         1.92         0.02\n",
      "     10    40        0.128        0.128     0.000353        0.177        0.237          1.2       0.0125\n",
      "     10    50       0.0737       0.0737     2.92e-06        0.131         0.18        0.105       0.0011\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     10     2        0.105        0.105     5.93e-05         0.16        0.214        0.391      0.00407\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              10   27.470    0.002        0.105      0.00176        0.106        0.159        0.214         2.13       0.0222\n",
      "! Validation         10   27.470    0.002        0.105     4.62e-05        0.105        0.159        0.215        0.359      0.00374\n",
      "Wall time: 27.470121140999936\n",
      "! Best model       10    0.105\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     11    10        0.105        0.105     7.21e-06         0.16        0.215        0.172      0.00179\n",
      "     11    20      0.00667       0.0064     0.000273       0.0374        0.053         1.05       0.0109\n",
      "     11    30        0.114        0.114     7.21e-06        0.169        0.224        0.172      0.00179\n",
      "     11    40       0.0818       0.0813     0.000501        0.146        0.189         1.42       0.0148\n",
      "     11    50       0.0956       0.0949     0.000695        0.155        0.204         1.68       0.0175\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     11     2       0.0969       0.0969     6.52e-05        0.154        0.206        0.398      0.00415\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              11   30.023    0.002       0.0956     0.000306       0.0959        0.152        0.205        0.914      0.00952\n",
      "! Validation         11   30.023    0.002       0.0978     5.41e-05       0.0978        0.154        0.207        0.384        0.004\n",
      "Wall time: 30.023551389999966\n",
      "! Best model       11    0.098\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     12    10        0.126        0.122      0.00394         0.17        0.232         3.99       0.0416\n",
      "     12    20       0.0959       0.0956     0.000303        0.157        0.205         1.11       0.0115\n",
      "     12    30        0.104          0.1       0.0034        0.159         0.21         3.71       0.0386\n",
      "     12    40       0.0861        0.085      0.00107        0.147        0.193         2.08       0.0217\n",
      "     12    50         0.05         0.05     7.66e-06          0.1        0.148        0.176      0.00183\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     12     2       0.0907       0.0906     5.53e-05        0.149          0.2        0.372      0.00387\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              12   32.760    0.002       0.0901      0.00128       0.0914        0.148        0.199         1.89       0.0197\n",
      "! Validation         12   32.760    0.002       0.0918      4.4e-05       0.0919        0.149        0.201        0.348      0.00362\n",
      "Wall time: 32.760365958999955\n",
      "! Best model       12    0.092\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     13    10       0.0151       0.0149     0.000185       0.0584        0.081        0.867      0.00903\n",
      "     13    20        0.111         0.11     0.000417        0.164         0.22          1.3       0.0135\n",
      "     13    30       0.0612       0.0607     0.000476         0.12        0.163         1.39       0.0145\n",
      "     13    40       0.0862       0.0861     7.03e-05        0.147        0.195        0.535      0.00557\n",
      "     13    50        0.102        0.101     0.000553         0.16        0.211          1.5       0.0156\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     13     2       0.0856       0.0855     4.66e-05        0.145        0.194        0.349      0.00364\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              13   35.542    0.002       0.0838     0.000448       0.0843        0.143        0.192         1.14       0.0118\n",
      "! Validation         13   35.542    0.002       0.0869     3.58e-05       0.0869        0.145        0.195        0.312      0.00325\n",
      "Wall time: 35.54334195699994\n",
      "! Best model       13    0.087\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     14    10        0.076        0.075     0.000951        0.137        0.182         1.96       0.0205\n",
      "     14    20        0.101       0.0967      0.00393        0.159        0.206         3.99       0.0415\n",
      "     14    30       0.0835       0.0835     2.23e-05        0.145        0.192        0.301      0.00313\n",
      "     14    40       0.0932       0.0931     0.000147        0.152        0.202        0.773      0.00806\n",
      "     14    50       0.0832       0.0829     0.000341        0.136        0.191         1.18       0.0122\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     14     2       0.0809       0.0808     4.49e-05         0.14        0.188        0.348      0.00363\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              14   39.431    0.002       0.0798     0.000798       0.0806        0.139        0.187         1.49       0.0155\n",
      "! Validation         14   39.431    0.002       0.0824     3.36e-05       0.0824        0.141         0.19        0.304      0.00317\n",
      "Wall time: 39.431310725\n",
      "! Best model       14    0.082\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     15    10       0.0885       0.0874       0.0011         0.15        0.196         2.11        0.022\n",
      "     15    20       0.0715       0.0709     0.000674        0.131        0.176         1.66       0.0173\n",
      "     15    30        0.064       0.0633     0.000748        0.126        0.167         1.74       0.0181\n",
      "     15    40       0.0487       0.0481     0.000538        0.108        0.145         1.48       0.0154\n",
      "     15    50       0.0684       0.0683     0.000163        0.131        0.173        0.812      0.00846\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     15     2       0.0769       0.0769     4.43e-05        0.137        0.184        0.334      0.00347\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              15   42.482    0.002       0.0752     0.000641       0.0758        0.135        0.182          1.4       0.0146\n",
      "! Validation         15   42.482    0.002       0.0785     3.33e-05       0.0785        0.138        0.186        0.295      0.00307\n",
      "Wall time: 42.4831193519999\n",
      "! Best model       15    0.079\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     16    10       0.0492       0.0467      0.00245        0.107        0.143         3.15       0.0328\n",
      "     16    20       0.0687       0.0678     0.000858        0.126        0.173         1.86       0.0194\n",
      "     16    30       0.0702       0.0699     0.000292         0.13        0.175         1.09       0.0113\n",
      "     16    40       0.0792       0.0792     7.58e-05        0.141        0.187        0.555      0.00578\n",
      "     16    50       0.0734       0.0733     5.48e-05        0.135        0.179        0.473      0.00492\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     16     2       0.0734       0.0734     4.32e-05        0.134         0.18        0.325      0.00339\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              16   45.247    0.002         0.07     0.000898       0.0709         0.13        0.175         1.69       0.0176\n",
      "! Validation         16   45.247    0.002        0.075     3.29e-05        0.075        0.135        0.182        0.291      0.00303\n",
      "Wall time: 45.248227239000016\n",
      "! Best model       16    0.075\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     17    10       0.0677       0.0677     4.45e-05         0.13        0.172        0.426      0.00444\n",
      "     17    20       0.0804       0.0791      0.00135        0.143        0.186         2.34       0.0243\n",
      "     17    30       0.0663        0.066     0.000375        0.127         0.17         1.23       0.0129\n",
      "     17    40       0.0532       0.0531     0.000126         0.11        0.153        0.719      0.00749\n",
      "     17    50       0.0596       0.0594     0.000215        0.121        0.162        0.934      0.00972\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     17     2       0.0702       0.0702     4.58e-05         0.13        0.176         0.33      0.00344\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              17   49.210    0.002       0.0672     0.000637       0.0678        0.127        0.172         1.33       0.0139\n",
      "! Validation         17   49.210    0.002       0.0718     3.49e-05       0.0719        0.132        0.178        0.296      0.00309\n",
      "Wall time: 49.21072192199995\n",
      "! Best model       17    0.072\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     18    10       0.0987       0.0986     7.03e-05        0.158        0.208        0.535      0.00557\n",
      "     18    20       0.0674       0.0652      0.00223        0.123        0.169            3       0.0313\n",
      "     18    30       0.0669       0.0664     0.000572        0.127        0.171         1.52       0.0159\n",
      "     18    40        0.061       0.0605     0.000546        0.123        0.163         1.48       0.0155\n",
      "     18    50       0.0477       0.0474     0.000315        0.107        0.144         1.13       0.0118\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     18     2       0.0675       0.0674     3.49e-05        0.128        0.172        0.289      0.00301\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              18   51.648    0.002       0.0634     0.000518        0.064        0.124        0.167         1.14       0.0119\n",
      "! Validation         18   51.648    0.002        0.069     2.68e-05       0.0691        0.129        0.174         0.26      0.00271\n",
      "Wall time: 51.648405351000065\n",
      "! Best model       18    0.069\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     19    10       0.0622       0.0622      1.3e-06        0.127        0.165       0.0703     0.000732\n",
      "     19    20       0.0559        0.053      0.00289        0.113        0.153         3.42       0.0356\n",
      "     19    30       0.0833       0.0824     0.000971        0.139         0.19         1.98       0.0207\n",
      "     19    40       0.0776       0.0776     1.22e-05         0.14        0.185        0.223      0.00232\n",
      "     19    50       0.0648       0.0641     0.000752        0.126        0.168         1.75       0.0182\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     19     2       0.0649       0.0648     3.37e-05        0.125        0.169        0.287      0.00299\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              19   54.291    0.002       0.0621     0.000659       0.0628        0.123        0.165         1.32       0.0138\n",
      "! Validation         19   54.291    0.002       0.0664     2.58e-05       0.0665        0.127        0.171        0.255      0.00266\n",
      "Wall time: 54.29182740900001\n",
      "! Best model       19    0.066\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     20    10       0.0626       0.0619     0.000757        0.127        0.165         1.75       0.0182\n",
      "     20    20       0.0542       0.0534     0.000811        0.114        0.153         1.81       0.0189\n",
      "     20    30       0.0552       0.0551     0.000115        0.116        0.156        0.684      0.00712\n",
      "     20    40       0.0952       0.0947     0.000534        0.144        0.204         1.47       0.0153\n",
      "     20    50        0.048       0.0478     0.000196        0.111        0.145        0.891      0.00928\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     20     2       0.0626       0.0626     3.03e-05        0.123        0.166        0.279      0.00291\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              20   56.896    0.002       0.0591     0.000752       0.0599        0.119        0.161         1.44        0.015\n",
      "! Validation         20   56.896    0.002       0.0642     2.28e-05       0.0642        0.124        0.168        0.236      0.00246\n",
      "Wall time: 56.896131506000074\n",
      "! Best model       20    0.064\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     21    10       0.0518       0.0512     0.000576        0.115         0.15         1.53       0.0159\n",
      "     21    20       0.0516       0.0513     0.000284        0.114         0.15         1.07       0.0112\n",
      "     21    30       0.0655       0.0653     0.000135        0.128        0.169        0.742      0.00773\n",
      "     21    40       0.0659       0.0658     1.93e-05        0.129         0.17        0.281      0.00293\n",
      "     21    50       0.0558        0.055     0.000887        0.116        0.155         1.89       0.0197\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     21     2       0.0606       0.0606     2.51e-05        0.121        0.163        0.252      0.00263\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              21   59.505    0.002       0.0579     0.000462       0.0583        0.118        0.159         1.15        0.012\n",
      "! Validation         21   59.505    0.002       0.0621     1.93e-05       0.0622        0.122        0.165        0.218      0.00227\n",
      "Wall time: 59.50528892900002\n",
      "! Best model       21    0.062\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     22    10       0.0643       0.0643     4.13e-05        0.124        0.168        0.406      0.00423\n",
      "     22    20       0.0415       0.0413     0.000219       0.0986        0.135        0.945      0.00985\n",
      "     22    30       0.0515       0.0509     0.000519        0.111         0.15         1.45       0.0151\n",
      "     22    40       0.0533       0.0529     0.000363        0.117        0.152         1.21       0.0126\n",
      "     22    50       0.0605       0.0603     0.000174        0.123        0.163         0.84      0.00875\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     22     2       0.0588       0.0588     2.48e-05        0.119        0.161        0.251      0.00261\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              22   62.171    0.002       0.0546     0.000229       0.0549        0.114        0.155        0.835       0.0087\n",
      "! Validation         22   62.171    0.002       0.0603     1.86e-05       0.0604         0.12        0.163        0.212      0.00221\n",
      "Wall time: 62.17213652600003\n",
      "! Best model       22    0.060\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     23    10       0.0401        0.039      0.00105        0.104        0.131         2.06       0.0215\n",
      "     23    20       0.0491       0.0428       0.0063        0.105        0.137         5.05       0.0526\n",
      "     23    30       0.0512       0.0512     5.25e-05        0.113         0.15        0.461       0.0048\n",
      "     23    40       0.0346       0.0345     0.000143       0.0947        0.123        0.758      0.00789\n",
      "     23    50       0.0475       0.0473     0.000232        0.108        0.144        0.969       0.0101\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     23     2       0.0571       0.0571     2.31e-05        0.117        0.158        0.236      0.00246\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              23   64.760    0.002       0.0536      0.00114       0.0548        0.113        0.154         1.68       0.0175\n",
      "! Validation         23   64.760    0.002       0.0586     1.76e-05       0.0587        0.118        0.161        0.204      0.00213\n",
      "Wall time: 64.760777835\n",
      "! Best model       23    0.059\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     24    10       0.0769       0.0756      0.00128         0.13        0.182         2.28       0.0237\n",
      "     24    20       0.0575       0.0574     0.000132        0.118        0.159         0.73      0.00761\n",
      "     24    30       0.0428       0.0428     1.46e-05       0.0988        0.137        0.246      0.00256\n",
      "     24    40       0.0451        0.045     5.85e-05        0.105        0.141        0.484      0.00505\n",
      "     24    50       0.0509       0.0504     0.000497        0.113        0.149         1.42       0.0148\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     24     2       0.0558       0.0557     1.78e-05        0.116        0.157        0.218      0.00227\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              24   67.346    0.002       0.0525     0.000412        0.053        0.112        0.152         1.07       0.0111\n",
      "! Validation         24   67.346    0.002       0.0573     1.29e-05       0.0573        0.117        0.159        0.175      0.00182\n",
      "Wall time: 67.34626884799991\n",
      "! Best model       24    0.057\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     25    10       0.0508       0.0498     0.000982        0.108        0.148         1.99       0.0208\n",
      "     25    20       0.0542        0.051       0.0032        0.112         0.15          3.6       0.0375\n",
      "     25    30       0.0573       0.0569     0.000335         0.12        0.158         1.16       0.0121\n",
      "     25    40        0.045       0.0449     6.89e-05        0.103         0.14        0.531      0.00553\n",
      "     25    50       0.0477       0.0476      6.1e-05        0.109        0.145          0.5      0.00521\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     25     2       0.0544       0.0544     1.66e-05        0.114        0.155        0.212      0.00221\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              25   69.930    0.002       0.0515     0.000752       0.0523        0.111         0.15          1.4       0.0146\n",
      "! Validation         25   69.930    0.002       0.0559     1.19e-05       0.0559        0.115        0.157        0.162      0.00169\n",
      "Wall time: 69.93098311099993\n",
      "! Best model       25    0.056\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     26    10       0.0369       0.0366     0.000309       0.0995        0.127         1.12       0.0116\n",
      "     26    20       0.0486       0.0482     0.000378         0.11        0.146         1.24       0.0129\n",
      "     26    30       0.0648       0.0646     0.000254        0.128        0.168         1.02       0.0106\n",
      "     26    40       0.0519       0.0488      0.00311        0.111        0.146         3.55        0.037\n",
      "     26    50       0.0486       0.0478     0.000743        0.112        0.145         1.74       0.0181\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     26     2       0.0532       0.0531     1.61e-05        0.113        0.153         0.21      0.00219\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              26   72.602    0.002       0.0495     0.000499         0.05        0.109        0.148         1.13       0.0117\n",
      "! Validation         26   72.602    0.002       0.0546     1.16e-05       0.0546        0.114        0.155         0.17      0.00177\n",
      "Wall time: 72.60320884199996\n",
      "! Best model       26    0.055\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     27    10       0.0571       0.0548      0.00225        0.112        0.155         3.02       0.0315\n",
      "     27    20        0.061       0.0587      0.00231         0.12        0.161         3.05       0.0318\n",
      "     27    30       0.0589       0.0587     0.000232        0.125        0.161        0.969       0.0101\n",
      "     27    40       0.0457       0.0446      0.00111        0.109         0.14         2.12       0.0221\n",
      "     27    50       0.0666       0.0663     0.000344        0.123        0.171         1.18       0.0123\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     27     2       0.0521       0.0521     1.29e-05        0.112        0.151        0.191      0.00199\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              27   75.232    0.002       0.0491      0.00101       0.0501        0.108        0.147         1.77       0.0185\n",
      "! Validation         27   75.232    0.002       0.0536     9.85e-06       0.0536        0.113        0.153        0.157      0.00164\n",
      "Wall time: 75.23210237900003\n",
      "! Best model       27    0.054\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     28    10       0.0448       0.0445      0.00027        0.101         0.14         1.05       0.0109\n",
      "     28    20       0.0431       0.0427     0.000335       0.0987        0.137         1.17       0.0122\n",
      "     28    30       0.0618       0.0618     4.78e-05        0.122        0.165        0.438      0.00456\n",
      "     28    40       0.0528       0.0526     0.000237        0.117        0.152         0.98       0.0102\n",
      "     28    50      0.00571      0.00567     3.93e-05       0.0276       0.0499        0.398      0.00415\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     28     2       0.0511       0.0511     1.21e-05        0.111         0.15        0.192        0.002\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              28   77.732    0.002       0.0476     0.000206       0.0478        0.106        0.145        0.702      0.00731\n",
      "! Validation         28   77.732    0.002       0.0525     8.95e-06       0.0525        0.111        0.152        0.153       0.0016\n",
      "Wall time: 77.73276986699989\n",
      "! Best model       28    0.053\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     29    10       0.0478       0.0474     0.000479        0.107        0.144         1.39       0.0145\n",
      "     29    20       0.0632       0.0613      0.00198        0.125        0.164         2.83       0.0295\n",
      "     29    30       0.0478       0.0476     0.000155        0.105        0.145        0.797       0.0083\n",
      "     29    40       0.0431        0.043      0.00011       0.0986        0.138        0.664      0.00692\n",
      "     29    50        0.052       0.0519     5.85e-05         0.11        0.151        0.484      0.00505\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     29     2       0.0502       0.0502     1.05e-05         0.11        0.149        0.177      0.00185\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              29   80.313    0.002       0.0471     0.000476       0.0476        0.106        0.144         1.17       0.0122\n",
      "! Validation         29   80.313    0.002       0.0516     7.46e-06       0.0516         0.11        0.151        0.138      0.00144\n",
      "Wall time: 80.31351888799986\n",
      "! Best model       29    0.052\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     30    10       0.0594       0.0593     0.000137         0.12        0.161        0.746      0.00777\n",
      "     30    20       0.0452       0.0446     0.000616        0.104         0.14         1.58       0.0164\n",
      "     30    30       0.0341       0.0341     1.66e-07       0.0961        0.122       0.0234     0.000244\n",
      "     30    40       0.0471       0.0464     0.000766        0.107        0.143         1.76       0.0184\n",
      "     30    50       0.0486       0.0485     6.49e-05         0.11        0.146        0.512      0.00533\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     30     2       0.0495       0.0495     8.79e-06        0.109        0.147        0.166      0.00173\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              30   82.851    0.002       0.0468     0.000555       0.0474        0.106        0.143         1.22       0.0127\n",
      "! Validation         30   82.851    0.002       0.0509     6.16e-06       0.0509         0.11         0.15        0.127      0.00132\n",
      "Wall time: 82.85133402399993\n",
      "! Best model       30    0.051\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     31    10       0.0363       0.0361     0.000119       0.0939        0.126        0.691       0.0072\n",
      "     31    20       0.0447       0.0445     0.000227          0.1         0.14        0.957      0.00997\n",
      "     31    30       0.0535        0.053     0.000538        0.113        0.153         1.47       0.0153\n",
      "     31    40       0.0402       0.0397     0.000441       0.0987        0.132         1.34       0.0139\n",
      "     31    50       0.0456        0.045     0.000542        0.104        0.141         1.48       0.0155\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     31     2       0.0487       0.0487     8.13e-06        0.108        0.146        0.163       0.0017\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              31   85.349    0.002        0.046     0.000356       0.0463        0.105        0.142        0.983       0.0102\n",
      "! Validation         31   85.349    0.002         0.05     5.84e-06         0.05        0.108        0.148         0.13      0.00136\n",
      "Wall time: 85.34940708499994\n",
      "! Best model       31    0.050\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     32    10       0.0686       0.0679     0.000784        0.126        0.173         1.78       0.0186\n",
      "     32    20       0.0487       0.0481     0.000612        0.105        0.145         1.57       0.0164\n",
      "     32    30        0.045       0.0445     0.000424        0.105         0.14         1.31       0.0137\n",
      "     32    40       0.0579       0.0578     6.36e-05        0.115        0.159        0.508      0.00529\n",
      "     32    50       0.0447       0.0443     0.000394       0.0997         0.14         1.26       0.0131\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     32     2        0.048        0.048     8.82e-06        0.107        0.145        0.167      0.00174\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              32   87.833    0.002       0.0453     0.000497       0.0458        0.104        0.141         1.16       0.0121\n",
      "! Validation         32   87.833    0.002       0.0493     6.13e-06       0.0493        0.108        0.147        0.129      0.00135\n",
      "Wall time: 87.83413425599997\n",
      "! Best model       32    0.049\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     33    10       0.0432       0.0432     3.21e-06        0.102        0.138        0.113      0.00118\n",
      "     33    20       0.0506       0.0504     0.000135        0.111        0.149        0.742      0.00773\n",
      "     33    30       0.0374        0.037     0.000338       0.0937        0.128         1.17       0.0122\n",
      "     33    40       0.0411       0.0408     0.000363       0.0968        0.134         1.21       0.0126\n",
      "     33    50       0.0542       0.0539     0.000298        0.116        0.154          1.1       0.0114\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     33     2       0.0472       0.0472     6.79e-06        0.106        0.144        0.152      0.00158\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              33   90.279    0.002       0.0448     0.000439       0.0453        0.103         0.14         1.13       0.0117\n",
      "! Validation         33   90.279    0.002       0.0485     4.68e-06       0.0485        0.107        0.146        0.118      0.00123\n",
      "Wall time: 90.27922453099995\n",
      "! Best model       33    0.049\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     34    10       0.0641        0.064      7.3e-05         0.12        0.168        0.543      0.00566\n",
      "     34    20        0.038       0.0373     0.000691       0.0998        0.128         1.68       0.0175\n",
      "     34    30        0.038        0.038     5.19e-06       0.0953        0.129        0.145      0.00151\n",
      "     34    40       0.0388       0.0388     2.92e-06       0.0957        0.131        0.109      0.00114\n",
      "     34    50       0.0435       0.0433     0.000227       0.0981        0.138        0.957      0.00997\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     34     2       0.0467       0.0466     8.38e-06        0.106        0.143        0.161      0.00168\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              34   92.710    0.002       0.0444     0.000224       0.0446        0.102         0.14        0.777      0.00809\n",
      "! Validation         34   92.710    0.002       0.0479      5.8e-06       0.0479        0.106        0.145        0.126      0.00131\n",
      "Wall time: 92.713230028\n",
      "! Best model       34    0.048\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     35    10       0.0613       0.0608     0.000472        0.125        0.164         1.38       0.0144\n",
      "     35    20       0.0537       0.0537     2.23e-05        0.115        0.154        0.297      0.00309\n",
      "     35    30       0.0312       0.0312     1.06e-05        0.092        0.117        0.207      0.00216\n",
      "     35    40       0.0432       0.0393      0.00394       0.0943        0.131            4       0.0416\n",
      "     35    50       0.0588       0.0588     5.57e-06        0.119        0.161        0.152      0.00159\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     35     2       0.0462       0.0462     6.95e-06        0.105        0.142        0.148      0.00154\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              35   96.170    0.002       0.0446     0.000918       0.0455        0.103         0.14         1.57       0.0164\n",
      "! Validation         35   96.170    0.002       0.0474     4.68e-06       0.0474        0.105        0.144        0.115       0.0012\n",
      "Wall time: 96.17110213499996\n",
      "! Best model       35    0.047\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     36    10       0.0243       0.0243     1.49e-06       0.0793        0.103       0.0781     0.000814\n",
      "     36    20       0.0389       0.0389     1.46e-05        0.102        0.131        0.246      0.00256\n",
      "     36    30       0.0517       0.0512     0.000592        0.109         0.15         1.55       0.0161\n",
      "     36    40       0.0364       0.0364     1.66e-07       0.0946        0.126       0.0273     0.000285\n",
      "     36    50       0.0491       0.0484     0.000734        0.112        0.146         1.72       0.0179\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     36     2       0.0457       0.0457     5.66e-06        0.105        0.142        0.135      0.00141\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              36   99.853    0.002       0.0427     0.000248       0.0429        0.101        0.137        0.821      0.00855\n",
      "! Validation         36   99.853    0.002       0.0469     4.05e-06       0.0469        0.105        0.143        0.112      0.00116\n",
      "Wall time: 99.85368417099994\n",
      "! Best model       36    0.047\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     37    10       0.0525       0.0507      0.00174        0.116        0.149         2.65       0.0276\n",
      "     37    20       0.0593       0.0593     1.66e-07        0.115        0.161       0.0273     0.000285\n",
      "     37    30       0.0452       0.0451     7.44e-05        0.105        0.141        0.551      0.00574\n",
      "     37    40       0.0547       0.0547     3.81e-06        0.112        0.155        0.125       0.0013\n",
      "     37    50       0.0518       0.0516     0.000174        0.113        0.151         0.84      0.00875\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     37     2       0.0451       0.0451     6.43e-06        0.104        0.141        0.141      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              37  102.174    0.002       0.0425     0.000392       0.0429        0.101        0.137         1.04       0.0109\n",
      "! Validation         37  102.174    0.002       0.0463     4.59e-06       0.0463        0.104        0.143        0.116       0.0012\n",
      "Wall time: 102.17472062599995\n",
      "! Best model       37    0.046\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     38    10       0.0313       0.0312     8.11e-06       0.0912        0.117         0.18      0.00187\n",
      "     38    20       0.0395       0.0392     0.000249       0.0939        0.131         1.01       0.0105\n",
      "     38    30       0.0394       0.0387     0.000704       0.0938         0.13         1.69       0.0176\n",
      "     38    40       0.0497       0.0497     3.93e-05        0.109        0.148        0.398      0.00415\n",
      "     38    50       0.0368       0.0367     0.000176        0.094        0.127        0.848      0.00883\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     38     2       0.0446       0.0446     5.98e-06        0.103         0.14        0.132      0.00138\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              38  104.505    0.002        0.042     0.000232       0.0422          0.1        0.136        0.739       0.0077\n",
      "! Validation         38  104.505    0.002       0.0457      4.2e-06       0.0457        0.104        0.142        0.109      0.00114\n",
      "Wall time: 104.50578732200006\n",
      "! Best model       38    0.046\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     39    10        0.036        0.036     6.62e-09       0.0924        0.126      0.00391     4.07e-05\n",
      "     39    20       0.0484       0.0472      0.00118         0.11        0.144         2.18       0.0227\n",
      "     39    30       0.0269       0.0269     1.72e-05       0.0839        0.109        0.262      0.00273\n",
      "     39    40       0.0435       0.0434     9.54e-05          0.1        0.138        0.625      0.00651\n",
      "     39    50       0.0284       0.0282     0.000147       0.0855        0.111         0.77      0.00802\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     39     2       0.0442       0.0442     7.45e-06        0.103        0.139        0.152      0.00158\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              39  106.734    0.002       0.0416     0.000351        0.042       0.0995        0.135        0.966       0.0101\n",
      "! Validation         39  106.734    0.002       0.0452     5.32e-06       0.0452        0.103        0.141        0.123      0.00128\n",
      "Wall time: 106.73495652799988\n",
      "! Best model       39    0.045\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     40    10       0.0312       0.0312     1.01e-05       0.0857        0.117        0.199      0.00208\n",
      "     40    20       0.0406       0.0398     0.000757       0.0978        0.132         1.75       0.0182\n",
      "     40    30       0.0257       0.0256     5.61e-05       0.0816        0.106        0.477      0.00496\n",
      "     40    40       0.0387       0.0373      0.00137       0.0918        0.128         2.35       0.0245\n",
      "     40    50        0.052       0.0506      0.00138        0.112        0.149         2.37       0.0247\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     40     2       0.0437       0.0437     4.98e-06        0.103        0.139        0.127      0.00132\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              40  109.178    0.002       0.0416     0.000587       0.0422       0.0997        0.135         1.31       0.0137\n",
      "! Validation         40  109.178    0.002       0.0448     3.51e-06       0.0448        0.102         0.14        0.106       0.0011\n",
      "Wall time: 109.17903025500004\n",
      "! Best model       40    0.045\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     41    10        0.046        0.046     4.67e-05        0.106        0.142        0.434      0.00452\n",
      "     41    20       0.0355       0.0355     1.53e-05       0.0973        0.125        0.246      0.00256\n",
      "     41    30        0.029       0.0284     0.000641       0.0823        0.112         1.61       0.0168\n",
      "     41    40       0.0355       0.0354     8.16e-05       0.0961        0.125        0.574      0.00598\n",
      "     41    50       0.0429        0.042     0.000892       0.0977        0.136          1.9       0.0198\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     41     2       0.0433       0.0433     6.73e-06        0.102        0.138        0.141      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              41  111.699    0.002       0.0412     0.000469       0.0416       0.0992        0.134         1.23       0.0128\n",
      "! Validation         41  111.699    0.002       0.0443     4.66e-06       0.0443        0.102         0.14        0.114      0.00119\n",
      "Wall time: 111.69951252400006\n",
      "! Best model       41    0.044\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     42    10       0.0495       0.0495     2.97e-05        0.108        0.148        0.348      0.00362\n",
      "     42    20       0.0418       0.0413     0.000497       0.0982        0.135         1.42       0.0148\n",
      "     42    30       0.0534       0.0532     0.000132        0.112        0.153        0.727      0.00757\n",
      "     42    40        0.034       0.0339     3.06e-05       0.0891        0.122        0.352      0.00366\n",
      "     42    50       0.0436       0.0435     0.000153        0.103        0.138        0.789      0.00822\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     42     2       0.0429       0.0429     6.03e-06        0.101        0.137        0.131      0.00137\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              42  114.406    0.002       0.0402     0.000232       0.0404       0.0975        0.133        0.796       0.0083\n",
      "! Validation         42  114.406    0.002       0.0439     4.32e-06       0.0439        0.101        0.139        0.111      0.00115\n",
      "Wall time: 114.40700569699993\n",
      "! Best model       42    0.044\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     43    10        0.038       0.0376     0.000375        0.091        0.129         1.23       0.0128\n",
      "     43    20       0.0243       0.0241     0.000143       0.0743        0.103        0.762      0.00793\n",
      "     43    30       0.0349       0.0338      0.00106        0.091        0.122         2.08       0.0216\n",
      "     43    40       0.0485       0.0484     0.000117        0.108        0.146        0.688      0.00716\n",
      "     43    50       0.0351       0.0348     0.000292       0.0906        0.124         1.09       0.0114\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     43     2       0.0425       0.0425     6.49e-06        0.101        0.137         0.14      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              43  116.799    0.002       0.0394     0.000356       0.0397       0.0964        0.132            1       0.0104\n",
      "! Validation         43  116.799    0.002       0.0435     4.39e-06       0.0435        0.101        0.138        0.113      0.00118\n",
      "Wall time: 116.80028762500001\n",
      "! Best model       43    0.043\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     44    10        0.033        0.033     1.11e-05       0.0889         0.12        0.215      0.00224\n",
      "     44    20       0.0399       0.0399     1.34e-05       0.0985        0.132        0.234      0.00244\n",
      "     44    30       0.0498       0.0497      0.00017        0.109        0.148        0.832      0.00867\n",
      "     44    40       0.0374       0.0372     0.000178       0.0983        0.128        0.848      0.00883\n",
      "     44    50       0.0508       0.0497      0.00111        0.114        0.148         2.12       0.0221\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     44     2       0.0421       0.0421     5.48e-06        0.101        0.136        0.123      0.00129\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              44  119.131    0.002       0.0396     0.000258       0.0399        0.097        0.132        0.761      0.00792\n",
      "! Validation         44  119.131    0.002        0.043      3.9e-06        0.043          0.1        0.138        0.107      0.00111\n",
      "Wall time: 119.13180282600001\n",
      "! Best model       44    0.043\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     45    10       0.0312       0.0309     0.000222       0.0861        0.117        0.945      0.00985\n",
      "     45    20       0.0371       0.0358      0.00126       0.0919        0.125         2.26       0.0236\n",
      "     45    30       0.0223       0.0221     0.000191       0.0706       0.0986        0.883       0.0092\n",
      "     45    40       0.0368       0.0368     1.53e-05       0.0928        0.127         0.25       0.0026\n",
      "     45    50       0.0333       0.0332     7.44e-05       0.0903        0.121        0.547       0.0057\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     45     2       0.0417       0.0417     6.22e-06          0.1        0.135        0.136      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              45  121.683    0.002       0.0394     0.000402       0.0398       0.0967        0.132          1.1       0.0114\n",
      "! Validation         45  121.683    0.002       0.0426     4.28e-06       0.0426       0.0999        0.137        0.111      0.00116\n",
      "Wall time: 121.6837276839999\n",
      "! Best model       45    0.043\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     46    10       0.0236       0.0234     0.000194       0.0796        0.101        0.883       0.0092\n",
      "     46    20       0.0338       0.0334     0.000332       0.0885        0.121         1.16       0.0121\n",
      "     46    30       0.0473       0.0471     0.000207        0.106        0.144        0.918      0.00956\n",
      "     46    40       0.0365       0.0365     2.65e-06        0.093        0.127        0.105       0.0011\n",
      "     46    50       0.0307       0.0302     0.000455       0.0847        0.115         1.36       0.0141\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     46     2       0.0412       0.0412     5.22e-06       0.0994        0.135        0.119      0.00124\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              46  125.388    0.002       0.0387     9.64e-05       0.0388       0.0955         0.13        0.489       0.0051\n",
      "! Validation         46  125.388    0.002       0.0421     3.58e-06       0.0421       0.0993        0.136          0.1      0.00105\n",
      "Wall time: 125.38877350899998\n",
      "! Best model       46    0.042\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     47    10       0.0378       0.0375     0.000278       0.0912        0.128         1.06       0.0111\n",
      "     47    20       0.0416       0.0415     3.34e-05       0.0989        0.135        0.367      0.00382\n",
      "     47    30       0.0312       0.0308      0.00035       0.0846        0.116         1.19       0.0124\n",
      "     47    40        0.047       0.0437       0.0033        0.105        0.139         3.66       0.0381\n",
      "     47    50       0.0635        0.063     0.000501        0.125        0.166         1.43       0.0149\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     47     2       0.0409       0.0409     7.25e-06        0.099        0.134        0.145      0.00151\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              47  128.101    0.002         0.04     0.000387       0.0404       0.0975        0.133        0.992       0.0103\n",
      "! Validation         47  128.101    0.002       0.0417      5.2e-06       0.0417       0.0988        0.135        0.123      0.00128\n",
      "Wall time: 128.10186933\n",
      "! Best model       47    0.042\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     48    10       0.0347       0.0347     7.21e-06       0.0925        0.123        0.172      0.00179\n",
      "     48    20       0.0379       0.0379     6.62e-09       0.0932        0.129      0.00391     4.07e-05\n",
      "     48    30       0.0339       0.0339     1.34e-05       0.0899        0.122         0.23       0.0024\n",
      "     48    40       0.0516       0.0516     5.36e-07        0.111        0.151        0.043     0.000448\n",
      "     48    50       0.0316       0.0309     0.000726       0.0846        0.116         1.71       0.0179\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     48     2       0.0405       0.0405     8.23e-06       0.0985        0.133        0.152      0.00159\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              48  130.908    0.002       0.0381     0.000271       0.0384       0.0951        0.129        0.855      0.00891\n",
      "! Validation         48  130.908    0.002       0.0413     6.05e-06       0.0413       0.0984        0.135         0.13      0.00136\n",
      "Wall time: 130.90835193499993\n",
      "! Best model       48    0.041\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     49    10       0.0384       0.0358      0.00252       0.0951        0.126          3.2       0.0333\n",
      "     49    20       0.0526       0.0526     2.15e-06        0.114        0.152       0.0938     0.000977\n",
      "     49    30       0.0478       0.0472     0.000584        0.107        0.144         1.54        0.016\n",
      "     49    40       0.0341       0.0341     6.49e-05       0.0904        0.122        0.516      0.00537\n",
      "     49    50       0.0515       0.0512     0.000312        0.108         0.15         1.12       0.0117\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     49     2       0.0401       0.0401     5.79e-06       0.0979        0.133        0.127      0.00133\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              49  133.653    0.002       0.0382     0.000448       0.0386       0.0954        0.129         1.06        0.011\n",
      "! Validation         49  133.653    0.002       0.0409     3.82e-06       0.0409       0.0978        0.134        0.102      0.00106\n",
      "Wall time: 133.653860763\n",
      "! Best model       49    0.041\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     50    10       0.0355       0.0354     0.000145        0.096        0.125        0.766      0.00798\n",
      "     50    20       0.0465       0.0465     2.88e-05        0.106        0.143         0.34      0.00354\n",
      "     50    30       0.0529       0.0529     4.56e-05        0.109        0.152         0.43      0.00448\n",
      "     50    40       0.0362       0.0355     0.000653       0.0898        0.125         1.62       0.0169\n",
      "     50    50       0.0402       0.0402     6.36e-06          0.1        0.133         0.16      0.00167\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     50     2       0.0397       0.0397     6.78e-06       0.0974        0.132         0.14      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              50  136.362    0.002       0.0386     0.000438        0.039       0.0956         0.13         1.09       0.0113\n",
      "! Validation         50  136.362    0.002       0.0405     4.55e-06       0.0405       0.0973        0.133        0.114      0.00118\n",
      "Wall time: 136.36249979800004\n",
      "! Best model       50    0.040\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     51    10       0.0253        0.025     0.000224       0.0802        0.105        0.949      0.00989\n",
      "     51    20       0.0478       0.0464      0.00135        0.105        0.143         2.34       0.0244\n",
      "     51    30       0.0353       0.0351     0.000178        0.092        0.124        0.852      0.00887\n",
      "     51    40       0.0337       0.0334     0.000262       0.0891        0.121         1.03       0.0107\n",
      "     51    50       0.0324       0.0322     0.000201       0.0897        0.119        0.898      0.00936\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     51     2       0.0394       0.0393     6.43e-06       0.0969        0.131        0.134       0.0014\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              51  139.844    0.002       0.0393     0.000529       0.0398       0.0969        0.131         1.15        0.012\n",
      "! Validation         51  139.844    0.002       0.0401     4.23e-06       0.0401       0.0968        0.133        0.108      0.00113\n",
      "Wall time: 139.84428389100003\n",
      "! Best model       51    0.040\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     52    10       0.0357       0.0357     1.06e-05       0.0885        0.125        0.207      0.00216\n",
      "     52    20       0.0456       0.0453     0.000252        0.105        0.141         1.01       0.0105\n",
      "     52    30        0.031        0.031     4.14e-06       0.0834        0.117        0.129      0.00134\n",
      "     52    40       0.0404       0.0404     4.83e-06        0.101        0.133        0.141      0.00146\n",
      "     52    50       0.0488       0.0482      0.00062        0.111        0.145         1.59       0.0165\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     52     2        0.039        0.039     7.55e-06       0.0963        0.131        0.142      0.00148\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              52  142.338    0.002        0.037     0.000247       0.0373       0.0937        0.128        0.823      0.00857\n",
      "! Validation         52  142.338    0.002       0.0397     5.43e-06       0.0397       0.0963        0.132        0.123      0.00128\n",
      "Wall time: 142.33888871199997\n",
      "! Best model       52    0.040\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     53    10       0.0401       0.0401     2.15e-05        0.103        0.133        0.297      0.00309\n",
      "     53    20       0.0239       0.0239     2.92e-06       0.0791        0.102        0.109      0.00114\n",
      "     53    30       0.0368       0.0368     5.96e-06       0.0919        0.127        0.152      0.00159\n",
      "     53    40        0.049       0.0481     0.000946        0.108        0.145         1.96       0.0204\n",
      "     53    50       0.0366       0.0365     0.000102       0.0937        0.127        0.645      0.00671\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     53     2       0.0387       0.0387     6.54e-06        0.096         0.13        0.124      0.00129\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              53  145.204    0.002       0.0362     0.000228       0.0365       0.0925        0.126        0.788      0.00821\n",
      "! Validation         53  145.204    0.002       0.0394     4.54e-06       0.0394       0.0959        0.132        0.109      0.00113\n",
      "Wall time: 145.20480507899993\n",
      "! Best model       53    0.039\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     54    10       0.0396       0.0389     0.000726       0.0979        0.131         1.71       0.0179\n",
      "     54    20       0.0467       0.0467      1.3e-06        0.106        0.143       0.0703     0.000732\n",
      "     54    30       0.0319       0.0317     0.000185       0.0868        0.118        0.867      0.00903\n",
      "     54    40        0.039       0.0358      0.00323       0.0911        0.125         3.62       0.0377\n",
      "     54    50       0.0378       0.0369     0.000902       0.0934        0.127         1.91       0.0199\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     54     2       0.0383       0.0383     5.84e-06       0.0955         0.13        0.126      0.00131\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              54  147.982    0.002       0.0366     0.000493       0.0371       0.0934        0.127         1.12       0.0116\n",
      "! Validation         54  147.982    0.002        0.039     3.77e-06        0.039       0.0954        0.131       0.0992      0.00103\n",
      "Wall time: 147.98287076199995\n",
      "! Best model       54    0.039\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     55    10       0.0306         0.03     0.000546       0.0891        0.115         1.49       0.0155\n",
      "     55    20       0.0345       0.0345     3.25e-07       0.0908        0.123       0.0391     0.000407\n",
      "     55    30       0.0445       0.0443     0.000189        0.103         0.14        0.875      0.00911\n",
      "     55    40       0.0463       0.0463     2.88e-05        0.105        0.143         0.34      0.00354\n",
      "     55    50       0.0376       0.0341      0.00355       0.0873        0.122         3.79       0.0395\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     55     2        0.038        0.038     6.27e-06       0.0951        0.129        0.122      0.00127\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              55  151.694    0.002       0.0365     0.000631       0.0371       0.0932        0.127         1.24       0.0129\n",
      "! Validation         55  151.694    0.002       0.0387     3.89e-06       0.0387        0.095         0.13       0.0973      0.00101\n",
      "Wall time: 151.69421486800002\n",
      "! Best model       55    0.039\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     56    10        0.044       0.0439     4.03e-05       0.0997        0.139        0.402      0.00419\n",
      "     56    20       0.0357       0.0353     0.000369       0.0916        0.125         1.22       0.0127\n",
      "     56    30       0.0362        0.036     0.000176        0.091        0.126        0.844      0.00879\n",
      "     56    40       0.0448       0.0445     0.000306        0.106         0.14         1.11       0.0116\n",
      "     56    50       0.0313       0.0302      0.00117       0.0845        0.115         2.18       0.0227\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     56     2       0.0377       0.0377      8.8e-06       0.0946        0.129        0.151      0.00157\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              56  154.170    0.002        0.036     0.000506       0.0365       0.0923        0.126         1.16       0.0121\n",
      "! Validation         56  154.170    0.002       0.0383     6.21e-06       0.0383       0.0945         0.13         0.13      0.00135\n",
      "Wall time: 154.17102831099987\n",
      "! Best model       56    0.038\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     57    10       0.0345       0.0323      0.00213       0.0861        0.119         2.93       0.0306\n",
      "     57    20       0.0463       0.0445      0.00178       0.0999         0.14         2.68       0.0279\n",
      "     57    30       0.0344       0.0341     0.000239       0.0952        0.122        0.984       0.0103\n",
      "     57    40       0.0241       0.0237     0.000378       0.0774        0.102         1.23       0.0129\n",
      "     57    50       0.0275       0.0275     2.23e-05       0.0847         0.11        0.301      0.00313\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     57     2       0.0374       0.0374     5.77e-06       0.0941        0.128        0.119      0.00124\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              57  156.995    0.002       0.0365      0.00103       0.0375       0.0926        0.127          1.7       0.0177\n",
      "! Validation         57  156.995    0.002       0.0379     3.72e-06       0.0379        0.094        0.129       0.0957     0.000997\n",
      "Wall time: 156.9960286129999\n",
      "! Best model       57    0.038\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     58    10       0.0248       0.0247     3.34e-05       0.0811        0.104        0.367      0.00382\n",
      "     58    20       0.0391        0.039     8.46e-05       0.0989        0.131        0.586       0.0061\n",
      "     58    30       0.0433       0.0433     2.55e-05        0.105        0.138         0.32      0.00334\n",
      "     58    40       0.0446        0.043      0.00155        0.105        0.138          2.5       0.0261\n",
      "     58    50       0.0495       0.0464      0.00314        0.106        0.143         3.57       0.0372\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     58     2       0.0371       0.0371     8.71e-06       0.0937        0.128        0.147      0.00153\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              58  160.376    0.002       0.0358     0.000679       0.0365       0.0923        0.126         1.35       0.0141\n",
      "! Validation         58  160.376    0.002       0.0376     5.82e-06       0.0376       0.0936        0.129        0.121      0.00126\n",
      "Wall time: 160.37669042900006\n",
      "! Best model       58    0.038\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     59    10       0.0525       0.0523     0.000137        0.109        0.152        0.746      0.00777\n",
      "     59    20       0.0227       0.0222     0.000427        0.075       0.0988         1.32       0.0137\n",
      "     59    30       0.0341       0.0341     3.63e-05       0.0889        0.122        0.387      0.00403\n",
      "     59    40       0.0447       0.0447     9.56e-06        0.104         0.14        0.199      0.00208\n",
      "     59    50       0.0287       0.0287     9.54e-07       0.0852        0.112       0.0625     0.000651\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     59     2       0.0367       0.0367     6.81e-06       0.0931        0.127        0.124      0.00129\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              59  163.801    0.002       0.0346     0.000344        0.035        0.091        0.123        0.972       0.0101\n",
      "! Validation         59  163.801    0.002       0.0372     4.39e-06       0.0372        0.093        0.128          0.1      0.00104\n",
      "Wall time: 163.80206902999998\n",
      "! Best model       59    0.037\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     60    10       0.0295       0.0292     0.000284       0.0823        0.113         1.07       0.0112\n",
      "     60    20       0.0479       0.0477     0.000249        0.102        0.145            1       0.0105\n",
      "     60    30       0.0425       0.0425     8.11e-06        0.101        0.137         0.18      0.00187\n",
      "     60    40       0.0375       0.0371     0.000375       0.0943        0.128         1.23       0.0128\n",
      "     60    50       0.0385       0.0384     6.49e-05        0.101         0.13        0.512      0.00533\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     60     2       0.0363       0.0363      7.3e-06       0.0925        0.126        0.134       0.0014\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              60  167.385    0.002       0.0338     0.000153       0.0339       0.0894        0.122        0.666      0.00694\n",
      "! Validation         60  167.385    0.002       0.0368     4.71e-06       0.0368       0.0925        0.127        0.108      0.00113\n",
      "Wall time: 167.38628777600002\n",
      "! Best model       60    0.037\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     61    10       0.0256       0.0256     6.62e-07       0.0823        0.106       0.0508     0.000529\n",
      "     61    20       0.0351       0.0328      0.00234       0.0856         0.12         3.08       0.0321\n",
      "     61    30       0.0386       0.0386     1.22e-05          0.1         0.13        0.223      0.00232\n",
      "     61    40       0.0267       0.0267     2.15e-05       0.0786        0.108        0.297      0.00309\n",
      "     61    50       0.0353       0.0319       0.0034       0.0871        0.118         3.71       0.0387\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     61     2       0.0359       0.0358     4.75e-06        0.092        0.126        0.112      0.00116\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              61  170.094    0.002       0.0339      0.00059       0.0345       0.0899        0.122          1.2       0.0125\n",
      "! Validation         61  170.094    0.002       0.0363     3.02e-06       0.0364       0.0919        0.126       0.0895     0.000932\n",
      "Wall time: 170.09491900800003\n",
      "! Best model       61    0.036\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     62    10       0.0359       0.0358     0.000159       0.0894        0.125        0.801      0.00834\n",
      "     62    20       0.0314        0.031     0.000458       0.0825        0.117         1.36       0.0142\n",
      "     62    30        0.034        0.034     4.45e-05       0.0882        0.122        0.426      0.00444\n",
      "     62    40       0.0278       0.0273     0.000434       0.0808         0.11         1.32       0.0138\n",
      "     62    50       0.0352       0.0352     4.78e-05       0.0965        0.124        0.438      0.00456\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     62     2       0.0355       0.0355     7.09e-06       0.0914        0.125        0.127      0.00132\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              62  172.760    0.002       0.0333     0.000478       0.0338       0.0889        0.121         1.13       0.0118\n",
      "! Validation         62  172.760    0.002       0.0359     4.47e-06       0.0359       0.0913        0.126        0.101      0.00105\n",
      "Wall time: 172.76111955099987\n",
      "! Best model       62    0.036\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     63    10        0.031       0.0308     0.000203       0.0822        0.116        0.902       0.0094\n",
      "     63    20        0.034       0.0323      0.00179       0.0858        0.119         2.69        0.028\n",
      "     63    30       0.0323       0.0317     0.000565       0.0871        0.118         1.51       0.0157\n",
      "     63    40       0.0429       0.0427      0.00021        0.102        0.137        0.922       0.0096\n",
      "     63    50       0.0323       0.0323     8.11e-06       0.0856        0.119        0.184      0.00191\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     63     2       0.0351       0.0351     7.13e-06       0.0908        0.124        0.129      0.00134\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              63  175.422    0.002       0.0333      0.00055       0.0338       0.0886        0.121         1.18       0.0123\n",
      "! Validation         63  175.422    0.002       0.0355     4.35e-06       0.0355       0.0907        0.125       0.0996      0.00104\n",
      "Wall time: 175.422552193\n",
      "! Best model       63    0.036\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     64    10       0.0254       0.0253     5.25e-05        0.079        0.106        0.465      0.00484\n",
      "     64    20      0.00548      0.00545     3.06e-05       0.0347       0.0489        0.352      0.00366\n",
      "     64    30       0.0286       0.0284     0.000203       0.0823        0.112        0.906      0.00944\n",
      "     64    40       0.0436        0.043     0.000569       0.0994        0.137         1.52       0.0158\n",
      "     64    50       0.0355       0.0355     1.59e-05       0.0945        0.125        0.254      0.00264\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     64     2       0.0347       0.0347     7.86e-06       0.0903        0.123        0.137      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              64  178.069    0.002       0.0328     0.000258       0.0331       0.0884         0.12        0.865      0.00901\n",
      "! Validation         64  178.069    0.002       0.0351     4.73e-06       0.0351       0.0902        0.124          0.1      0.00105\n",
      "Wall time: 178.0698993179999\n",
      "! Best model       64    0.035\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     65    10       0.0416       0.0402      0.00142       0.0992        0.133          2.4        0.025\n",
      "     65    20       0.0313       0.0313     3.06e-05       0.0885        0.117        0.352      0.00366\n",
      "     65    30       0.0443       0.0436     0.000708        0.101        0.138          1.7       0.0177\n",
      "     65    40        0.033       0.0323     0.000717       0.0847        0.119         1.71       0.0178\n",
      "     65    50       0.0288       0.0287     9.54e-05       0.0866        0.112        0.621      0.00647\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     65     2       0.0342       0.0342     7.92e-06       0.0897        0.123        0.138      0.00144\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              65  180.831    0.002       0.0325     0.000384       0.0329       0.0877         0.12            1       0.0104\n",
      "! Validation         65  180.831    0.002       0.0346     4.89e-06       0.0346       0.0896        0.123        0.104      0.00109\n",
      "Wall time: 180.83132492100003\n",
      "! Best model       65    0.035\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     66    10        0.029        0.029     1.12e-06       0.0809        0.113       0.0664     0.000692\n",
      "     66    20       0.0348       0.0348     3.93e-05       0.0958        0.124        0.398      0.00415\n",
      "     66    30       0.0225       0.0225     8.01e-07       0.0771       0.0995       0.0586      0.00061\n",
      "     66    40       0.0307       0.0299     0.000816       0.0841        0.115         1.82        0.019\n",
      "     66    50         0.03       0.0295      0.00049        0.082        0.114         1.41       0.0147\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     66     2       0.0338       0.0338     7.31e-06       0.0891        0.122         0.13      0.00135\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              66  184.577    0.002       0.0318     0.000244        0.032       0.0869        0.118        0.784      0.00817\n",
      "! Validation         66  184.577    0.002       0.0341     4.51e-06       0.0341        0.089        0.122       0.0992      0.00103\n",
      "Wall time: 184.57950736399994\n",
      "! Best model       66    0.034\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     67    10       0.0232       0.0232     2.15e-06       0.0779        0.101       0.0938     0.000977\n",
      "     67    20       0.0247       0.0247      1.4e-05       0.0778        0.104        0.242      0.00252\n",
      "     67    30       0.0275       0.0261      0.00132       0.0811        0.107         2.31       0.0241\n",
      "     67    40        0.037        0.037      2.8e-05       0.0956        0.127         0.34      0.00354\n",
      "     67    50       0.0419       0.0395      0.00237       0.0981        0.132          3.1       0.0323\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     67     2       0.0333       0.0333     7.99e-06       0.0885        0.121         0.14      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              67  187.160    0.002       0.0316     0.000534       0.0322       0.0863        0.118         1.18       0.0123\n",
      "! Validation         67  187.160    0.002       0.0336     5.23e-06       0.0337       0.0883        0.122        0.112      0.00116\n",
      "Wall time: 187.1606173329999\n",
      "! Best model       67    0.034\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     68    10       0.0353       0.0349     0.000441       0.0879        0.124         1.33       0.0139\n",
      "     68    20       0.0257       0.0253     0.000462       0.0801        0.105         1.37       0.0142\n",
      "     68    30       0.0222        0.022     0.000227       0.0736       0.0984        0.957      0.00997\n",
      "     68    40        0.036       0.0359     0.000189       0.0922        0.126        0.875      0.00911\n",
      "     68    50       0.0244       0.0232      0.00125       0.0752        0.101         2.25       0.0234\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     68     2       0.0328       0.0328     5.33e-06       0.0878         0.12         0.11      0.00115\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              68  189.651    0.002       0.0315     0.000579       0.0321       0.0867        0.118         1.28       0.0134\n",
      "! Validation         68  189.651    0.002       0.0331      3.1e-06       0.0331       0.0877        0.121       0.0812     0.000846\n",
      "Wall time: 189.65190850399995\n",
      "! Best model       68    0.033\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     69    10        0.024       0.0224      0.00159       0.0741       0.0991         2.54       0.0264\n",
      "     69    20       0.0366       0.0359     0.000632       0.0949        0.126          1.6       0.0167\n",
      "     69    30       0.0282       0.0279     0.000281       0.0856        0.111         1.07       0.0111\n",
      "     69    40        0.018        0.018     2.65e-08       0.0683       0.0889      0.00781     8.14e-05\n",
      "     69    50      0.00609      0.00602     6.23e-05       0.0342       0.0514        0.504      0.00525\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     69     2       0.0323       0.0323     7.21e-06       0.0871        0.119        0.133      0.00138\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              69  192.380    0.002       0.0301     0.000556       0.0306       0.0845        0.115         1.22       0.0127\n",
      "! Validation         69  192.380    0.002       0.0326     4.25e-06       0.0326       0.0871         0.12       0.0969      0.00101\n",
      "Wall time: 192.38056824399996\n",
      "! Best model       69    0.033\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     70    10       0.0327       0.0326     6.23e-05       0.0888         0.12          0.5      0.00521\n",
      "     70    20       0.0293       0.0287     0.000657       0.0819        0.112         1.63        0.017\n",
      "     70    30       0.0306       0.0302     0.000417       0.0877        0.115          1.3       0.0135\n",
      "     70    40       0.0371        0.037     7.44e-05       0.0944        0.128        0.547       0.0057\n",
      "     70    50       0.0364       0.0362     0.000201       0.0932        0.126        0.898      0.00936\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     70     2       0.0317       0.0317     5.71e-06       0.0864        0.118        0.117      0.00122\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              70  196.259    0.002       0.0305     0.000663       0.0312        0.085        0.116         1.41       0.0147\n",
      "! Validation         70  196.259    0.002       0.0321     3.22e-06       0.0321       0.0863        0.119        0.082     0.000854\n",
      "Wall time: 196.25962321199995\n",
      "! Best model       70    0.032\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     71    10       0.0298       0.0297     0.000149       0.0832        0.114        0.773      0.00806\n",
      "     71    20       0.0267        0.026     0.000687       0.0813        0.107         1.67       0.0174\n",
      "     71    30       0.0308       0.0307     0.000151       0.0871        0.116        0.781      0.00814\n",
      "     71    40       0.0284       0.0284      3.5e-06       0.0799        0.112        0.121      0.00126\n",
      "     71    50       0.0323       0.0323     4.67e-05       0.0929        0.119        0.434      0.00452\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     71     2       0.0312       0.0312     6.36e-06       0.0857        0.117        0.123      0.00129\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              71  200.293    0.002         0.03     0.000652       0.0307       0.0845        0.115         1.35       0.0141\n",
      "! Validation         71  200.293    0.002       0.0316      3.8e-06       0.0316       0.0856        0.118       0.0891     0.000928\n",
      "Wall time: 200.293652457\n",
      "! Best model       71    0.032\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     72    10       0.0264       0.0262     0.000205       0.0788        0.107         0.91      0.00948\n",
      "     72    20       0.0265       0.0263     0.000174       0.0807        0.107        0.836      0.00871\n",
      "     72    30       0.0268       0.0265     0.000257        0.082        0.108         1.02       0.0106\n",
      "     72    40       0.0327        0.032     0.000726       0.0864        0.119         1.72       0.0179\n",
      "     72    50       0.0358       0.0357     0.000107       0.0913        0.125         0.66      0.00688\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     72     2       0.0307       0.0307     7.01e-06        0.085        0.116        0.134       0.0014\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              72  204.293    0.002       0.0291     0.000391       0.0295       0.0832        0.113         1.04       0.0108\n",
      "! Validation         72  204.293    0.002        0.031     4.14e-06        0.031       0.0849        0.117       0.0941     0.000981\n",
      "Wall time: 204.2932638719999\n",
      "! Best model       72    0.031\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     73    10       0.0261       0.0256     0.000476       0.0792        0.106         1.39       0.0145\n",
      "     73    20       0.0298       0.0298     2.88e-05        0.086        0.114         0.34      0.00354\n",
      "     73    30         0.04       0.0385      0.00144       0.0932         0.13         2.41       0.0251\n",
      "     73    40       0.0292        0.029     0.000194       0.0862        0.113        0.887      0.00924\n",
      "     73    50       0.0135       0.0126     0.000912        0.053       0.0744         1.92         0.02\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     73     2       0.0301       0.0301     6.67e-06       0.0842        0.115        0.138      0.00143\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              73  207.201    0.002       0.0288     0.000628       0.0295       0.0835        0.113         1.31       0.0136\n",
      "! Validation         73  207.201    0.002       0.0304     3.99e-06       0.0304       0.0842        0.116       0.0992      0.00103\n",
      "Wall time: 207.20122171000003\n",
      "! Best model       73    0.030\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     74    10       0.0211       0.0209     0.000203        0.071       0.0957        0.906      0.00944\n",
      "     74    20       0.0281       0.0276     0.000549       0.0832         0.11         1.49       0.0155\n",
      "     74    30        0.026        0.026     3.83e-05       0.0829        0.107        0.395      0.00411\n",
      "     74    40       0.0367       0.0329      0.00381       0.0871         0.12         3.92       0.0409\n",
      "     74    50      0.00604      0.00576     0.000287       0.0377       0.0503         1.07       0.0112\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     74     2       0.0296       0.0296     7.09e-06       0.0835        0.114        0.139      0.00145\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              74  210.217    0.002        0.028     0.000836       0.0288       0.0822        0.111         1.53       0.0159\n",
      "! Validation         74  210.217    0.002       0.0299     4.23e-06       0.0299       0.0835        0.115        0.101      0.00105\n",
      "Wall time: 210.21735986\n",
      "! Best model       74    0.030\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     75    10       0.0228       0.0228     8.01e-07       0.0744          0.1       0.0547      0.00057\n",
      "     75    20       0.0293       0.0293     3.83e-05       0.0828        0.113        0.395      0.00411\n",
      "     75    30       0.0239       0.0229     0.000936       0.0784          0.1         1.95       0.0203\n",
      "     75    40       0.0266       0.0266     8.58e-06       0.0819        0.108        0.184      0.00191\n",
      "     75    50       0.0352       0.0351      0.00017       0.0942        0.124        0.828      0.00863\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     75     2       0.0291       0.0291     6.03e-06       0.0828        0.113         0.13      0.00136\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              75  213.404    0.002       0.0275     0.000266       0.0278       0.0815         0.11        0.859      0.00895\n",
      "! Validation         75  213.404    0.002       0.0294     3.42e-06       0.0294       0.0828        0.114       0.0906     0.000944\n",
      "Wall time: 213.40526160299987\n",
      "! Best model       75    0.029\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     76    10       0.0287       0.0279     0.000811       0.0806        0.111         1.82       0.0189\n",
      "     76    20       0.0271       0.0265     0.000661       0.0797        0.108         1.64       0.0171\n",
      "     76    30       0.0295       0.0288     0.000708       0.0838        0.113         1.69       0.0176\n",
      "     76    40       0.0252       0.0252      1.7e-06        0.081        0.105        0.082     0.000854\n",
      "     76    50       0.0238       0.0237     0.000128       0.0768        0.102        0.719      0.00749\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     76     2       0.0286       0.0286     6.53e-06       0.0822        0.112        0.138      0.00144\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              76  217.256    0.002       0.0269     0.000504       0.0274       0.0804        0.109         1.18       0.0123\n",
      "! Validation         76  217.256    0.002       0.0289     3.83e-06       0.0289       0.0821        0.113       0.0992      0.00103\n",
      "Wall time: 217.25701302800007\n",
      "! Best model       76    0.029\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     77    10       0.0343       0.0342     5.13e-05       0.0906        0.123        0.457      0.00476\n",
      "     77    20       0.0277       0.0263      0.00137       0.0816        0.108         2.36       0.0246\n",
      "     77    30       0.0414        0.041     0.000469        0.108        0.134         1.38       0.0143\n",
      "     77    40       0.0339       0.0339     9.54e-07       0.0916        0.122       0.0625     0.000651\n",
      "     77    50       0.0265       0.0255     0.000982       0.0809        0.106            2       0.0208\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     77     2        0.028        0.028     6.01e-06       0.0814        0.111        0.132      0.00138\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              77  219.821    0.002       0.0274     0.000522       0.0279       0.0816         0.11         1.17       0.0122\n",
      "! Validation         77  219.821    0.002       0.0283     3.53e-06       0.0283       0.0814        0.112       0.0965      0.00101\n",
      "Wall time: 219.82188049299998\n",
      "! Best model       77    0.028\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     78    10       0.0275       0.0275     3.21e-06       0.0824         0.11        0.113      0.00118\n",
      "     78    20       0.0244       0.0244     3.15e-05       0.0778        0.104        0.355       0.0037\n",
      "     78    30       0.0309       0.0307     0.000161        0.087        0.116        0.805      0.00838\n",
      "     78    40       0.0244       0.0241     0.000353       0.0773        0.103          1.2       0.0125\n",
      "     78    50       0.0273       0.0256      0.00179       0.0813        0.106          2.7       0.0281\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     78     2       0.0275       0.0275     4.94e-06       0.0808         0.11        0.123      0.00128\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              78  222.293    0.002        0.026     0.000292       0.0263       0.0794        0.107        0.862      0.00898\n",
      "! Validation         78  222.293    0.002       0.0278     2.69e-06       0.0278       0.0807        0.111       0.0789     0.000822\n",
      "Wall time: 222.29403121799987\n",
      "! Best model       78    0.028\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     79    10       0.0162       0.0159     0.000252        0.064       0.0836         1.01       0.0105\n",
      "     79    20       0.0245       0.0245     1.28e-05       0.0799        0.104        0.227      0.00236\n",
      "     79    30       0.0196       0.0188     0.000743        0.066       0.0909         1.73       0.0181\n",
      "     79    40        0.028       0.0272     0.000775       0.0884        0.109         1.77       0.0184\n",
      "     79    50       0.0279       0.0272     0.000682        0.083        0.109         1.66       0.0173\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     79     2       0.0271       0.0271     5.85e-06       0.0802        0.109        0.134      0.00139\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              79  224.937    0.002       0.0262     0.000807        0.027       0.0796        0.107         1.54       0.0161\n",
      "! Validation         79  224.937    0.002       0.0273     3.37e-06       0.0273       0.0801         0.11       0.0961        0.001\n",
      "Wall time: 224.93803017799996\n",
      "! Best model       79    0.027\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     80    10       0.0285       0.0249      0.00361       0.0764        0.105         3.82       0.0398\n",
      "     80    20       0.0288       0.0287       0.0001       0.0885        0.112        0.637      0.00663\n",
      "     80    30       0.0235       0.0231     0.000444       0.0729        0.101         1.34        0.014\n",
      "     80    40       0.0234        0.023     0.000398       0.0749          0.1         1.27       0.0132\n",
      "     80    50       0.0235       0.0235     6.78e-06       0.0782        0.102        0.164      0.00171\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     80     2       0.0266       0.0266     6.18e-06       0.0795        0.108        0.136      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              80  227.450    0.002       0.0265     0.000732       0.0272       0.0799        0.108         1.34        0.014\n",
      "! Validation         80  227.450    0.002       0.0269     3.65e-06       0.0269       0.0794        0.109          0.1      0.00105\n",
      "Wall time: 227.45078908300002\n",
      "! Best model       80    0.027\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     81    10       0.0298       0.0287      0.00109       0.0822        0.112          2.1       0.0219\n",
      "     81    20       0.0373       0.0371     0.000172       0.0924        0.128        0.836      0.00871\n",
      "     81    30       0.0243       0.0243     6.36e-06       0.0806        0.103        0.164      0.00171\n",
      "     81    40       0.0335       0.0327      0.00073       0.0917         0.12         1.72       0.0179\n",
      "     81    50       0.0323       0.0321     0.000219       0.0879        0.119        0.945      0.00985\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     81     2       0.0262       0.0262     5.92e-06       0.0789        0.107        0.136      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              81  230.365    0.002       0.0251     0.000275       0.0253       0.0778        0.105        0.846      0.00881\n",
      "! Validation         81  230.365    0.002       0.0265     3.38e-06       0.0265       0.0789        0.108       0.0957     0.000997\n",
      "Wall time: 230.36518647899993\n",
      "! Best model       81    0.026\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     82    10       0.0223       0.0222     2.38e-05       0.0706       0.0989        0.312      0.00326\n",
      "     82    20       0.0344       0.0332      0.00122       0.0902        0.121         2.23       0.0232\n",
      "     82    30       0.0237       0.0237     4.24e-07       0.0787        0.102       0.0391     0.000407\n",
      "     82    40       0.0339       0.0314      0.00253       0.0898        0.117          3.2       0.0333\n",
      "     82    50       0.0349       0.0324      0.00255       0.0855        0.119         3.22       0.0335\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     82     2       0.0258       0.0258     9.42e-06       0.0783        0.106        0.169      0.00176\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              82  232.775    0.002       0.0254     0.000723       0.0262       0.0778        0.106         1.44        0.015\n",
      "! Validation         82  232.775    0.002       0.0261     5.82e-06       0.0261       0.0784        0.107        0.127      0.00132\n",
      "Wall time: 232.77564235299997\n",
      "! Best model       82    0.026\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     83    10       0.0255       0.0253     0.000139       0.0793        0.106        0.754      0.00785\n",
      "     83    20       0.0305       0.0305     3.06e-05       0.0906        0.116        0.352      0.00366\n",
      "     83    30       0.0323       0.0318     0.000441       0.0837        0.118         1.33       0.0139\n",
      "     83    40       0.0309       0.0309     8.01e-05       0.0837        0.116         0.57      0.00594\n",
      "     83    50        0.033       0.0323     0.000704       0.0871        0.119         1.69       0.0176\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     83     2       0.0254       0.0254     6.01e-06       0.0778        0.106        0.136      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              83  235.256    0.002       0.0255     0.000307       0.0258       0.0787        0.106         0.87      0.00907\n",
      "! Validation         83  235.256    0.002       0.0257     3.37e-06       0.0257       0.0779        0.106       0.0934     0.000972\n",
      "Wall time: 235.25705568500007\n",
      "! Best model       83    0.026\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     84    10       0.0253       0.0253     2.38e-07       0.0829        0.105       0.0312     0.000326\n",
      "     84    20       0.0186       0.0183     0.000273        0.065       0.0897         1.05        0.011\n",
      "     84    30       0.0304         0.03     0.000385       0.0861        0.115         1.25        0.013\n",
      "     84    40       0.0241       0.0233     0.000734       0.0748        0.101         1.73        0.018\n",
      "     84    50       0.0307       0.0279      0.00275       0.0823        0.111         3.34       0.0347\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     84     2       0.0251       0.0251     5.88e-06       0.0773        0.105        0.137      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              84  237.704    0.002       0.0238     0.000484       0.0243       0.0754        0.102         1.05       0.0109\n",
      "! Validation         84  237.704    0.002       0.0254     3.32e-06       0.0254       0.0774        0.106       0.0949     0.000989\n",
      "Wall time: 237.7050416909999\n",
      "! Best model       84    0.025\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     85    10       0.0296       0.0286     0.000987       0.0878        0.112            2       0.0208\n",
      "     85    20       0.0205       0.0205     5.57e-06         0.07        0.095        0.152      0.00159\n",
      "     85    30        0.023       0.0227     0.000321       0.0772       0.0998         1.14       0.0119\n",
      "     85    40       0.0264       0.0221      0.00437       0.0727       0.0985          4.2       0.0438\n",
      "     85    50       0.0154       0.0153     6.62e-05       0.0629       0.0821        0.516      0.00537\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     85     2       0.0247       0.0247     7.51e-06       0.0769        0.104        0.152      0.00158\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              85  240.187    0.002       0.0247     0.000503       0.0252       0.0778        0.104         1.11       0.0115\n",
      "! Validation         85  240.187    0.002        0.025     4.43e-06        0.025       0.0769        0.105        0.111      0.00116\n",
      "Wall time: 240.18709924699988\n",
      "! Best model       85    0.025\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     86    10       0.0191       0.0155      0.00361       0.0568       0.0825         3.82       0.0398\n",
      "     86    20       0.0236       0.0233     0.000281       0.0772        0.101         1.07       0.0111\n",
      "     86    30       0.0185       0.0185     6.36e-05       0.0655       0.0901        0.508      0.00529\n",
      "     86    40       0.0184       0.0183     0.000128        0.063       0.0897        0.719      0.00749\n",
      "     86    50       0.0287        0.026      0.00266       0.0809        0.107         3.29       0.0342\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     86     2       0.0244       0.0244     1.05e-05       0.0764        0.104        0.181      0.00189\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              86  242.752    0.002       0.0242     0.000694       0.0249       0.0763        0.103         1.39       0.0145\n",
      "! Validation         86  242.752    0.002       0.0247     6.38e-06       0.0247       0.0765        0.104        0.134       0.0014\n",
      "Wall time: 242.75294197900007\n",
      "! Best model       86    0.025\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     87    10       0.0179       0.0179     3.73e-05       0.0673       0.0887        0.391      0.00407\n",
      "     87    20       0.0235       0.0235     3.93e-05       0.0778        0.102        0.402      0.00419\n",
      "     87    30       0.0259       0.0248      0.00116       0.0773        0.104         2.17       0.0226\n",
      "     87    40       0.0301       0.0295     0.000612       0.0845        0.114         1.58       0.0164\n",
      "     87    50       0.0297       0.0286      0.00112       0.0834        0.112         2.13       0.0222\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     87     2       0.0241       0.0241     8.33e-06        0.076        0.103        0.158      0.00164\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              87  245.252    0.002       0.0241     0.000504       0.0246       0.0762        0.103         1.17       0.0122\n",
      "! Validation         87  245.252    0.002       0.0244     4.93e-06       0.0244       0.0761        0.104        0.116       0.0012\n",
      "Wall time: 245.25341713199987\n",
      "! Best model       87    0.024\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     88    10       0.0226       0.0224     0.000239       0.0747       0.0992        0.984       0.0103\n",
      "     88    20        0.027       0.0265     0.000538       0.0794        0.108         1.48       0.0154\n",
      "     88    30       0.0185       0.0179     0.000569        0.067       0.0887         1.52       0.0158\n",
      "     88    40       0.0264       0.0263     9.54e-05       0.0809        0.108        0.621      0.00647\n",
      "     88    50       0.0285       0.0284     0.000117       0.0862        0.112        0.688      0.00716\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     88     2       0.0238       0.0238     6.93e-06       0.0756        0.102        0.145      0.00151\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              88  247.804    0.002       0.0245     0.000658       0.0252        0.077        0.104         1.35       0.0141\n",
      "! Validation         88  247.804    0.002       0.0241     3.88e-06       0.0241       0.0757        0.103          0.1      0.00104\n",
      "Wall time: 247.80435691699995\n",
      "! Best model       88    0.024\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     89    10       0.0169       0.0136      0.00329       0.0562       0.0774         3.65        0.038\n",
      "     89    20       0.0381       0.0358      0.00232       0.0883        0.125         3.07       0.0319\n",
      "     89    30        0.018       0.0176     0.000369       0.0706        0.088         1.22       0.0127\n",
      "     89    40       0.0179       0.0178     0.000157       0.0654       0.0884        0.797       0.0083\n",
      "     89    50       0.0228       0.0221     0.000624       0.0742       0.0986         1.59       0.0166\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     89     2       0.0236       0.0236     4.92e-06       0.0752        0.102        0.123      0.00128\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              89  250.400    0.002       0.0238     0.000985       0.0248       0.0753        0.102          1.7       0.0177\n",
      "! Validation         89  250.400    0.002       0.0239     2.75e-06       0.0239       0.0753        0.102       0.0828     0.000863\n",
      "Wall time: 250.40017436799985\n",
      "! Best model       89    0.024\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     90    10        0.022       0.0218     0.000234       0.0735       0.0978        0.973       0.0101\n",
      "     90    20        0.017       0.0165     0.000486       0.0641       0.0853         1.41       0.0146\n",
      "     90    30        0.018       0.0161      0.00195       0.0619        0.084         2.81       0.0293\n",
      "     90    40       0.0381       0.0371      0.00105       0.0994        0.128         2.07       0.0215\n",
      "     90    50        0.022       0.0219     5.13e-05        0.075       0.0982        0.457      0.00476\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     90     2       0.0234       0.0234     4.67e-06       0.0748        0.101        0.125       0.0013\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              90  254.031    0.002        0.023     0.000627       0.0236       0.0744          0.1         1.33       0.0138\n",
      "! Validation         90  254.031    0.002       0.0236     2.69e-06       0.0236       0.0749        0.102       0.0863     0.000899\n",
      "Wall time: 254.03180447299997\n",
      "! Best model       90    0.024\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     91    10        0.017        0.017     2.46e-05        0.065       0.0865        0.316       0.0033\n",
      "     91    20       0.0188       0.0186     0.000205        0.067       0.0904         0.91      0.00948\n",
      "     91    30       0.0302       0.0301     0.000121       0.0854        0.115        0.699      0.00728\n",
      "     91    40       0.0248       0.0247     0.000126       0.0789        0.104        0.715      0.00745\n",
      "     91    50       0.0207       0.0207     4.56e-05       0.0713       0.0953        0.434      0.00452\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     91     2       0.0231       0.0231     5.42e-06       0.0744        0.101        0.132      0.00138\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              91  257.663    0.002       0.0229     0.000341       0.0233       0.0744          0.1        0.921      0.00959\n",
      "! Validation         91  257.663    0.002       0.0234     3.09e-06       0.0234       0.0745        0.101        0.091     0.000948\n",
      "Wall time: 257.6636529099999\n",
      "! Best model       91    0.023\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     92    10       0.0235       0.0217      0.00178       0.0699       0.0976         2.69        0.028\n",
      "     92    20       0.0285       0.0267      0.00178       0.0785        0.108         2.68        0.028\n",
      "     92    30       0.0225       0.0223     0.000161       0.0741        0.099        0.805      0.00838\n",
      "     92    40       0.0145       0.0142     0.000224       0.0565       0.0791        0.953      0.00993\n",
      "     92    50       0.0153       0.0152     6.76e-05       0.0633       0.0818        0.523      0.00545\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     92     2       0.0229       0.0229     6.39e-06        0.074          0.1        0.136      0.00142\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              92  261.389    0.002       0.0232     0.000566       0.0238       0.0744        0.101         1.25        0.013\n",
      "! Validation         92  261.389    0.002       0.0232     3.63e-06       0.0232       0.0742        0.101       0.0938     0.000977\n",
      "Wall time: 261.38951539000004\n",
      "! Best model       92    0.023\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     93    10       0.0223       0.0219     0.000323        0.075       0.0982         1.14       0.0119\n",
      "     93    20       0.0194       0.0156      0.00384       0.0587       0.0828         3.94       0.0411\n",
      "     93    30       0.0238       0.0237     4.78e-05       0.0754        0.102        0.441       0.0046\n",
      "     93    40       0.0238        0.022      0.00179       0.0728       0.0984         2.69        0.028\n",
      "     93    50       0.0209       0.0209     3.06e-05       0.0725       0.0958        0.352      0.00366\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     93     2       0.0227       0.0227      5.4e-06       0.0737       0.0999         0.13      0.00135\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              93  265.011    0.002       0.0228     0.000571       0.0234       0.0744          0.1         1.17       0.0122\n",
      "! Validation         93  265.011    0.002        0.023     3.08e-06        0.023       0.0738          0.1       0.0906     0.000944\n",
      "Wall time: 265.01239106\n",
      "! Best model       93    0.023\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     94    10       0.0216       0.0207     0.000971       0.0736       0.0953         1.98       0.0207\n",
      "     94    20       0.0242       0.0212      0.00302       0.0695       0.0966         3.49       0.0364\n",
      "     94    30       0.0265       0.0263     0.000167       0.0825        0.107        0.824      0.00859\n",
      "     94    40       0.0329       0.0307      0.00213       0.0882        0.116         2.94       0.0306\n",
      "     94    50       0.0226       0.0226     3.06e-05       0.0756       0.0996        0.352      0.00366\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     94     2       0.0225       0.0225     5.22e-06       0.0733       0.0994         0.13      0.00136\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              94  267.707    0.002       0.0223     0.000599       0.0229       0.0732       0.0989         1.26       0.0131\n",
      "! Validation         94  267.707    0.002       0.0227     2.96e-06       0.0227       0.0735          0.1       0.0891     0.000928\n",
      "Wall time: 267.7078271749999\n",
      "! Best model       94    0.023\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     95    10       0.0144       0.0144     9.07e-06       0.0557       0.0797        0.191      0.00199\n",
      "     95    20       0.0284       0.0279     0.000479        0.083        0.111         1.39       0.0145\n",
      "     95    30       0.0226       0.0224     0.000215       0.0696       0.0991         0.93      0.00968\n",
      "     95    40       0.0197       0.0196     5.98e-05       0.0695       0.0929        0.492      0.00513\n",
      "     95    50       0.0285       0.0281     0.000366       0.0819        0.111         1.21       0.0127\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     95     2       0.0222       0.0222     4.58e-06        0.073       0.0989        0.122      0.00127\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              95  270.332    0.002       0.0212     0.000222       0.0214       0.0713       0.0966        0.806       0.0084\n",
      "! Validation         95  270.332    0.002       0.0225     2.59e-06       0.0225       0.0731       0.0994        0.084     0.000875\n",
      "Wall time: 270.33245174700005\n",
      "! Best model       95    0.022\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     96    10       0.0174       0.0166     0.000757        0.064       0.0855         1.75       0.0183\n",
      "     96    20       0.0137       0.0135     0.000172       0.0568       0.0771        0.832      0.00867\n",
      "     96    30       0.0275       0.0274     8.31e-05       0.0775         0.11        0.582      0.00606\n",
      "     96    40       0.0215        0.021     0.000448       0.0718       0.0961         1.35        0.014\n",
      "     96    50        0.014       0.0135     0.000476       0.0584        0.077         1.39       0.0144\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     96     2       0.0221       0.0221     4.61e-06       0.0726       0.0985        0.124      0.00129\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              96  272.896    0.002       0.0219     0.000404       0.0223       0.0722       0.0981         1.04       0.0109\n",
      "! Validation         96  272.896    0.002       0.0223      2.6e-06       0.0223       0.0728       0.0989       0.0836     0.000871\n",
      "Wall time: 272.89715267099996\n",
      "! Best model       96    0.022\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     97    10       0.0225       0.0225     3.83e-05       0.0728       0.0994        0.395      0.00411\n",
      "     97    20       0.0287       0.0286      9.7e-05       0.0821        0.112        0.625      0.00651\n",
      "     97    30       0.0293       0.0283      0.00101       0.0831        0.111         2.02       0.0211\n",
      "     97    40       0.0205       0.0201     0.000391       0.0686       0.0939         1.26       0.0131\n",
      "     97    50       0.0202         0.02      0.00021       0.0711       0.0937        0.922       0.0096\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     97     2       0.0218       0.0218      4.2e-06       0.0722       0.0979        0.116       0.0012\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              97  275.722    0.002       0.0217     0.000274        0.022       0.0718       0.0976        0.866      0.00902\n",
      "! Validation         97  275.722    0.002        0.022     2.39e-06        0.022       0.0724       0.0984       0.0793     0.000826\n",
      "Wall time: 275.7228083709999\n",
      "! Best model       97    0.022\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     98    10       0.0287       0.0277     0.000982       0.0857         0.11         1.99       0.0208\n",
      "     98    20       0.0268       0.0268     2.97e-05       0.0804        0.108        0.348      0.00362\n",
      "     98    30       0.0241       0.0238     0.000298       0.0725        0.102         1.09       0.0114\n",
      "     98    40       0.0223       0.0207      0.00162       0.0736       0.0954         2.56       0.0267\n",
      "     98    50        0.022       0.0218     0.000244       0.0757       0.0978        0.996       0.0104\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     98     2       0.0216       0.0216      4.3e-06       0.0719       0.0975        0.118      0.00123\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              98  279.337    0.002       0.0212     0.000416       0.0217       0.0715       0.0966         1.07       0.0112\n",
      "! Validation         98  279.337    0.002       0.0218     2.45e-06       0.0218       0.0721       0.0979       0.0797      0.00083\n",
      "Wall time: 279.33724571599987\n",
      "! Best model       98    0.022\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     99    10       0.0263        0.026     0.000353       0.0794        0.107          1.2       0.0125\n",
      "     99    20       0.0229       0.0226     0.000326       0.0788       0.0996         1.15        0.012\n",
      "     99    30       0.0162       0.0161     3.83e-05       0.0666       0.0842        0.391      0.00407\n",
      "     99    40       0.0209       0.0208     4.67e-05       0.0735       0.0956        0.438      0.00456\n",
      "     99    50       0.0185       0.0185     2.92e-06       0.0696       0.0901        0.109      0.00114\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     99     2       0.0214       0.0214     4.87e-06       0.0716        0.097        0.122      0.00127\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              99  282.050    0.002       0.0202     0.000225       0.0204       0.0696       0.0942        0.782      0.00815\n",
      "! Validation         99  282.050    0.002       0.0216     2.83e-06       0.0216       0.0717       0.0974       0.0855     0.000891\n",
      "Wall time: 282.050670223\n",
      "! Best model       99    0.022\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "    100    10       0.0165       0.0158      0.00067       0.0634       0.0833         1.64       0.0171\n",
      "    100    20       0.0197       0.0191     0.000604       0.0686       0.0917         1.57       0.0163\n",
      "    100    30       0.0183       0.0181      0.00018       0.0672       0.0892        0.855      0.00891\n",
      "    100    40       0.0275       0.0273     0.000119       0.0786         0.11        0.695      0.00724\n",
      "    100    50        0.035       0.0336      0.00146       0.0943        0.121         2.43       0.0253\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "    100     2       0.0212       0.0212     3.55e-06       0.0712       0.0965        0.107      0.00111\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train             100  285.568    0.002       0.0208     0.000416       0.0212        0.071       0.0956         1.09       0.0114\n",
      "! Validation        100  285.568    0.002       0.0214     2.09e-06       0.0214       0.0713       0.0969        0.073     0.000761\n",
      "Wall time: 285.5688821250001\n",
      "! Best model      100    0.021\n",
      "! Stop training: max epochs\n",
      "Wall time: 285.58286223100004\n",
      "Cumulative wall time: 285.58286223100004\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./results300\n",
    "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27720fb-b1c6-409e-981a-a0185203babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-11.8'\n",
      "Using device: cpu\n",
      "Loading dataset... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
      "    loading dataset took 0.0294s\n",
      "    loaded dataset of size 500 and sampled --n-data=2 frames\n",
      "    benchmark frames statistics:\n",
      "         number of atoms: 96\n",
      "         number of types: 5\n",
      "          avg. num edges: 2419.0\n",
      "         avg. neigh/atom: 25.19791603088379\n",
      "Building model... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "    building model took 0.4011s\n",
      "    model has 37608 weights\n",
      "    model has 37608 trainable weights\n",
      "    model weights and buffers take 0.14 MB\n",
      "Compile...\n",
      "    compilation took 0.1491s\n",
      "Warmup...\n",
      "    6 calls of warmup took 0.4420s\n",
      "Benchmarking...\n",
      " -- Results --\n",
      "PLEASE NOTE: these are speeds for the MODEL, evaluated on --n-data=2 configurations kept in memory.\n",
      "A variety of factors affect the performance in real molecular dynamics calculations:\n",
      "!!! Molecular dynamics speeds should be measured in LAMMPS; speeds from nequip-benchmark should only be used as an estimate of RELATIVE speed among different hyperparameters.\n",
      "Please further note that relative speed ordering of hyperparameters is NOT NECESSARILY CONSISTENT across different classes of GPUs (i.e. A100 vs V100 vs consumer) or GPUs vs CPUs.\n",
      "\n",
      "The average call took 20.36ms\n"
     ]
    }
   ],
   "source": [
    "!nequip-benchmark allegro/configs/tutorial.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3deb5ca1-ac25-49d8-a285-ccac3910932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trainer = torch.load(\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
      "    loaded model\n",
      "Loading original dataset...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
      "Loaded dataset specified in config.yaml.\n",
      "Using origial training dataset (500 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 440 frames.\n",
      "Starting...\n",
      "  0%|                                                   | 0/440 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "  0%|                                           | 1/440 [00:00<01:18,  5.60it/s]\n",
      "  0%|â–                                          | 2/440 [00:00<02:21,  3.11it/s]\n",
      "  1%|â–Ž                                          | 3/440 [00:00<02:17,  3.17it/s]\n",
      "f_mae = 0.0494 | f_rmse = 0.0741 | e_mae = 0.1631 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0462 | f_rmse = 0.0692 | e_mae = 0.1805 | e/N_mae = 0.0019\u001b[A\n",
      "f_mae = 0.0453 | f_rmse = 0.0674 | e_mae = 0.1849 | e/N_mae = 0.0019\u001b[A\n",
      "f_mae = 0.0452 | f_rmse = 0.0665 | e_mae = 0.1758 | e/N_mae = 0.0018\u001b[A\n",
      "  2%|â–Š                                          | 8/440 [00:01<00:40, 10.75it/s]\n",
      "f_mae = 0.0462 | f_rmse = 0.0669 | e_mae = 0.1576 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0460 | f_rmse = 0.0663 | e_mae = 0.1508 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0458 | f_rmse = 0.0657 | e_mae = 0.1428 | e/N_mae = 0.0015\u001b[A\n",
      "  3%|â–ˆâ–                                        | 12/440 [00:01<00:26, 16.30it/s]\n",
      "f_mae = 0.0460 | f_rmse = 0.0653 | e_mae = 0.1286 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0465 | f_rmse = 0.0657 | e_mae = 0.1242 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0468 | f_rmse = 0.0660 | e_mae = 0.1198 | e/N_mae = 0.0012\u001b[A\n",
      "  4%|â–ˆâ–Œ                                        | 16/440 [00:01<00:19, 21.30it/s]\n",
      "f_mae = 0.0469 | f_rmse = 0.0658 | e_mae = 0.1108 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0471 | f_rmse = 0.0659 | e_mae = 0.1081 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0474 | f_rmse = 0.0662 | e_mae = 0.1063 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0479 | f_rmse = 0.0670 | e_mae = 0.1068 | e/N_mae = 0.0011\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                        | 21/440 [00:01<00:15, 26.85it/s]\n",
      "f_mae = 0.0489 | f_rmse = 0.0683 | e_mae = 0.1030 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0492 | f_rmse = 0.0687 | e_mae = 0.1012 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0494 | f_rmse = 0.0689 | e_mae = 0.0996 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0496 | f_rmse = 0.0691 | e_mae = 0.0972 | e/N_mae = 0.0010\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–                                       | 26/440 [00:01<00:12, 32.09it/s]\n",
      "f_mae = 0.0504 | f_rmse = 0.0701 | e_mae = 0.0901 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0509 | f_rmse = 0.0707 | e_mae = 0.0873 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0514 | f_rmse = 0.0715 | e_mae = 0.0865 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0519 | f_rmse = 0.0723 | e_mae = 0.0874 | e/N_mae = 0.0009\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–‰                                       | 31/440 [00:01<00:11, 36.07it/s]\n",
      "f_mae = 0.0528 | f_rmse = 0.0736 | e_mae = 0.0886 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0534 | f_rmse = 0.0744 | e_mae = 0.0870 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0538 | f_rmse = 0.0750 | e_mae = 0.0846 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0541 | f_rmse = 0.0754 | e_mae = 0.0825 | e/N_mae = 0.0009\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 36/440 [00:01<00:10, 39.28it/s]\n",
      "f_mae = 0.0556 | f_rmse = 0.0778 | e_mae = 0.0799 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0561 | f_rmse = 0.0786 | e_mae = 0.0794 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0564 | f_rmse = 0.0790 | e_mae = 0.0791 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0566 | f_rmse = 0.0793 | e_mae = 0.0780 | e/N_mae = 0.0008\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 41/440 [00:01<00:09, 41.55it/s]\n",
      "f_mae = 0.0572 | f_rmse = 0.0799 | e_mae = 0.0745 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0575 | f_rmse = 0.0803 | e_mae = 0.0736 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0576 | f_rmse = 0.0804 | e_mae = 0.0732 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0578 | f_rmse = 0.0806 | e_mae = 0.0734 | e/N_mae = 0.0008\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 46/440 [00:01<00:09, 43.16it/s]\n",
      "f_mae = 0.0582 | f_rmse = 0.0812 | e_mae = 0.0730 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0583 | f_rmse = 0.0813 | e_mae = 0.0741 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0584 | f_rmse = 0.0814 | e_mae = 0.0757 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0586 | f_rmse = 0.0816 | e_mae = 0.0774 | e/N_mae = 0.0008\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 51/440 [00:01<00:09, 42.75it/s]\n",
      "f_mae = 0.0590 | f_rmse = 0.0821 | e_mae = 0.0796 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0591 | f_rmse = 0.0823 | e_mae = 0.0813 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0591 | f_rmse = 0.0823 | e_mae = 0.0836 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0592 | f_rmse = 0.0825 | e_mae = 0.0846 | e/N_mae = 0.0009\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 56/440 [00:02<00:09, 42.47it/s]\n",
      "f_mae = 0.0597 | f_rmse = 0.0831 | e_mae = 0.0824 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0598 | f_rmse = 0.0833 | e_mae = 0.0817 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0599 | f_rmse = 0.0833 | e_mae = 0.0817 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0601 | f_rmse = 0.0835 | e_mae = 0.0809 | e/N_mae = 0.0008\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 61/440 [00:02<00:08, 42.53it/s]\n",
      "f_mae = 0.0606 | f_rmse = 0.0841 | e_mae = 0.0788 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0606 | f_rmse = 0.0842 | e_mae = 0.0782 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0606 | f_rmse = 0.0841 | e_mae = 0.0782 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0607 | f_rmse = 0.0842 | e_mae = 0.0777 | e/N_mae = 0.0008\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   | 66/440 [00:02<00:08, 42.46it/s]\n",
      "f_mae = 0.0611 | f_rmse = 0.0846 | e_mae = 0.0760 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0613 | f_rmse = 0.0851 | e_mae = 0.0754 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0615 | f_rmse = 0.0854 | e_mae = 0.0744 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0616 | f_rmse = 0.0855 | e_mae = 0.0736 | e/N_mae = 0.0008\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 71/440 [00:02<00:08, 41.27it/s]\n",
      "f_mae = 0.0619 | f_rmse = 0.0859 | e_mae = 0.0720 | e/N_mae = 0.0007\u001b[A\n",
      "f_mae = 0.0621 | f_rmse = 0.0861 | e_mae = 0.0717 | e/N_mae = 0.0007\u001b[A\n",
      "f_mae = 0.0622 | f_rmse = 0.0862 | e_mae = 0.0722 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0623 | f_rmse = 0.0863 | e_mae = 0.0726 | e/N_mae = 0.0008\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 76/440 [00:02<00:08, 41.52it/s]\n",
      "f_mae = 0.0624 | f_rmse = 0.0864 | e_mae = 0.0732 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0626 | f_rmse = 0.0867 | e_mae = 0.0739 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0627 | f_rmse = 0.0869 | e_mae = 0.0753 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0629 | f_rmse = 0.0872 | e_mae = 0.0770 | e/N_mae = 0.0008\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 81/440 [00:02<00:08, 43.10it/s]\n",
      "f_mae = 0.0632 | f_rmse = 0.0877 | e_mae = 0.0805 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0634 | f_rmse = 0.0881 | e_mae = 0.0817 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0635 | f_rmse = 0.0883 | e_mae = 0.0827 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0637 | f_rmse = 0.0885 | e_mae = 0.0839 | e/N_mae = 0.0009\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 86/440 [00:02<00:08, 44.03it/s]\n",
      "f_mae = 0.0639 | f_rmse = 0.0889 | e_mae = 0.0839 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0640 | f_rmse = 0.0890 | e_mae = 0.0844 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0641 | f_rmse = 0.0892 | e_mae = 0.0849 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0642 | f_rmse = 0.0893 | e_mae = 0.0852 | e/N_mae = 0.0009\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 91/440 [00:02<00:07, 45.03it/s]\n",
      "f_mae = 0.0643 | f_rmse = 0.0896 | e_mae = 0.0862 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0644 | f_rmse = 0.0897 | e_mae = 0.0867 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0645 | f_rmse = 0.0899 | e_mae = 0.0871 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0647 | f_rmse = 0.0900 | e_mae = 0.0874 | e/N_mae = 0.0009\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 96/440 [00:03<00:07, 45.80it/s]\n",
      "f_mae = 0.0648 | f_rmse = 0.0903 | e_mae = 0.0881 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0650 | f_rmse = 0.0905 | e_mae = 0.0879 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0650 | f_rmse = 0.0906 | e_mae = 0.0878 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0651 | f_rmse = 0.0907 | e_mae = 0.0875 | e/N_mae = 0.0009\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 101/440 [00:03<00:07, 46.23it/s]\n",
      "f_mae = 0.0651 | f_rmse = 0.0907 | e_mae = 0.0873 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0652 | f_rmse = 0.0908 | e_mae = 0.0868 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0652 | f_rmse = 0.0908 | e_mae = 0.0865 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0653 | f_rmse = 0.0909 | e_mae = 0.0859 | e/N_mae = 0.0009\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 106/440 [00:03<00:07, 46.65it/s]\n",
      "f_mae = 0.0654 | f_rmse = 0.0910 | e_mae = 0.0859 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0655 | f_rmse = 0.0911 | e_mae = 0.0858 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0656 | f_rmse = 0.0912 | e_mae = 0.0851 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0657 | f_rmse = 0.0913 | e_mae = 0.0849 | e/N_mae = 0.0009\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 111/440 [00:03<00:07, 46.27it/s]\n",
      "f_mae = 0.0658 | f_rmse = 0.0915 | e_mae = 0.0838 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0658 | f_rmse = 0.0916 | e_mae = 0.0833 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0658 | f_rmse = 0.0916 | e_mae = 0.0827 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0658 | f_rmse = 0.0916 | e_mae = 0.0823 | e/N_mae = 0.0009\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 116/440 [00:03<00:07, 45.94it/s]\n",
      "f_mae = 0.0659 | f_rmse = 0.0916 | e_mae = 0.0812 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0659 | f_rmse = 0.0918 | e_mae = 0.0805 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0659 | f_rmse = 0.0918 | e_mae = 0.0802 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0660 | f_rmse = 0.0917 | e_mae = 0.0803 | e/N_mae = 0.0008\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 121/440 [00:03<00:07, 44.89it/s]\n",
      "f_mae = 0.0660 | f_rmse = 0.0917 | e_mae = 0.0811 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0660 | f_rmse = 0.0918 | e_mae = 0.0815 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0660 | f_rmse = 0.0918 | e_mae = 0.0823 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0661 | f_rmse = 0.0918 | e_mae = 0.0834 | e/N_mae = 0.0009\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 126/440 [00:03<00:06, 44.93it/s]\n",
      "f_mae = 0.0660 | f_rmse = 0.0917 | e_mae = 0.0851 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0661 | f_rmse = 0.0917 | e_mae = 0.0855 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0661 | f_rmse = 0.0917 | e_mae = 0.0857 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0662 | f_rmse = 0.0918 | e_mae = 0.0860 | e/N_mae = 0.0009\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 131/440 [00:03<00:07, 39.52it/s]\n",
      "f_mae = 0.0662 | f_rmse = 0.0918 | e_mae = 0.0861 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0663 | f_rmse = 0.0918 | e_mae = 0.0859 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0663 | f_rmse = 0.0918 | e_mae = 0.0859 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0664 | f_rmse = 0.0919 | e_mae = 0.0860 | e/N_mae = 0.0009\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 136/440 [00:03<00:07, 39.30it/s]\n",
      "f_mae = 0.0664 | f_rmse = 0.0919 | e_mae = 0.0865 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0665 | f_rmse = 0.0920 | e_mae = 0.0867 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0666 | f_rmse = 0.0921 | e_mae = 0.0866 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0666 | f_rmse = 0.0922 | e_mae = 0.0863 | e/N_mae = 0.0009\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 141/440 [00:04<00:07, 39.31it/s]\n",
      "f_mae = 0.0667 | f_rmse = 0.0922 | e_mae = 0.0856 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0668 | f_rmse = 0.0924 | e_mae = 0.0851 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0669 | f_rmse = 0.0927 | e_mae = 0.0848 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0670 | f_rmse = 0.0928 | e_mae = 0.0849 | e/N_mae = 0.0009\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 146/440 [00:04<00:07, 40.20it/s]\n",
      "f_mae = 0.0671 | f_rmse = 0.0930 | e_mae = 0.0847 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0672 | f_rmse = 0.0931 | e_mae = 0.0842 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0672 | f_rmse = 0.0932 | e_mae = 0.0838 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0672 | f_rmse = 0.0932 | e_mae = 0.0833 | e/N_mae = 0.0009\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 151/440 [00:04<00:07, 40.11it/s]\n",
      "f_mae = 0.0673 | f_rmse = 0.0932 | e_mae = 0.0827 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0673 | f_rmse = 0.0932 | e_mae = 0.0828 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0674 | f_rmse = 0.0934 | e_mae = 0.0826 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0674 | f_rmse = 0.0934 | e_mae = 0.0822 | e/N_mae = 0.0009\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 156/440 [00:04<00:07, 39.79it/s]\n",
      "f_mae = 0.0675 | f_rmse = 0.0935 | e_mae = 0.0813 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0676 | f_rmse = 0.0936 | e_mae = 0.0810 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0677 | f_rmse = 0.0936 | e_mae = 0.0807 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0678 | f_rmse = 0.0937 | e_mae = 0.0806 | e/N_mae = 0.0008\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 161/440 [00:04<00:07, 39.81it/s]\n",
      "f_mae = 0.0678 | f_rmse = 0.0937 | e_mae = 0.0797 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0678 | f_rmse = 0.0937 | e_mae = 0.0793 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0678 | f_rmse = 0.0937 | e_mae = 0.0792 | e/N_mae = 0.0008\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 165/440 [00:04<00:06, 39.74it/s]\n",
      "f_mae = 0.0680 | f_rmse = 0.0938 | e_mae = 0.0791 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0681 | f_rmse = 0.0939 | e_mae = 0.0790 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0682 | f_rmse = 0.0940 | e_mae = 0.0787 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0682 | f_rmse = 0.0940 | e_mae = 0.0784 | e/N_mae = 0.0008\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 170/440 [00:04<00:06, 39.84it/s]\n",
      "f_mae = 0.0683 | f_rmse = 0.0941 | e_mae = 0.0779 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0683 | f_rmse = 0.0941 | e_mae = 0.0778 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0684 | f_rmse = 0.0942 | e_mae = 0.0779 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0685 | f_rmse = 0.0942 | e_mae = 0.0781 | e/N_mae = 0.0008\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 175/440 [00:04<00:06, 40.80it/s]\n",
      "f_mae = 0.0685 | f_rmse = 0.0943 | e_mae = 0.0791 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0686 | f_rmse = 0.0943 | e_mae = 0.0794 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0686 | f_rmse = 0.0944 | e_mae = 0.0795 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0687 | f_rmse = 0.0945 | e_mae = 0.0797 | e/N_mae = 0.0008\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 180/440 [00:05<00:06, 42.28it/s]\n",
      "f_mae = 0.0688 | f_rmse = 0.0945 | e_mae = 0.0807 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0688 | f_rmse = 0.0946 | e_mae = 0.0812 | e/N_mae = 0.0008\u001b[A\n",
      "f_mae = 0.0688 | f_rmse = 0.0946 | e_mae = 0.0816 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0689 | f_rmse = 0.0946 | e_mae = 0.0821 | e/N_mae = 0.0009\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 185/440 [00:05<00:05, 43.65it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0946 | e_mae = 0.0836 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0946 | e_mae = 0.0842 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0946 | e_mae = 0.0846 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0946 | e_mae = 0.0850 | e/N_mae = 0.0009\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 190/440 [00:05<00:05, 43.90it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0948 | e_mae = 0.0860 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0692 | f_rmse = 0.0948 | e_mae = 0.0867 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0692 | f_rmse = 0.0948 | e_mae = 0.0875 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0693 | f_rmse = 0.0949 | e_mae = 0.0882 | e/N_mae = 0.0009\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 195/440 [00:05<00:05, 42.58it/s]\n",
      "f_mae = 0.0694 | f_rmse = 0.0949 | e_mae = 0.0895 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0694 | f_rmse = 0.0950 | e_mae = 0.0905 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0695 | f_rmse = 0.0950 | e_mae = 0.0914 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0695 | f_rmse = 0.0950 | e_mae = 0.0918 | e/N_mae = 0.0010\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 200/440 [00:05<00:05, 42.65it/s]\n",
      "f_mae = 0.0696 | f_rmse = 0.0951 | e_mae = 0.0925 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0696 | f_rmse = 0.0951 | e_mae = 0.0933 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0696 | f_rmse = 0.0952 | e_mae = 0.0942 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0952 | e_mae = 0.0948 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0952 | e_mae = 0.0952 | e/N_mae = 0.0010\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 206/440 [00:05<00:05, 45.11it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0953 | e_mae = 0.0958 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0959 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0959 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0958 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0960 | e/N_mae = 0.0010\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 212/440 [00:05<00:04, 47.25it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0964 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0967 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0954 | e_mae = 0.0969 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0954 | e_mae = 0.0973 | e/N_mae = 0.0010\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 217/440 [00:05<00:04, 46.20it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0954 | e_mae = 0.0981 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0954 | e_mae = 0.0985 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0954 | e_mae = 0.0988 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0954 | e_mae = 0.0988 | e/N_mae = 0.0010\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 222/440 [00:05<00:04, 44.29it/s]\n",
      "f_mae = 0.0701 | f_rmse = 0.0955 | e_mae = 0.0983 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0701 | f_rmse = 0.0954 | e_mae = 0.0981 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0701 | f_rmse = 0.0954 | e_mae = 0.0978 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0954 | e_mae = 0.0976 | e/N_mae = 0.0010\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 227/440 [00:06<00:04, 42.80it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0953 | e_mae = 0.0973 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0953 | e_mae = 0.0969 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0701 | f_rmse = 0.0953 | e_mae = 0.0966 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0953 | e_mae = 0.0964 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0965 | e/N_mae = 0.0010\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 233/440 [00:06<00:04, 45.49it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0966 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0965 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0950 | e_mae = 0.0967 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0950 | e_mae = 0.0971 | e/N_mae = 0.0010\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 238/440 [00:06<00:04, 45.45it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0949 | e_mae = 0.0972 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0949 | e_mae = 0.0972 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0949 | e_mae = 0.0971 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0948 | e_mae = 0.0972 | e/N_mae = 0.0010\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 243/440 [00:06<00:04, 44.81it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0970 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0967 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0965 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0946 | e_mae = 0.0962 | e/N_mae = 0.0010\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 248/440 [00:06<00:04, 44.55it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0960 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0946 | e_mae = 0.0959 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0946 | e_mae = 0.0958 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0946 | e_mae = 0.0956 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0955 | e/N_mae = 0.0010\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 254/440 [00:06<00:03, 47.02it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0955 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.0954 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0947 | e_mae = 0.0955 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0947 | e_mae = 0.0956 | e/N_mae = 0.0010\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 259/440 [00:06<00:03, 45.69it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0948 | e_mae = 0.0958 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0948 | e_mae = 0.0959 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0948 | e_mae = 0.0960 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0948 | e_mae = 0.0961 | e/N_mae = 0.0010\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 264/440 [00:06<00:03, 45.23it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0948 | e_mae = 0.0961 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0949 | e_mae = 0.0961 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0949 | e_mae = 0.0962 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0949 | e_mae = 0.0963 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0949 | e_mae = 0.0963 | e/N_mae = 0.0010\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 270/440 [00:07<00:03, 47.41it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0950 | e_mae = 0.0960 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0950 | e_mae = 0.0958 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0950 | e_mae = 0.0955 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0950 | e_mae = 0.0953 | e/N_mae = 0.0010\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 275/440 [00:07<00:03, 46.73it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0950 | e_mae = 0.0948 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0950 | e_mae = 0.0945 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0949 | e_mae = 0.0942 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0949 | e_mae = 0.0940 | e/N_mae = 0.0010\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 280/440 [00:07<00:03, 46.62it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0949 | e_mae = 0.0936 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0949 | e_mae = 0.0933 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0949 | e_mae = 0.0930 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0948 | e_mae = 0.0928 | e/N_mae = 0.0010\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 285/440 [00:07<00:03, 47.22it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0948 | e_mae = 0.0926 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0948 | e_mae = 0.0925 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0948 | e_mae = 0.0922 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0948 | e_mae = 0.0920 | e/N_mae = 0.0010\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 290/440 [00:07<00:03, 46.03it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0947 | e_mae = 0.0915 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0947 | e_mae = 0.0913 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0947 | e_mae = 0.0910 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0947 | e_mae = 0.0909 | e/N_mae = 0.0009\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 295/440 [00:07<00:03, 45.99it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0946 | e_mae = 0.0905 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0946 | e_mae = 0.0902 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0946 | e_mae = 0.0901 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0945 | e_mae = 0.0899 | e/N_mae = 0.0009\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 300/440 [00:07<00:02, 46.91it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0945 | e_mae = 0.0897 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0944 | e_mae = 0.0897 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0944 | e_mae = 0.0898 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0944 | e_mae = 0.0899 | e/N_mae = 0.0009\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 305/440 [00:07<00:02, 46.20it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0944 | e_mae = 0.0902 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0944 | e_mae = 0.0905 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0944 | e_mae = 0.0909 | e/N_mae = 0.0009\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0945 | e_mae = 0.0914 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0945 | e_mae = 0.0918 | e/N_mae = 0.0010\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 311/440 [00:07<00:02, 46.42it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0945 | e_mae = 0.0926 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0946 | e_mae = 0.0931 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0946 | e_mae = 0.0936 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0946 | e_mae = 0.0939 | e/N_mae = 0.0010\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 316/440 [00:08<00:03, 39.45it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0947 | e_mae = 0.0945 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0947 | e_mae = 0.0948 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0947 | e_mae = 0.0952 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0947 | e_mae = 0.0957 | e/N_mae = 0.0010\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 321/440 [00:08<00:03, 37.56it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0948 | e_mae = 0.0964 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0948 | e_mae = 0.0969 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0948 | e_mae = 0.0973 | e/N_mae = 0.0010\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 325/440 [00:08<00:03, 36.42it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0949 | e_mae = 0.0978 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0949 | e_mae = 0.0979 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0949 | e_mae = 0.0981 | e/N_mae = 0.0010\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 329/440 [00:08<00:03, 35.76it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0950 | e_mae = 0.0986 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0950 | e_mae = 0.0989 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0950 | e_mae = 0.0991 | e/N_mae = 0.0010\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 333/440 [00:08<00:03, 34.08it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0996 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0998 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0999 | e/N_mae = 0.0010\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 337/440 [00:08<00:03, 34.08it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0951 | e_mae = 0.0997 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0996 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0995 | e/N_mae = 0.0010\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 341/440 [00:08<00:02, 33.77it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0953 | e_mae = 0.0991 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0989 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0986 | e/N_mae = 0.0010\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 345/440 [00:08<00:02, 34.45it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0953 | e_mae = 0.0983 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0983 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0982 | e/N_mae = 0.0010\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 349/440 [00:09<00:02, 34.92it/s]\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0981 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0982 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0700 | f_rmse = 0.0952 | e_mae = 0.0983 | e/N_mae = 0.0010\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 353/440 [00:09<00:02, 34.80it/s]\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0987 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0951 | e_mae = 0.0989 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0699 | f_rmse = 0.0950 | e_mae = 0.0993 | e/N_mae = 0.0010\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 357/440 [00:09<00:02, 33.51it/s]\n",
      "f_mae = 0.0698 | f_rmse = 0.0950 | e_mae = 0.0999 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0949 | e_mae = 0.1003 | e/N_mae = 0.0010\u001b[A\n",
      "f_mae = 0.0698 | f_rmse = 0.0949 | e_mae = 0.1006 | e/N_mae = 0.0010\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 361/440 [00:09<00:02, 31.99it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0948 | e_mae = 0.1009 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0948 | e_mae = 0.1013 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0948 | e_mae = 0.1016 | e/N_mae = 0.0011\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 365/440 [00:09<00:02, 31.48it/s]\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.1020 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.1023 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0697 | f_rmse = 0.0947 | e_mae = 0.1026 | e/N_mae = 0.0011\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 369/440 [00:09<00:02, 30.90it/s]\n",
      "f_mae = 0.0696 | f_rmse = 0.0947 | e_mae = 0.1031 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0696 | f_rmse = 0.0947 | e_mae = 0.1031 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0696 | f_rmse = 0.0947 | e_mae = 0.1032 | e/N_mae = 0.0011\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 373/440 [00:09<00:02, 30.79it/s]\n",
      "f_mae = 0.0696 | f_rmse = 0.0946 | e_mae = 0.1035 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0696 | f_rmse = 0.0946 | e_mae = 0.1036 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0695 | f_rmse = 0.0946 | e_mae = 0.1036 | e/N_mae = 0.0011\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 377/440 [00:09<00:01, 31.64it/s]\n",
      "f_mae = 0.0695 | f_rmse = 0.0945 | e_mae = 0.1041 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0695 | f_rmse = 0.0945 | e_mae = 0.1043 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0695 | f_rmse = 0.0945 | e_mae = 0.1046 | e/N_mae = 0.0011\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 381/440 [00:10<00:01, 32.65it/s]\n",
      "f_mae = 0.0694 | f_rmse = 0.0944 | e_mae = 0.1052 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0694 | f_rmse = 0.0944 | e_mae = 0.1056 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0694 | f_rmse = 0.0943 | e_mae = 0.1060 | e/N_mae = 0.0011\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 385/440 [00:10<00:01, 31.59it/s]\n",
      "f_mae = 0.0693 | f_rmse = 0.0943 | e_mae = 0.1067 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0693 | f_rmse = 0.0943 | e_mae = 0.1070 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0693 | f_rmse = 0.0942 | e_mae = 0.1073 | e/N_mae = 0.0011\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 389/440 [00:10<00:01, 32.50it/s]\n",
      "f_mae = 0.0692 | f_rmse = 0.0942 | e_mae = 0.1078 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0692 | f_rmse = 0.0942 | e_mae = 0.1082 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0692 | f_rmse = 0.0941 | e_mae = 0.1085 | e/N_mae = 0.0011\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 393/440 [00:10<00:01, 32.18it/s]\n",
      "f_mae = 0.0692 | f_rmse = 0.0941 | e_mae = 0.1090 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0692 | f_rmse = 0.0941 | e_mae = 0.1093 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0941 | e_mae = 0.1095 | e/N_mae = 0.0011\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 397/440 [00:10<00:01, 29.93it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1098 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1099 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1099 | e/N_mae = 0.0011\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 401/440 [00:10<00:01, 30.65it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1102 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1102 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1101 | e/N_mae = 0.0011\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 405/440 [00:10<00:01, 31.00it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1100 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1099 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1099 | e/N_mae = 0.0011\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 409/440 [00:10<00:00, 31.75it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1099 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1099 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1100 | e/N_mae = 0.0011\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 413/440 [00:11<00:00, 31.65it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1101 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1101 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1101 | e/N_mae = 0.0011\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 417/440 [00:11<00:00, 32.14it/s]\n",
      "f_mae = 0.0691 | f_rmse = 0.0940 | e_mae = 0.1102 | e/N_mae = 0.0011\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1104 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1106 | e/N_mae = 0.0012\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 421/440 [00:11<00:00, 33.03it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1109 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1110 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1112 | e/N_mae = 0.0012\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 425/440 [00:11<00:00, 32.38it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1114 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1115 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1116 | e/N_mae = 0.0012\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 429/440 [00:11<00:00, 31.75it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1120 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1121 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1123 | e/N_mae = 0.0012\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 433/440 [00:11<00:00, 31.59it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1125 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1126 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1127 | e/N_mae = 0.0012\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 437/440 [00:11<00:00, 31.59it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1129 | e/N_mae = 0.0012\u001b[A\n",
      "f_mae = 0.0690 | f_rmse = 0.0938 | e_mae = 0.1130 | e/N_mae = 0.0012\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440/440 [00:11<00:00, 36.86it/s]\n",
      "f_mae = 0.0690 | f_rmse = 0.0939 | e_mae = 0.1132 | e/N_mae = 0.0012\n",
      "\n",
      "--- Final result: ---\n",
      "               f_mae =  0.069039           \n",
      "              f_rmse =  0.093868           \n",
      "               e_mae =  0.113184           \n",
      "             e/N_mae =  0.001179           \n",
      "               f_mae =  0.069039           \n",
      "              f_rmse =  0.093868           \n",
      "               e_mae =  0.113184           \n",
      "             e/N_mae =  0.001179           \n"
     ]
    }
   ],
   "source": [
    "!nequip-evaluate --train-dir results300/temp300-results/atT300 --batch-size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafb171a-54f9-481f-aa20-eb89fdbd707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "INFO:root:Loading best_model.pth from training session...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "INFO:root:Compiled & optimized model.\n",
      "model1000K.pth\tsi-deployed.pth\n"
     ]
    }
   ],
   "source": [
    "!nequip-deploy build --train-dir results300/temp300-results/atT300 model300K.pth\n",
    "!ls *pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fcae3-c291-400b-9dc4-4077cfb4f91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nequip_tt",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
