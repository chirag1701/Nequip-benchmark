{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ceb19d-b62d-4c7c-80e0-c238a1dab7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82a562a-2b22-47a6-ac1e-a8b92964c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nequip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83bb046-e741-4fd9-b91f-30663e51ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dea6599-3390-48b5-842f-57904b859fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'allegro'...\n",
      "remote: Enumerating objects: 44, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 44 (delta 0), reused 24 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (44/44), 71.19 KiB | 645.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/mir-group/allegro.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c76f117d-13c8-4b4f-a7ae-90a819c6437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./allegro\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nequip>=0.5.3 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from mir-allegro==0.2.0) (0.6.1)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (2.1.2)\n",
      "Requirement already satisfied: ase in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.23.0)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.5)\n",
      "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.1)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.2)\n",
      "Requirement already satisfied: torch-runstats>=0.2.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: torch-ema>=0.3.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
      "Requirement already satisfied: sympy in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.3)\n",
      "Requirement already satisfied: scipy in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.4.1)\n",
      "Requirement already satisfied: opt-einsum-fx>=0.1.4 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: opt-einsum in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (12.6.77)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.1.5)\n",
      "Building wheels for collected packages: mir-allegro\n",
      "  Building wheel for mir-allegro (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27432 sha256=149b947ada950b9a0c2592c7dedc950c7dacf3167fe1034237f3656c6701ac38\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pvb2_sk1/wheels/88/92/d1/98ebf31f2e21a35268c80a3801704cb528e2c31746689a2b57\n",
      "Successfully built mir-allegro\n",
      "Installing collected packages: mir-allegro\n",
      "Successfully installed mir-allegro-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install allegro/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d865e4-6d71-42a5-a5e3-d676e5a246dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lammps'...\n",
      "remote: Enumerating objects: 14050, done.\u001b[K\n",
      "remote: Counting objects: 100% (14050/14050), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10186/10186), done.\u001b[K\n",
      "remote: Total 14050 (delta 4777), reused 8030 (delta 3628), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (14050/14050), 128.97 MiB | 3.67 MiB/s, done.\n",
      "Resolving deltas: 100% (4777/4777), done.\n",
      "Updating files: 100% (13396/13396), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth=1 https://github.com/lammps/lammps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f8e28db-9398-4668-8702-ba9aee20cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pair_allegro'...\n",
      "remote: Enumerating objects: 28, done.\u001b[K\n",
      "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 28 (delta 0), reused 13 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (28/28), 193.53 KiB | 1.11 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/mir-group/pair_allegro.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae39fc8-b6f1-43a2-a9d0-8c45f61c7139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-06 15:13:20--  https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.11.0%2Bcu102.zip\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 2600:9000:256e:8a00:d:607e:4540:93a1, 2600:9000:256e:9200:d:607e:4540:93a1, 2600:9000:256e:e000:d:607e:4540:93a1, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|2600:9000:256e:8a00:d:607e:4540:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 813658822 (776M) [application/zip]\n",
      "Saving to: ‘libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip’\n",
      "\n",
      "libtorch-cxx11-abi- 100%[===================>] 775.96M  3.63MB/s    in 3m 35s  \n",
      "\n",
      "2024-10-06 15:16:56 (3.60 MB/s) - ‘libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip’ saved [813658822/813658822]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.11.0%2Bcu102.zip && unzip -q libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30300888-cca5-4d98-a183-0f77fa96f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files...\n",
      "Updating CMakeLists.txt...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!cd pair_allegro && bash patch_lammps.sh ../lammps/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1da2afe-df23-4c89-a8e7-22f825e54c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mkl-include\n",
      "  Downloading mkl_include-2024.2.2-py2.py3-none-manylinux1_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading mkl_include-2024.2.2-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mkl-include\n",
      "Successfully installed mkl-include-2024.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mkl-include\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d55fa8c-0dbe-4000-98b1-cc7797539043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-06 15:38:30--  https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241006%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241006T100737Z&X-Amz-Expires=300&X-Amz-Signature=4c9ea8fe969a18d58d3efa81afaf012d197ba0eaec159d5a5a2947195c6e9e24&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-10-06 15:38:30--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241006%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241006T100737Z&X-Amz-Expires=300&X-Amz-Signature=4c9ea8fe969a18d58d3efa81afaf012d197ba0eaec159d5a5a2947195c6e9e24&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46004365 (44M) [application/octet-stream]\n",
      "Saving to: ‘cmake-3.23.1-linux-x86_64.sh’\n",
      "\n",
      "cmake-3.23.1-linux- 100%[===================>]  43.87M  3.73MB/s    in 12s     \n",
      "\n",
      "2024-10-06 15:38:43 (3.75 MB/s) - ‘cmake-3.23.1-linux-x86_64.sh’ saved [46004365/46004365]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b1cec8-f474-49ad-9719-8cd36b67292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x cmake-3.23.1-linux-x86_64.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8187524-659c-4c38-940b-8074fb77b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3946968\n",
      "drwxrwxr-x  8 chirag chirag       4096 Oct  6 15:11 allegro\n",
      "drwxrwxr-x 31 chirag chirag       4096 Oct  5 21:42 anaconda3\n",
      "drwxrwxr-x  6 chirag chirag       4096 Oct  6 15:24 cmake-3.23.1-linux-x86_64\n",
      "-rwxrwxr-x  1 chirag chirag   46004365 Apr 12  2022 cmake-3.23.1-linux-x86_64.sh\n",
      "-rw-rw-r--  1 chirag chirag 3181904146 Sep 30  2022 cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
      "drwxr-xr-x  2 chirag chirag       4096 May 31 19:55 Desktop\n",
      "drwxr-xr-x  2 chirag chirag       4096 May 31 18:46 Documents\n",
      "drwxr-xr-x  4 chirag chirag       4096 Oct  5 22:44 Downloads\n",
      "drwxrwxr-x 16 chirag chirag       4096 Oct  6 15:25 lammps\n",
      "drwxr-xr-x  6 chirag chirag       4096 Mar  8  2022 libtorch\n",
      "-rw-rw-r--  1 chirag chirag  813658822 Mar 10  2022 libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip\n",
      "drwxr-xr-x  2 chirag chirag       4096 May 31 18:46 Music\n",
      "drwxrwxr-x  6 chirag chirag       4096 Oct  6 15:13 pair_allegro\n",
      "drwxr-xr-x  4 chirag chirag       4096 Jun  1 13:44 Pictures\n",
      "drwxr-xr-x  2 chirag chirag       4096 May 31 18:46 Public\n",
      "drwx------  7 chirag chirag       4096 Jun  1 00:00 snap\n",
      "drwxr-xr-x  2 chirag chirag       4096 May 31 18:46 Templates\n",
      "-rw-rw-r--  1 chirag chirag         72 Oct  5 21:47 Untitled1.ipynb\n",
      "-rw-rw-r--  1 chirag chirag        337 Oct  6 15:05 Untitled2.ipynb\n",
      "-rw-rw-r--  1 chirag chirag        337 Oct  6 15:05 Untitled3.ipynb\n",
      "-rw-rw-r--  1 chirag chirag        337 Oct  6 15:05 Untitled4.ipynb\n",
      "-rw-rw-r--  1 chirag chirag        337 Oct  6 15:06 Untitled5.ipynb\n",
      "-rw-rw-r--  1 chirag chirag      18155 Oct  6 15:39 Untitled6.ipynb\n",
      "-rw-rw-r--  1 chirag chirag         72 Oct  5 21:46 Untitled.ipynb\n",
      "drwxr-xr-x  3 chirag chirag       4096 May 31 18:55 Videos\n",
      "drwxrwxr-x  3 chirag chirag       4096 Jun  1 20:35 Week-0_chirag\n"
     ]
    }
   ],
   "source": [
    "!ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5c7cb8-8ee1-40b6-b244-f8cce4d5baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxr-x 1 chirag chirag 44M Apr 12  2022 cmake-3.23.1-linux-x86_64.sh\n"
     ]
    }
   ],
   "source": [
    "!ls -lh cmake-3.23.1-linux-x86_64.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3270a5-fa54-4a1f-8db1-298514c27a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (775380373.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    unzip libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip -d ~/libtorch\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "unzip libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip -d ~/libtorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9725163c-23e3-4cba-854d-71f8f7160ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:18 (cmake_policy):\n",
      "  The OLD behavior for policy CMP0109 will be removed from a future version\n",
      "  of CMake.\n",
      "\n",
      "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
      "  policies are deprecated and that a policy should be set to OLD only under\n",
      "  specific short-term circumstances.  Projects should be ported to the NEW\n",
      "  behavior and not rely on setting a policy to OLD.\n",
      "\n",
      "\u001b[0m\n",
      "-- The CXX compiler identification is GNU 9.5.0\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found CUDA: /usr/local/cuda-11.8 (found version \"11.8\") \n",
      "-- The CUDA compiler identification is NVIDIA 11.8.89\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda-11.8/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Found CUDAToolkit: /usr/local/cuda-11.8/include (found version \"11.8.89\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Caffe2: CUDA detected: 11.8\n",
      "-- Caffe2: CUDA nvcc is: /usr/local/cuda-11.8/bin/nvcc\n",
      "-- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.8\n",
      "-- Caffe2: Header version is: 11.8\n",
      "-- /usr/local/cuda-11.8/lib64/libnvrtc.so shorthash is 672ee683\n",
      "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
      "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
      "-- Automatic GPU detection failed. Building for common architectures.\n",
      "-- Autodetected CUDA architecture(s): 3.5;5.0;8.0;8.6;8.9;9.0;8.9+PTX;9.0+PTX\n",
      "-- Added CUDA NVCC flags for: -gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_89,code=sm_89;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_89,code=compute_89;-gencode;arch=compute_90,code=compute_90\n",
      "\u001b[33mCMake Warning at /home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
      "  static library kineto_LIBRARY-NOTFOUND not found.\n",
      "Call Stack (most recent call first):\n",
      "  /home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n",
      "  CMakeLists.txt:50 (find_package)\n",
      "\n",
      "\u001b[0m\n",
      "-- Found Torch: /home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/lib/libtorch.so  \n",
      "-- Found Git: /usr/bin/git (found version \"2.43.0\") \n",
      "-- Running check for auto-generated files from make-based build system\n",
      "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
      "-- Found MPI: TRUE (found version \"3.1\")  \n",
      "-- Looking for C++ include omp.h\n",
      "-- Looking for C++ include omp.h - found\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\") found components: CXX \n",
      "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
      "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.43\") \n",
      "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.3\")  \n",
      "-- Found GZIP: /usr/bin/gzip  \n",
      "-- Could NOT find FFMPEG (missing: FFMPEG_EXECUTABLE) \n",
      "-- Looking for C++ include cmath\n",
      "-- Looking for C++ include cmath - found\n",
      "-- Generating style headers...\n",
      "-- Generating package headers...\n",
      "-- Generating lmpinstalledpkgs.h...\n",
      "-- Found Python3: /home/chirag/anaconda3/bin/python3 (found version \"3.12.4\") found components: Interpreter \n",
      "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"11.0\")\n",
      "-- The following tools and libraries have been found and configured:\n",
      " * CUDA\n",
      " * Threads\n",
      " * CUDAToolkit\n",
      " * Caffe2\n",
      " * Torch\n",
      " * Git\n",
      " * MPI\n",
      " * OpenMP\n",
      " * JPEG\n",
      " * PNG\n",
      " * ZLIB\n",
      " * Python3\n",
      "\n",
      "-- <<< Build configuration >>>\n",
      "   LAMMPS Version:   20240829 224468d-modified\n",
      "   Operating System: Linux Ubuntu\" 24.04\n",
      "   CMake Version:    3.28.3\n",
      "   Build type:       Release\n",
      "   Install path:     /home/chirag/.local\n",
      "   Generator:        Unix Makefiles using /usr/bin/gmake\n",
      "-- Enabled packages: <None>\n",
      "-- <<< Compilers and Flags: >>>\n",
      "-- C++ Compiler:     /usr/bin/c++\n",
      "      Type:          GNU\n",
      "      Version:       9.5.0\n",
      "      C++ Standard:  17\n",
      "      C++ Flags:     -O3 -DNDEBUG\n",
      "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP\n",
      "-- <<< Linker flags: >>>\n",
      "-- Executable name:  lmp\n",
      "-- Static library flags:    \n",
      "-- <<< MPI flags >>>\n",
      "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
      "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\n",
      "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
      "\u001b[33mCMake Warning at /home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
      "  static library kineto_LIBRARY-NOTFOUND not found.\n",
      "Call Stack (most recent call first):\n",
      "  /home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n",
      "  CMakeLists.txt:1095 (find_package)\n",
      "\n",
      "\u001b[0m\n",
      "-- Configuring done (3.4s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/chirag/lammps/build\n",
      "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
      "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
      "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/format.h\u001b[0m\n",
      "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/command.h\u001b[0m\n",
      "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
      "-- Git Directory: /home/chirag/lammps/.git\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
      "[  2%] Built target fmt_format.h\n",
      "[  2%] Built target dihedral.h\n",
      "[  2%] Built target comm.h\n",
      "[  2%] Built target error.h\n",
      "[  2%] Built target compute.h\n",
      "[  2%] Built target angle.h\n",
      "[  2%] Built target atom.h\n",
      "[  2%] Built target bond.h\n",
      "[  2%] Built target command.h\n",
      "-- Generating lmpgitversion.h...\n",
      "[  2%] Built target citeme.h\n",
      "[  2%] Built target domain.h\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/exceptions.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
      "[  5%] Built target gitversion\n",
      "[  5%] Built target group.h\n",
      "[  5%] Built target exceptions.h\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
      "[  5%] Built target improper.h\n",
      "[  5%] Built target force.h\n",
      "[  5%] Built target input.h\n",
      "[  5%] Built target fix.h\n",
      "[  5%] Built target kspace.h\n",
      "[  5%] Built target info.h\n",
      "[  5%] Built target lattice.h\n",
      "[  5%] Built target lammps.h\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
      "[  5%] Built target library.h\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/platform.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
      "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
      "[  7%] Built target lmppython.h\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
      "[  8%] Built target modify.h\n",
      "[  8%] Built target memory.h\n",
      "[  8%] Built target lmptype.h\n",
      "[  8%] Built target neighbor.h\n",
      "[  8%] Built target pointers.h\n",
      "[  8%] Built target output.h\n",
      "[  8%] Built target region.h\n",
      "[  8%] Built target neigh_list.h\n",
      "[  8%] Built target platform.h\n",
      "[  8%] Built target pair.h\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/core.h\u001b[0m\n",
      "[  8%] Built target timer.h\n",
      "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
      "[  9%] Built target update.h\n",
      "[  9%] Built target utils.h\n",
      "[  9%] Built target universe.h\n",
      "[  9%] Built target fmt_core.h\n",
      "[  9%] Built target variable.h\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/angle_zero.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/angle_write.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/angle.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/arg_info.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_map.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/balance.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/body.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/bond.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/bond_zero.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/change_box.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/citeme.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/comm.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/comm_brick.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_allegro.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_angle.cpp.o\u001b[0m\n",
      "In file included from \u001b[01m\u001b[K/usr/include/string.h:548\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/chirag/lammps/src/fmt/base.h:18\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/chirag/lammps/src/fmt/format.h:41\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/chirag/lammps/src/pointers.h:33\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/chirag/lammps/src/atom_vec.h:17\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/chirag/lammps/src/atom_vec.cpp:14\u001b[m\u001b[K:\n",
      "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvirtual void LAMMPS_NS::AtomVec::write_data_restricted_to_general()\u001b[m\u001b[K’ at \u001b[01m\u001b[K/home/chirag/lammps/src/atom_vec.cpp:2272:21\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin_memcpy(void*, const void*, long unsigned int)\u001b[m\u001b[K’ specified bound between 18446744056529682432 and 18446744073709551592 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
      "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
      "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "   30 | \u001b[01;35m\u001b[K     __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
      "      |      \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K  \n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_bond.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_chunk.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_com.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_count_type.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_improper.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_ke.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_msd.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_pair.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_pe.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_property_grid.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_slice.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/create_atoms.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/create_bonds.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/create_box.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/deprecated.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dihedral.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dihedral_write.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/domain.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_atom.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_custom.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_grid.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_grid_vtk.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_local.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_image.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_movie.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/error.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/finish.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_grid.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_balance.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_bond_history.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_deform.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_efield.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_external.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_group.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_halt.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_heat.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_indent.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_move.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nh.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nph.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_npt.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nve.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_pair.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_press_langevin.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_print.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_respa.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_spring.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_store_atom.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_store_global.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_store_local.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_vector.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fix_wall_table.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/force.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/grid2d.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/grid3d.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/group.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/hashlittle.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/image.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/imbalance.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/improper.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/improper_zero.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/info.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/input.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/integrate.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/irregular.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/kspace.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/label_map.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/lammps.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/lattice.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/library.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/lmppython.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/math_eigen.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/math_extra.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/math_special.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/memory.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_cg.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_deprecated.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_fire.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_hftn.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/min_sd.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/minimize.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/modify.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/molecule.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/my_page.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nbin.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/neigh_list.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/neigh_request.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/neighbor.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_bin.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_bin_ghost.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_copy.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_halffull.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_multi.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_multi_old.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_nsq.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_nsq_ghost.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_respa_bin.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_respa_nsq.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_skip.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nstencil.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/npair_trim.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nstencil_bin.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nstencil_ghost_bin.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nstencil_multi.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/nstencil_multi_old.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/output.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_allegro.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_born.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_buck.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_hybrid_molecular.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_morse.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_soft.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_table.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/pair_zero.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/platform.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/procmap.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/random_mars.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/random_park.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/rcb.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/read_data.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/read_dump.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/read_restart.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/reader.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/reader_native.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_block.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_cone.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_ellipsoid.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_intersect.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_plane.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_prism.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_sphere.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/region_union.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/replicate.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/rerun.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/reset_atoms_id.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/reset_atoms_image.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/reset_atoms_mol.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/respa.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/run.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/set.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/special.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/tabular_function.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/thermo.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/timer.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/tokenizer.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/universe.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/update.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/utils.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/variable.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/velocity.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/verlet.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/write_coeff.cpp.o\u001b[0m\n",
      "\u001b[01m\u001b[K/home/chirag/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/home/chirag/lammps/src/variable.cpp:799:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  799 |         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
      "      |         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/write_data.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/write_dump.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/home/chirag/lammps/src/write_restart.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
      "[ 98%] Built target lammps\n",
      "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/home/chirag/lammps/src/main.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
      "[100%] Built target lmp\n"
     ]
    }
   ],
   "source": [
    "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df27171f-8c77-42e1-906a-40fba4741ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports and pre-definitions\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.size'] = 30\n",
    "\n",
    "def parse_lammps_rdf(rdffile):\n",
    "    \"\"\"Parse the RDF file written by LAMMPS\n",
    "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
    "    \"\"\"\n",
    "    with open(rdffile, 'r') as rdfout:\n",
    "        rdfs = []; buffer = []\n",
    "        for line in rdfout:\n",
    "            values = line.split()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elif len(values) == 2:\n",
    "                nbins = values[1]\n",
    "            else:\n",
    "                buffer.append([float(values[1]), float(values[2])])\n",
    "                if len(buffer) == int(nbins):\n",
    "                    frame = np.transpose(np.array(buffer))\n",
    "                    rdfs.append(frame)\n",
    "                    buffer = []\n",
    "    return rdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a62306c-72d0-45b7-ba91-9c5447799d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from gdown) (4.66.5)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/nequip_tt/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 gdown-5.2.0 soupsieve-2.6\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Retrieving folder contents\n",
      "Processing file 1zh75BVuaoopTQf9_LOnRylTMnpL-HtL6 sitraj.xyz\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zh75BVuaoopTQf9_LOnRylTMnpL-HtL6\n",
      "To: /home/chirag/Si_data/sitraj.xyz\n",
      "100%|████████████████████████████████████████| 785k/785k [00:00<00:00, 2.01MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown  # Install gdown if you haven't already\n",
    "!gdown --folder --id 1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0 --no-cookies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa609f3-3490-49c8-bb1f-7a8f63ca8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: gdown: command not found\n",
      "mkdir: cannot create directory ‘Si_data’: File exists\n",
      "mv: cannot stat '*.xyz': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0?usp=sharing\n",
    "!mkdir Si_data\n",
    "!mv *.xyz ./Si_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ce99ea-8970-4fed-b13c-8a88b1b18967",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls Si_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d2a7d15-189f-4881-b5cd-6a57e6f324fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/train.py:180: UserWarning: default_dtype=float32 but we strongly recommend float64\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchirag17\u001b[0m (\u001b[33mchirag17-indian-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/chirag/wandb/run-20241007_012330-5GPZYqL31nE6JUnpWkvBvrnc-Iix0NLTI8rZmaBIfSY\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial/runs/5GPZYqL31nE6JUnpWkvBvrnc-Iix0NLTI8rZmaBIfSY\u001b[0m\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  datas = [torch.load(d) for d in datas]\n",
      "Loaded data: Batch(atomic_numbers=[7040, 1], batch=[7040], cell=[110, 3, 3], edge_cell_shift=[197238, 3], edge_index=[2, 197238], forces=[7040, 3], free_energy=[110], pbc=[110, 3], pos=[7040, 3], ptr=[111], stress=[110, 3, 3], total_energy=[110, 1])\n",
      "    processed data size: ~5.54 MB\n",
      "Cached processed data to disk\n",
      "Done!\n",
      "Successfully loaded the data set of type ASEDataset(110)...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Replace string dataset_per_atom_total_energy_mean to -129.9612274169922\n",
      "Atomic outputs are scaled by: [Si: None], shifted by [Si: -129.961227].\n",
      "Replace string dataset_forces_rms to 0.9016265869140625\n",
      "Initially outputs are globally scaled by: 0.9016265869140625, total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Equivariance test passed; equivariance errors:\n",
      "   Errors are in real units, where relevant.\n",
      "   Please note that the large scale of the typical\n",
      "   shifts to the (atomic) energy can cause\n",
      "   catastrophic cancellation and give incorrectly\n",
      "   the equivariance error as zero for those fields.\n",
      "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
      "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
      "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_embedding             -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_cutoff                -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
      "   edge permutation equivariance of field edge_features              -> max error=1.192e-07\n",
      "   edge permutation equivariance of field edge_energy                -> max error=6.706e-08\n",
      "   node permutation equivariance of field atomic_energy              -> max error=0.000e+00\n",
      "   edge & node permutation invariance for field total_energy         -> max error=0.000e+00\n",
      "   node permutation equivariance of field forces                     -> max error=2.012e-07\n",
      "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=4.665e-07\n",
      "   (parity_k=0, did_translate=False, field=cell                )     -> max error=3.805e-07\n",
      "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=9.655e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=2.812e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=6.899e-06\n",
      "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=3.725e-07\n",
      "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=False, field=forces              )     -> max error=2.133e-06\n",
      "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=9.486e-07\n",
      "   (parity_k=0, did_translate=True , field=cell                )     -> max error=3.838e-07\n",
      "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=8.025e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_cutoff         )     -> max error=4.768e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=3.172e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=6.884e-06\n",
      "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=5.513e-07\n",
      "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=0, did_translate=True , field=forces              )     -> max error=3.167e-06\n",
      "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=4.743e-07\n",
      "   (parity_k=1, did_translate=False, field=cell                )     -> max error=4.675e-07\n",
      "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=7.878e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_cutoff         )     -> max error=4.768e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.654e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=4.585e-06\n",
      "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=5.513e-07\n",
      "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=False, field=forces              )     -> max error=1.340e-06\n",
      "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=4.160e-07\n",
      "   (parity_k=1, did_translate=True , field=cell                )     -> max error=4.572e-07\n",
      "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=9.274e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_cutoff         )     -> max error=5.722e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=2.237e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=6.369e-06\n",
      "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=4.992e-07\n",
      "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
      "   (parity_k=1, did_translate=True , field=forces              )     -> max error=2.434e-06\n",
      "Number of weights: 37352\n",
      "Number of trainable weights: 37352\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      0     2         1.01        0.996       0.0137        0.718          0.9         6.75        0.105\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Initial Validation          0    0.108    0.002        0.992        0.014         1.01        0.709        0.898         6.78        0.106\n",
      "Wall time: 0.10781012299958093\n",
      "! Best model        0    1.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1    10        0.613         0.61      0.00226        0.572        0.704         2.75       0.0429\n",
      "      1    20        0.276        0.276     3.71e-05        0.369        0.474        0.351      0.00548\n",
      "      1    30        0.142        0.142     2.82e-05        0.268         0.34        0.306      0.00478\n",
      "      1    40       0.0946       0.0944     0.000181        0.223        0.277        0.775       0.0121\n",
      "      1    50       0.0401       0.0401     2.17e-05        0.142        0.181        0.269       0.0042\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1     2       0.0621       0.0621     1.45e-05        0.179        0.225        0.153       0.0024\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               1    1.374    0.002        0.291       0.0011        0.292        0.351        0.486         1.41        0.022\n",
      "! Validation          1    1.374    0.002       0.0711     2.52e-05       0.0712        0.183         0.24        0.209      0.00327\n",
      "Wall time: 1.3741461199997502\n",
      "! Best model        1    0.071\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      2    10       0.0293       0.0292     4.37e-05        0.119        0.154        0.382      0.00597\n",
      "      2    20       0.0287       0.0287     9.31e-06        0.123        0.153        0.176      0.00275\n",
      "      2    30       0.0254       0.0254     3.13e-06        0.116        0.144        0.103       0.0016\n",
      "      2    40       0.0378       0.0378     6.34e-06        0.143        0.175        0.146      0.00227\n",
      "      2    50       0.0209       0.0209      2.8e-05        0.106         0.13        0.306      0.00478\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      2     2       0.0257       0.0257     2.51e-05        0.114        0.145        0.263       0.0041\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               2    2.520    0.002       0.0314     7.06e-05       0.0315        0.124         0.16         0.38      0.00594\n",
      "! Validation          2    2.520    0.002       0.0285     2.41e-05       0.0286        0.114        0.152        0.242      0.00378\n",
      "Wall time: 2.5204474819993266\n",
      "! Best model        2    0.029\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      3    10       0.0219       0.0217     0.000215        0.106        0.133        0.847       0.0132\n",
      "      3    20       0.0218       0.0218     9.04e-06        0.105        0.133        0.174      0.00272\n",
      "      3    30       0.0258       0.0258     3.35e-08        0.114        0.145       0.0107     0.000168\n",
      "      3    40       0.0195       0.0195     1.54e-05       0.0986        0.126        0.227      0.00354\n",
      "      3    50       0.0295       0.0294      9.3e-05        0.119        0.155        0.557       0.0087\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      3     2       0.0236       0.0236     1.71e-05        0.109        0.138        0.205       0.0032\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               3    3.634    0.002        0.024     5.44e-05       0.0241        0.109         0.14        0.342      0.00535\n",
      "! Validation          3    3.634    0.002        0.026     1.35e-05       0.0261         0.11        0.146        0.184      0.00287\n",
      "Wall time: 3.634711389999211\n",
      "! Best model        3    0.026\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      4    10       0.0222       0.0221     5.75e-05        0.108        0.134        0.438      0.00685\n",
      "      4    20       0.0254       0.0254     5.38e-06        0.115        0.144        0.134      0.00209\n",
      "      4    30       0.0253       0.0253     4.12e-06        0.113        0.144        0.117      0.00183\n",
      "      4    40       0.0242       0.0241     2.88e-05        0.109         0.14         0.31      0.00484\n",
      "      4    50        0.021       0.0209     0.000155        0.102         0.13        0.718       0.0112\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      4     2       0.0208       0.0208     2.15e-05        0.103         0.13        0.239      0.00374\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               4    4.807    0.002       0.0214     5.86e-05       0.0214        0.103        0.132        0.375      0.00586\n",
      "! Validation          4    4.807    0.002       0.0231     1.97e-05       0.0231        0.104        0.137        0.214      0.00334\n",
      "Wall time: 4.807245329999205\n",
      "! Best model        4    0.023\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      5    10         0.02       0.0199     3.97e-05        0.103        0.127        0.364      0.00569\n",
      "      5    20       0.0159       0.0159     2.04e-05       0.0923        0.114        0.261      0.00407\n",
      "      5    30       0.0184       0.0184     3.54e-07       0.0972        0.122       0.0352     0.000549\n",
      "      5    40       0.0178       0.0178     9.78e-06        0.096         0.12         0.18      0.00281\n",
      "      5    50        0.018        0.018     1.09e-05        0.093        0.121         0.19      0.00298\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      5     2       0.0166       0.0165     1.99e-05       0.0917        0.116        0.229      0.00357\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               5    5.987    0.002       0.0173     7.58e-05       0.0174       0.0929        0.119        0.396      0.00618\n",
      "! Validation          5    5.987    0.002       0.0188     1.88e-05       0.0188       0.0947        0.124        0.207      0.00324\n",
      "Wall time: 5.987445412999477\n",
      "! Best model        5    0.019\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      6    10       0.0169       0.0169     4.95e-05       0.0964        0.117        0.405      0.00633\n",
      "      6    20       0.0162       0.0162     6.73e-06       0.0899        0.115         0.15      0.00235\n",
      "      6    30       0.0127       0.0125     0.000238       0.0805        0.101        0.891       0.0139\n",
      "      6    40       0.0134       0.0134     3.73e-07       0.0834        0.104       0.0352     0.000549\n",
      "      6    50       0.0135       0.0135     9.24e-07       0.0822        0.105       0.0557      0.00087\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      6     2       0.0137       0.0137     1.68e-05       0.0835        0.105        0.206      0.00322\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               6    7.725    0.002       0.0146     5.24e-05       0.0147       0.0855        0.109        0.347      0.00543\n",
      "! Validation          6    7.725    0.002       0.0159     1.64e-05       0.0159       0.0876        0.114        0.192      0.00301\n",
      "Wall time: 7.725107418999869\n",
      "! Best model        6    0.016\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      7    10       0.0158       0.0158     2.38e-07       0.0894        0.113       0.0273     0.000427\n",
      "      7    20       0.0139       0.0136     0.000299       0.0848        0.105        0.999       0.0156\n",
      "      7    30       0.0101       0.0101     9.84e-07       0.0717       0.0906       0.0566     0.000885\n",
      "      7    40       0.0128       0.0127     7.91e-05       0.0803        0.102        0.514      0.00803\n",
      "      7    50       0.0159       0.0159     1.18e-05       0.0926        0.114        0.198       0.0031\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      7     2       0.0125       0.0125     1.33e-05       0.0797        0.101        0.185      0.00289\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               7    9.530    0.002       0.0134     6.45e-05       0.0135       0.0819        0.105        0.378      0.00591\n",
      "! Validation          7    9.530    0.002       0.0145     1.32e-05       0.0146        0.084        0.109        0.178      0.00278\n",
      "Wall time: 9.530689936999806\n",
      "! Best model        7    0.015\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      8    10       0.0208       0.0206     0.000234       0.0909        0.129        0.883       0.0138\n",
      "      8    20       0.0127       0.0127     5.36e-07       0.0832        0.102        0.042     0.000656\n",
      "      8    30       0.0156       0.0155     9.66e-05       0.0893        0.112        0.567      0.00887\n",
      "      8    40       0.0146       0.0146     1.51e-05       0.0821        0.109        0.224      0.00349\n",
      "      8    50       0.0124       0.0123     1.32e-05       0.0789          0.1        0.209      0.00327\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      8     2       0.0119       0.0119     1.09e-05       0.0775       0.0982        0.166      0.00259\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               8   11.337    0.002       0.0129     3.59e-05        0.013       0.0803        0.102        0.275       0.0043\n",
      "! Validation          8   11.337    0.002       0.0138     1.06e-05       0.0138       0.0819        0.106        0.163      0.00255\n",
      "Wall time: 11.337398972999836\n",
      "! Best model        8    0.014\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      9    10       0.0122       0.0121     0.000105       0.0812       0.0992        0.592      0.00925\n",
      "      9    20       0.0155       0.0154     6.08e-05       0.0896        0.112         0.45      0.00703\n",
      "      9    30       0.0115       0.0115     3.63e-05       0.0778       0.0966        0.348      0.00543\n",
      "      9    40      0.00774      0.00774     1.83e-07       0.0641       0.0793       0.0254     0.000397\n",
      "      9    50        0.014       0.0139     2.38e-05       0.0873        0.106        0.281      0.00439\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      9     2       0.0115       0.0115     9.88e-06       0.0763       0.0967        0.152      0.00237\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               9   13.172    0.002       0.0126     3.74e-05       0.0126       0.0793        0.101        0.299      0.00467\n",
      "! Validation          9   13.172    0.002       0.0134     9.33e-06       0.0134       0.0806        0.104        0.152      0.00238\n",
      "Wall time: 13.172606729999643\n",
      "! Best model        9    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     10    10       0.0152       0.0152      3.4e-05       0.0884        0.111        0.336      0.00525\n",
      "     10    20       0.0123       0.0123     4.33e-05       0.0797       0.0998        0.379      0.00592\n",
      "     10    30      0.00958      0.00955     2.77e-05       0.0723       0.0881        0.304      0.00475\n",
      "     10    40       0.0136       0.0136     2.19e-05       0.0862        0.105        0.271      0.00423\n",
      "     10    50       0.0111        0.011      3.2e-05       0.0786       0.0947        0.327      0.00511\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     10     2       0.0112       0.0112     9.24e-06       0.0754       0.0954        0.142      0.00221\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              10   14.781    0.002       0.0123      5.6e-05       0.0123       0.0782       0.0999         0.36      0.00563\n",
      "! Validation         10   14.781    0.002        0.013     8.61e-06        0.013       0.0796        0.103        0.145      0.00226\n",
      "Wall time: 14.781402743999934\n",
      "! Best model       10    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     11    10       0.0115       0.0115     2.33e-06       0.0769       0.0967       0.0879      0.00137\n",
      "     11    20     4.25e-05     1.54e-12     4.25e-05     9.38e-07     1.12e-06        0.376      0.00587\n",
      "     11    30      0.00911      0.00906     5.34e-05       0.0677       0.0858        0.422      0.00659\n",
      "     11    40       0.0118       0.0118     1.72e-06       0.0801       0.0979       0.0762      0.00119\n",
      "     11    50       0.0135       0.0135     1.53e-05       0.0818        0.105        0.225      0.00351\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     11     2        0.011        0.011     9.49e-06       0.0746       0.0945        0.142      0.00222\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              11   16.192    0.002        0.012     2.46e-05       0.0121       0.0775       0.0989        0.231      0.00361\n",
      "! Validation         11   16.192    0.002       0.0127     8.36e-06       0.0128       0.0788        0.102        0.142      0.00222\n",
      "Wall time: 16.19210561899945\n",
      "! Best model       11    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     12    10       0.0137       0.0135     0.000163       0.0849        0.105        0.736       0.0115\n",
      "     12    20       0.0113       0.0109     0.000442       0.0762        0.094         1.21        0.019\n",
      "     12    30       0.0139       0.0138     7.43e-05       0.0845        0.106        0.497      0.00777\n",
      "     12    40       0.0139       0.0137     0.000194       0.0786        0.106        0.804       0.0126\n",
      "     12    50      0.00851      0.00842     8.58e-05        0.065       0.0827        0.534      0.00835\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     12     2       0.0108       0.0108     1.02e-05       0.0738       0.0936        0.145      0.00227\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              12   17.493    0.002       0.0118     0.000193        0.012       0.0768        0.098        0.715       0.0112\n",
      "! Validation         12   17.493    0.002       0.0125     8.59e-06       0.0125       0.0779        0.101        0.141       0.0022\n",
      "Wall time: 17.493175118999716\n",
      "! Best model       12    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     13    10     4.76e-06     1.21e-12     4.76e-06     8.36e-07     9.93e-07        0.126      0.00197\n",
      "     13    20       0.0134       0.0134     2.25e-05       0.0846        0.104        0.274      0.00429\n",
      "     13    30       0.0115       0.0115     1.22e-05       0.0764       0.0967        0.202      0.00316\n",
      "     13    40       0.0133       0.0133     3.36e-05       0.0829        0.104        0.335      0.00523\n",
      "     13    50       0.0119       0.0118     5.77e-05       0.0776       0.0979        0.438      0.00685\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     13     2       0.0106       0.0106     1.13e-05       0.0732       0.0927        0.149      0.00233\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              13   18.750    0.002       0.0116     5.85e-05       0.0117       0.0762       0.0972        0.343      0.00537\n",
      "! Validation         13   18.750    0.002       0.0123     9.15e-06       0.0123       0.0772       0.0998        0.142      0.00222\n",
      "Wall time: 18.75065186699976\n",
      "! Best model       13    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     14    10       0.0127       0.0127     7.83e-07       0.0821        0.101       0.0508     0.000793\n",
      "     14    20       0.0116       0.0115     1.22e-05       0.0771       0.0968        0.201      0.00314\n",
      "     14    30       0.0128       0.0128     2.46e-05        0.081        0.102        0.286      0.00447\n",
      "     14    40       0.0132       0.0132      2.1e-05       0.0819        0.104        0.265      0.00414\n",
      "     14    50       0.0113       0.0113     1.01e-06       0.0783       0.0958       0.0576       0.0009\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     14     2       0.0104       0.0104     1.23e-05       0.0724       0.0919         0.15      0.00234\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              14   19.910    0.002       0.0114     4.53e-05       0.0115       0.0756       0.0964          0.3      0.00468\n",
      "! Validation         14   19.910    0.002        0.012     9.76e-06        0.012       0.0765       0.0989        0.144      0.00225\n",
      "Wall time: 19.91001800399954\n",
      "! Best model       14    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     15    10        0.012       0.0119     1.61e-05       0.0778       0.0985        0.231      0.00362\n",
      "     15    20       0.0119       0.0119     2.85e-05       0.0794       0.0983        0.309      0.00482\n",
      "     15    30       0.0192       0.0187      0.00047       0.0856        0.123         1.25       0.0196\n",
      "     15    40      0.00963       0.0096     2.74e-05       0.0713       0.0883        0.302      0.00471\n",
      "     15    50       0.0132       0.0132     1.85e-05        0.077        0.103        0.249      0.00389\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     15     2       0.0102       0.0102      1.3e-05       0.0718       0.0911        0.152      0.00238\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              15   21.091    0.002       0.0112       0.0001       0.0113       0.0748       0.0954        0.456      0.00713\n",
      "! Validation         15   21.091    0.002       0.0118     1.02e-05       0.0118       0.0759       0.0979        0.146      0.00228\n",
      "Wall time: 21.09177736899983\n",
      "! Best model       15    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     16    10      0.00956      0.00948     7.72e-05        0.071       0.0878        0.507      0.00792\n",
      "     16    20       0.0101      0.00997     8.64e-05        0.071         0.09        0.536      0.00838\n",
      "     16    30       0.0129       0.0128     0.000113       0.0821        0.102        0.613      0.00958\n",
      "     16    40        0.011       0.0108     0.000172        0.075       0.0938        0.756       0.0118\n",
      "     16    50       0.0118       0.0118     4.56e-06       0.0771       0.0981        0.123      0.00192\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     16     2         0.01         0.01      1.5e-05       0.0712       0.0903        0.166      0.00259\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              16   22.428    0.002        0.011        8e-05       0.0111       0.0741       0.0946        0.428      0.00669\n",
      "! Validation         16   22.428    0.002       0.0116     1.16e-05       0.0116       0.0752        0.097        0.155      0.00243\n",
      "Wall time: 22.428705257999354\n",
      "! Best model       16    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     17    10       0.0118       0.0118     3.47e-06        0.079       0.0978        0.107      0.00168\n",
      "     17    20       0.0117       0.0116     7.67e-05       0.0777       0.0971        0.506       0.0079\n",
      "     17    30       0.0122       0.0122     7.13e-06       0.0779       0.0994        0.154      0.00241\n",
      "     17    40       0.0102       0.0102     6.32e-05       0.0731       0.0909        0.458      0.00716\n",
      "     17    50       0.0129       0.0129     1.81e-05       0.0762        0.102        0.245      0.00383\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     17     2      0.00986      0.00984     1.56e-05       0.0705       0.0894         0.17      0.00265\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              17   24.027    0.002       0.0108     6.89e-05       0.0109       0.0735       0.0937        0.393      0.00614\n",
      "! Validation         17   24.027    0.002       0.0113     1.21e-05       0.0113       0.0744       0.0959        0.157      0.00246\n",
      "Wall time: 24.02775357199971\n",
      "! Best model       17    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     18    10        0.012        0.012     5.23e-05       0.0803       0.0987        0.418      0.00653\n",
      "     18    20       0.0111        0.011     9.12e-05       0.0772       0.0945        0.552      0.00862\n",
      "     18    30       0.0124       0.0122     0.000121       0.0797       0.0997        0.635      0.00992\n",
      "     18    40       0.0116       0.0115      5.7e-05       0.0762       0.0968        0.436      0.00681\n",
      "     18    50        0.013        0.013     2.77e-06       0.0835        0.103       0.0967      0.00151\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     18     2      0.00969      0.00968     1.66e-05       0.0699       0.0887        0.175      0.00273\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              18   25.652    0.002       0.0107     2.88e-05       0.0107       0.0731       0.0933        0.248      0.00388\n",
      "! Validation         18   25.652    0.002       0.0111     1.29e-05       0.0111       0.0738        0.095        0.161      0.00251\n",
      "Wall time: 25.652387700999498\n",
      "! Best model       18    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     19    10      0.00968      0.00959     9.21e-05       0.0685       0.0883        0.554      0.00865\n",
      "     19    20       0.0119       0.0116      0.00031       0.0783       0.0971         1.01       0.0159\n",
      "     19    30      0.00858      0.00832     0.000268       0.0658       0.0822        0.945       0.0148\n",
      "     19    40      0.00969      0.00956     0.000129       0.0691       0.0882        0.654       0.0102\n",
      "     19    50        0.012       0.0118       0.0002       0.0769        0.098        0.815       0.0127\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     19     2      0.00953      0.00951     1.74e-05       0.0693       0.0879        0.182      0.00285\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              19   27.256    0.002       0.0105      0.00013       0.0107       0.0726       0.0926        0.533      0.00832\n",
      "! Validation         19   27.256    0.002       0.0109     1.35e-05       0.0109       0.0731        0.094        0.165      0.00258\n",
      "Wall time: 27.25675554099962\n",
      "! Best model       19    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     20    10       0.0115        0.011     0.000519       0.0741       0.0944         1.31       0.0205\n",
      "     20    20       0.0111        0.011     0.000146       0.0746       0.0945        0.697       0.0109\n",
      "     20    30      0.00949      0.00941     8.78e-05       0.0684       0.0874        0.541      0.00845\n",
      "     20    40       0.0119       0.0116     0.000369       0.0785       0.0969         1.11       0.0173\n",
      "     20    50      0.00964      0.00964     1.76e-06       0.0721       0.0885       0.0762      0.00119\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     20     2      0.00936      0.00934     1.82e-05       0.0687       0.0872         0.19      0.00297\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              20   28.884    0.002       0.0103     0.000156       0.0104       0.0717       0.0914        0.595       0.0093\n",
      "! Validation         20   28.884    0.002       0.0106     1.41e-05       0.0106       0.0724        0.093        0.169      0.00264\n",
      "Wall time: 28.884313121999185\n",
      "! Best model       20    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     21    10      0.00954      0.00954     7.83e-07       0.0705       0.0881       0.0518     0.000809\n",
      "     21    20      0.00967      0.00966     5.31e-06       0.0722       0.0886        0.133      0.00208\n",
      "     21    30       0.0114       0.0109     0.000505       0.0758       0.0941          1.3       0.0203\n",
      "     21    40       0.0091      0.00899      0.00011       0.0684       0.0855        0.604      0.00943\n",
      "     21    50       0.0108       0.0108     2.77e-05       0.0758       0.0936        0.304      0.00475\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     21     2       0.0092      0.00919     1.57e-05       0.0681       0.0864        0.166      0.00259\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              21   30.449    0.002       0.0102     7.72e-05       0.0102       0.0714       0.0909        0.388      0.00606\n",
      "! Validation         21   30.449    0.002       0.0104     1.23e-05       0.0104       0.0718        0.092        0.154      0.00241\n",
      "Wall time: 30.44988977599951\n",
      "! Best model       21    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     22    10      0.00894      0.00893     1.03e-05       0.0685       0.0852        0.185      0.00288\n",
      "     22    20      0.00983      0.00983     5.96e-08       0.0731       0.0894       0.0137     0.000214\n",
      "     22    30       0.0106       0.0105     0.000152       0.0741       0.0924         0.71       0.0111\n",
      "     22    40      0.00969      0.00957     0.000123       0.0673       0.0882        0.641         0.01\n",
      "     22    50      0.00994      0.00994     5.96e-08       0.0735       0.0899       0.0146     0.000229\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     22     2      0.00905      0.00903      1.4e-05       0.0676       0.0857        0.154       0.0024\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              22   32.126    0.002       0.0101     9.22e-05       0.0101       0.0709       0.0904        0.451      0.00705\n",
      "! Validation         22   32.126    0.002       0.0102     1.11e-05       0.0102       0.0711        0.091        0.146      0.00228\n",
      "Wall time: 32.126830427999266\n",
      "! Best model       22    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     23    10      0.00676      0.00642     0.000335       0.0597       0.0722         1.06       0.0165\n",
      "     23    20       0.0105       0.0104     8.22e-05       0.0717        0.092        0.523      0.00818\n",
      "     23    30       0.0118       0.0118     1.34e-05       0.0741       0.0978        0.212      0.00331\n",
      "     23    40       0.0111       0.0111     5.61e-05       0.0741       0.0949        0.432      0.00674\n",
      "     23    50       0.0122       0.0122     4.12e-06       0.0748       0.0994        0.117      0.00183\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     23     2      0.00891       0.0089     1.21e-05       0.0671       0.0851        0.147       0.0023\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              23   33.964    0.002      0.00983       0.0001      0.00993       0.0701       0.0894         0.45      0.00703\n",
      "! Validation         23   33.964    0.002      0.00999     9.91e-06         0.01       0.0706       0.0901        0.142      0.00222\n",
      "Wall time: 33.96411189099945\n",
      "! Best model       23    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     24    10       0.0119       0.0119     1.03e-05       0.0784       0.0983        0.185      0.00288\n",
      "     24    20      0.00686      0.00685      5.1e-06       0.0592       0.0746        0.131      0.00204\n",
      "     24    30      0.00969      0.00968     9.78e-06        0.073       0.0887        0.181      0.00282\n",
      "     24    40      0.00918      0.00911     7.64e-05       0.0703        0.086        0.504      0.00787\n",
      "     24    50      0.00896      0.00891     4.29e-05       0.0652       0.0851        0.377      0.00589\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     24     2      0.00878      0.00877     1.08e-05       0.0666       0.0844        0.143      0.00223\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              24   35.681    0.002      0.00973     6.85e-05       0.0098       0.0698        0.089        0.372      0.00581\n",
      "! Validation         24   35.681    0.002       0.0098     9.19e-06      0.00981         0.07       0.0893        0.142      0.00222\n",
      "Wall time: 35.681095809\n",
      "! Best model       24    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     25    10       0.0104       0.0101     0.000306       0.0733       0.0905         1.01       0.0158\n",
      "     25    20       0.0108       0.0106     0.000239       0.0726       0.0927        0.892       0.0139\n",
      "     25    30       0.0101      0.00998     7.91e-05       0.0722       0.0901        0.513      0.00801\n",
      "     25    40       0.0117       0.0115     0.000187       0.0752       0.0967        0.788       0.0123\n",
      "     25    50      0.00921      0.00921     3.02e-07       0.0672       0.0865       0.0312     0.000488\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     25     2      0.00865      0.00864     8.87e-06       0.0662       0.0838        0.134       0.0021\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              25   37.289    0.002      0.00959     0.000195      0.00979       0.0694       0.0883        0.649       0.0101\n",
      "! Validation         25   37.289    0.002      0.00961     8.34e-06      0.00962       0.0695       0.0884        0.143      0.00223\n",
      "Wall time: 37.289391552999405\n",
      "! Best model       25    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     26    10      0.00626      0.00626      1.8e-06       0.0585       0.0713       0.0771      0.00121\n",
      "     26    20       0.0105      0.00981     0.000714       0.0719       0.0893         1.54       0.0241\n",
      "     26    30       0.0107      0.00987     0.000874       0.0729       0.0896         1.71       0.0267\n",
      "     26    40       0.0102       0.0101     7.72e-05       0.0707       0.0908        0.507      0.00792\n",
      "     26    50      0.00819       0.0081     8.83e-05       0.0653       0.0812        0.542      0.00847\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     26     2      0.00854      0.00853     8.29e-06       0.0658       0.0833        0.132      0.00206\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              26   38.469    0.002      0.00946     0.000225      0.00969       0.0688       0.0877        0.667       0.0104\n",
      "! Validation         26   38.469    0.002      0.00944     8.27e-06      0.00945        0.069       0.0876        0.143      0.00224\n",
      "Wall time: 38.46901854299995\n",
      "! Best model       26    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     27    10       0.0106       0.0105     3.29e-05       0.0757       0.0925        0.331      0.00517\n",
      "     27    20       0.0108       0.0105      0.00021       0.0727       0.0926        0.836       0.0131\n",
      "     27    30      0.00902      0.00897     4.95e-05       0.0695       0.0854        0.406      0.00635\n",
      "     27    40      0.00903      0.00864     0.000387        0.067       0.0838         1.14       0.0177\n",
      "     27    50       0.0079       0.0079     3.73e-07       0.0645       0.0801       0.0352     0.000549\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     27     2      0.00844      0.00843     7.72e-06       0.0654       0.0828        0.127      0.00198\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              27   39.591    0.002      0.00937     0.000147      0.00952       0.0686       0.0873        0.595      0.00929\n",
      "! Validation         27   39.591    0.002       0.0093     8.33e-06      0.00931       0.0685       0.0869        0.143      0.00224\n",
      "Wall time: 39.59116603199982\n",
      "! Best model       27    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     28    10       0.0145       0.0144      8.1e-05       0.0789        0.108         0.52      0.00812\n",
      "     28    20      0.00813      0.00813     8.67e-06       0.0649       0.0813         0.17      0.00266\n",
      "     28    30      0.00705      0.00703     1.79e-05       0.0593       0.0756        0.244      0.00381\n",
      "     28    40      0.00955      0.00914     0.000409       0.0667       0.0862         1.17       0.0182\n",
      "     28    50      0.00049     1.33e-12      0.00049     8.53e-07     1.04e-06         1.28       0.0199\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     28     2      0.00834      0.00833     7.31e-06       0.0651       0.0823        0.129      0.00202\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              28   40.719    0.002      0.00924     0.000131      0.00937        0.068       0.0867        0.512      0.00799\n",
      "! Validation         28   40.719    0.002      0.00916     8.94e-06      0.00917       0.0681       0.0863        0.148      0.00231\n",
      "Wall time: 40.719738426999356\n",
      "! Best model       28    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     29    10      0.00969      0.00886     0.000827       0.0696       0.0849         1.66       0.0259\n",
      "     29    20       0.0087      0.00842     0.000278       0.0653       0.0827        0.962        0.015\n",
      "     29    30       0.0102       0.0101      8.1e-05       0.0709       0.0908         0.52      0.00812\n",
      "     29    40       0.0114       0.0112     0.000176       0.0738       0.0954        0.766        0.012\n",
      "     29    50       0.0063      0.00627     2.87e-05       0.0569       0.0714         0.31      0.00484\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     29     2      0.00825      0.00824     7.21e-06       0.0647       0.0819        0.129      0.00201\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              29   42.078    0.002      0.00911     0.000257      0.00937       0.0675       0.0861        0.798       0.0125\n",
      "! Validation         29   42.078    0.002      0.00903     9.21e-06      0.00903       0.0677       0.0857        0.149      0.00233\n",
      "Wall time: 42.078675632999875\n",
      "! Best model       29    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     30    10      0.00972      0.00966     5.64e-05       0.0718       0.0886        0.434      0.00677\n",
      "     30    20      0.00958      0.00957     6.34e-06        0.071       0.0882        0.146      0.00227\n",
      "     30    30      0.00602      0.00601     4.83e-06       0.0578       0.0699        0.127      0.00198\n",
      "     30    40      0.00867      0.00867     1.17e-06       0.0686        0.084       0.0625     0.000977\n",
      "     30    50       0.0114       0.0114     7.05e-06       0.0764       0.0962        0.153       0.0024\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     30     2      0.00816      0.00816     7.44e-06       0.0644       0.0814        0.133      0.00207\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              30   43.171    0.002      0.00904     7.04e-05      0.00911       0.0673       0.0857        0.394      0.00615\n",
      "! Validation         30   43.171    0.002       0.0089     9.82e-06      0.00891       0.0673       0.0851        0.153      0.00239\n",
      "Wall time: 43.170898175999355\n",
      "! Best model       30    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     31    10      0.00777      0.00776     8.94e-06        0.064       0.0794        0.172      0.00269\n",
      "     31    20      0.00955       0.0095     5.06e-05       0.0703       0.0879         0.41      0.00641\n",
      "     31    30      0.00623      0.00623     1.57e-06       0.0562       0.0711       0.0723      0.00113\n",
      "     31    40      0.00978      0.00975     2.69e-05       0.0686       0.0891        0.299      0.00467\n",
      "     31    50      0.00887      0.00853     0.000341       0.0682       0.0833         1.07       0.0166\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     31     2      0.00806      0.00806     7.27e-06        0.064       0.0809        0.131      0.00205\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              31   44.457    0.002      0.00903     5.64e-05      0.00908       0.0672       0.0857         0.34      0.00531\n",
      "! Validation         31   44.457    0.002      0.00878     1.01e-05      0.00879       0.0668       0.0845        0.153      0.00239\n",
      "Wall time: 44.45890707599938\n",
      "! Best model       31    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     32    10      0.00795      0.00779     0.000164       0.0651       0.0796        0.737       0.0115\n",
      "     32    20       0.0081      0.00807     3.38e-05       0.0649        0.081        0.335      0.00523\n",
      "     32    30      0.00934      0.00924     0.000101       0.0695       0.0867        0.579      0.00905\n",
      "     32    40        0.011       0.0107     0.000271       0.0739       0.0932        0.949       0.0148\n",
      "     32    50      0.00928       0.0092     8.08e-05       0.0678       0.0865        0.519       0.0081\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     32     2      0.00799      0.00798     7.36e-06       0.0637       0.0806        0.133      0.00207\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              32   45.766    0.002      0.00886     0.000247      0.00911       0.0665       0.0849        0.751       0.0117\n",
      "! Validation         32   45.766    0.002      0.00868     1.03e-05      0.00869       0.0665        0.084        0.154      0.00241\n",
      "Wall time: 45.766078294999716\n",
      "! Best model       32    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     33    10      0.00989      0.00961     0.000282       0.0731       0.0884         0.97       0.0152\n",
      "     33    20       0.0115       0.0115     1.23e-05        0.076       0.0965        0.202      0.00316\n",
      "     33    30      0.00871      0.00868     2.84e-05       0.0682        0.084        0.308      0.00481\n",
      "     33    40       0.0104       0.0103      8.1e-05       0.0705       0.0914         0.52      0.00812\n",
      "     33    50      0.00837      0.00835     2.22e-05       0.0653       0.0824        0.272      0.00426\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     33     2       0.0079      0.00789      7.2e-06       0.0633       0.0801         0.13      0.00204\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              33   47.024    0.002      0.00875     0.000137      0.00889       0.0662       0.0844        0.565      0.00883\n",
      "! Validation         33   47.024    0.002      0.00857     1.02e-05      0.00858       0.0661       0.0835        0.154       0.0024\n",
      "Wall time: 47.02439839899944\n",
      "! Best model       33    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     34    10       0.0077      0.00769     7.63e-06       0.0647       0.0791        0.159      0.00249\n",
      "     34    20      0.00952      0.00898     0.000546       0.0668       0.0854         1.35       0.0211\n",
      "     34    30      0.00819      0.00778     0.000412       0.0632       0.0795         1.17       0.0183\n",
      "     34    40      0.00837      0.00824     0.000129       0.0642       0.0818        0.655       0.0102\n",
      "     34    50       0.0133       0.0132     8.89e-05       0.0757        0.104        0.545      0.00851\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     34     2      0.00783      0.00782     7.17e-06       0.0631       0.0798         0.13      0.00203\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              34   48.205    0.002      0.00865     0.000202      0.00885       0.0657       0.0839        0.646       0.0101\n",
      "! Validation         34   48.205    0.002      0.00847     1.02e-05      0.00848       0.0657        0.083        0.154       0.0024\n",
      "Wall time: 48.20537830899957\n",
      "! Best model       34    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     35    10      0.00834      0.00834      8.1e-07       0.0667       0.0823       0.0518     0.000809\n",
      "     35    20       0.0119       0.0119     2.82e-06       0.0789       0.0984       0.0967      0.00151\n",
      "     35    30      0.00631       0.0057     0.000607       0.0554       0.0681         1.42       0.0222\n",
      "     35    40       0.0101       0.0101     4.69e-05         0.07       0.0906        0.396      0.00618\n",
      "     35    50      0.00865      0.00859     6.66e-05       0.0676       0.0835        0.471      0.00735\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     35     2      0.00775      0.00774     7.05e-06       0.0627       0.0793        0.129      0.00201\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              35   50.044    0.002      0.00857     0.000147      0.00871       0.0654       0.0834        0.565      0.00883\n",
      "! Validation         35   50.044    0.002      0.00838     1.02e-05      0.00839       0.0653       0.0825        0.153       0.0024\n",
      "Wall time: 50.044303726999715\n",
      "! Best model       35    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     36    10      0.00731      0.00731     3.19e-07       0.0633       0.0771       0.0322     0.000504\n",
      "     36    20       0.0077       0.0077      3.3e-06       0.0625       0.0791        0.105      0.00165\n",
      "     36    30       0.0119       0.0118     4.59e-05       0.0786       0.0981        0.391       0.0061\n",
      "     36    40      0.00901        0.009     1.12e-05       0.0654       0.0855        0.193      0.00302\n",
      "     36    50      0.00843      0.00839     4.25e-05       0.0634       0.0826        0.376      0.00587\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     36     2      0.00768      0.00767      6.9e-06       0.0624        0.079        0.127      0.00198\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              36   51.300    0.002       0.0085     6.71e-05      0.00857        0.065       0.0831        0.391       0.0061\n",
      "! Validation         36   51.300    0.002      0.00829     9.92e-06       0.0083        0.065       0.0821        0.152      0.00238\n",
      "Wall time: 51.30056912299915\n",
      "! Best model       36    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     37    10       0.0116       0.0114     0.000146       0.0754       0.0964        0.697       0.0109\n",
      "     37    20      0.00764      0.00754     9.57e-05       0.0639       0.0783        0.564      0.00882\n",
      "     37    30      0.00823      0.00822     5.59e-06       0.0616       0.0818        0.136      0.00212\n",
      "     37    40       0.0104       0.0102      0.00023       0.0724       0.0911        0.875       0.0137\n",
      "     37    50      0.00795      0.00795     3.93e-06        0.063       0.0804        0.114      0.00179\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     37     2      0.00762      0.00761     6.67e-06       0.0621       0.0787        0.123      0.00193\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              37   52.503    0.002      0.00851     0.000124      0.00863       0.0652       0.0832        0.522      0.00815\n",
      "! Validation         37   52.503    0.002      0.00821     9.59e-06      0.00822       0.0646       0.0817         0.15      0.00234\n",
      "Wall time: 52.50339730399992\n",
      "! Best model       37    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     38    10      0.00562      0.00561     8.23e-06       0.0556       0.0676        0.165      0.00258\n",
      "     38    20       0.0124       0.0123     8.55e-05       0.0741          0.1        0.534      0.00835\n",
      "     38    30      0.00833      0.00833     9.24e-07       0.0661       0.0823       0.0557      0.00087\n",
      "     38    40      0.00587      0.00585     1.75e-05       0.0539        0.069        0.241      0.00377\n",
      "     38    50      0.00795      0.00787        8e-05       0.0622         0.08        0.516      0.00806\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     38     2      0.00755      0.00754     6.68e-06       0.0618       0.0783        0.122      0.00191\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              38   53.712    0.002      0.00833     0.000102      0.00843       0.0644       0.0823        0.488      0.00763\n",
      "! Validation         38   53.712    0.002      0.00813     9.61e-06      0.00814       0.0643       0.0813         0.15      0.00234\n",
      "Wall time: 53.712618965999354\n",
      "! Best model       38    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     39    10      0.00727      0.00704     0.000228       0.0611       0.0756         0.87       0.0136\n",
      "     39    20      0.00847      0.00847     3.93e-08       0.0634        0.083       0.0117     0.000183\n",
      "     39    30      0.00738      0.00738     5.31e-06       0.0634       0.0775        0.133      0.00208\n",
      "     39    40      0.00924      0.00891     0.000325       0.0677       0.0851         1.04       0.0163\n",
      "     39    50      0.00941       0.0094     8.49e-06       0.0679       0.0874        0.168      0.00262\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     39     2      0.00749      0.00749     6.56e-06       0.0616        0.078        0.119      0.00185\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              39   55.415    0.002      0.00832     0.000115      0.00843       0.0643       0.0822        0.512        0.008\n",
      "! Validation         39   55.415    0.002      0.00806     9.25e-06      0.00807        0.064        0.081        0.147       0.0023\n",
      "Wall time: 55.415627160999975\n",
      "! Best model       39    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     40    10      0.00889      0.00886     2.43e-05       0.0672       0.0849        0.284      0.00444\n",
      "     40    20       0.0099      0.00975     0.000146       0.0685        0.089        0.698       0.0109\n",
      "     40    30       0.0108       0.0107     0.000126       0.0737       0.0932        0.647       0.0101\n",
      "     40    40      0.00736      0.00736     4.11e-07        0.062       0.0774       0.0371      0.00058\n",
      "     40    50       0.0073       0.0069     0.000405        0.061       0.0749         1.16       0.0181\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     40     2      0.00743      0.00742     6.49e-06       0.0613       0.0777        0.119      0.00186\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              40   57.120    0.002      0.00819     0.000122      0.00831       0.0639       0.0816         0.54      0.00843\n",
      "! Validation         40   57.120    0.002      0.00799     9.43e-06        0.008       0.0638       0.0806        0.148      0.00232\n",
      "Wall time: 57.12066553299974\n",
      "! Best model       40    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     41    10       0.0111       0.0111     1.27e-06       0.0739        0.095       0.0654      0.00102\n",
      "     41    20      0.00826      0.00811     0.000152       0.0631       0.0812        0.711       0.0111\n",
      "     41    30      0.00576      0.00523     0.000525       0.0512       0.0652         1.32       0.0207\n",
      "     41    40      0.00743      0.00741     1.65e-05       0.0631       0.0776        0.234      0.00366\n",
      "     41    50      0.00837      0.00828     8.52e-05       0.0641       0.0821        0.532      0.00832\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     41     2      0.00739      0.00738     6.42e-06       0.0611       0.0775        0.115       0.0018\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              41   58.227    0.002      0.00809     0.000149      0.00824       0.0635       0.0811        0.594      0.00928\n",
      "! Validation         41   58.227    0.002      0.00793     8.99e-06      0.00794       0.0635       0.0803        0.145      0.00226\n",
      "Wall time: 58.227391178999824\n",
      "! Best model       41    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     42    10      0.00697      0.00682     0.000152       0.0606       0.0745        0.711       0.0111\n",
      "     42    20       0.0075      0.00748     2.01e-05       0.0629        0.078        0.259      0.00404\n",
      "     42    30      0.00581      0.00574     7.22e-05       0.0555       0.0683         0.49      0.00766\n",
      "     42    40      0.00798      0.00782     0.000164       0.0648       0.0797        0.738       0.0115\n",
      "     42    50       0.0107       0.0107     1.81e-05       0.0728       0.0934        0.246      0.00385\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     42     2      0.00734      0.00733     6.43e-06       0.0609       0.0772        0.115      0.00179\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              42   59.520    0.002      0.00804     8.09e-05      0.00812       0.0633       0.0809        0.436      0.00682\n",
      "! Validation         42   59.520    0.002      0.00788     8.94e-06      0.00789       0.0632         0.08        0.145      0.00226\n",
      "Wall time: 59.52001498699974\n",
      "! Best model       42    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     43    10       0.0118       0.0117     2.08e-05       0.0722       0.0977        0.264      0.00412\n",
      "     43    20      0.00558      0.00523     0.000355       0.0504       0.0652         1.09        0.017\n",
      "     43    30      0.00815      0.00811     3.65e-05       0.0626       0.0812        0.349      0.00545\n",
      "     43    40      0.00917      0.00915     1.88e-05       0.0682       0.0862         0.25      0.00391\n",
      "     43    50      0.00784      0.00781     3.61e-05       0.0652       0.0797        0.347      0.00542\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     43     2      0.00727      0.00726     6.46e-06       0.0605       0.0768        0.115       0.0018\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              43   61.180    0.002        0.008     0.000112      0.00811        0.063       0.0806        0.488      0.00762\n",
      "! Validation         43   61.180    0.002      0.00781     8.72e-06      0.00782       0.0629       0.0797        0.144      0.00224\n",
      "Wall time: 61.180188697999256\n",
      "! Best model       43    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     44    10      0.00646      0.00639     7.43e-05        0.058       0.0721        0.497      0.00777\n",
      "     44    20      0.00852      0.00841     0.000116       0.0671       0.0827        0.621       0.0097\n",
      "     44    30      0.00551      0.00544     7.43e-05        0.053       0.0665        0.497      0.00777\n",
      "     44    40      0.00693       0.0069     3.51e-05       0.0581       0.0749        0.342      0.00534\n",
      "     44    50      0.00834      0.00832     2.14e-05       0.0666       0.0822        0.267      0.00417\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     44     2      0.00724      0.00723     6.38e-06       0.0603       0.0767        0.115      0.00179\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              44   62.349    0.002        0.008      8.4e-05      0.00808       0.0631       0.0806        0.432      0.00675\n",
      "! Validation         44   62.349    0.002      0.00777     8.76e-06      0.00778       0.0627       0.0795        0.144      0.00225\n",
      "Wall time: 62.34916216599959\n",
      "! Best model       44    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     45    10      0.00869      0.00865     3.24e-05       0.0672       0.0839        0.329      0.00514\n",
      "     45    20       0.0076      0.00755     4.49e-05       0.0622       0.0784        0.387      0.00604\n",
      "     45    30      0.00548      0.00547     4.06e-06       0.0517       0.0667        0.116      0.00182\n",
      "     45    40      0.00818      0.00812     6.01e-05       0.0662       0.0813        0.447      0.00699\n",
      "     45    50       0.0089      0.00856     0.000339       0.0631       0.0834         1.06       0.0166\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     45     2      0.00719      0.00718     6.37e-06       0.0601       0.0764        0.113      0.00177\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              45   63.477    0.002      0.00795     7.93e-05      0.00803        0.063       0.0804        0.437      0.00682\n",
      "! Validation         45   63.477    0.002      0.00771      8.8e-06      0.00772       0.0625       0.0792        0.144      0.00224\n",
      "Wall time: 63.476895508999405\n",
      "! Best model       45    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     46    10      0.00747       0.0066     0.000864       0.0597       0.0733          1.7       0.0265\n",
      "     46    20      0.00791      0.00761     0.000297       0.0636       0.0787        0.994       0.0155\n",
      "     46    30      0.00571      0.00567     3.74e-05       0.0521       0.0679        0.353      0.00551\n",
      "     46    40      0.00808      0.00791     0.000172        0.064       0.0802        0.757       0.0118\n",
      "     46    50      0.00764      0.00627      0.00138       0.0573       0.0714         2.14       0.0334\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     46     2      0.00714      0.00713      6.6e-06       0.0599       0.0761        0.116      0.00181\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              46   64.642    0.002      0.00785     0.000237      0.00809       0.0626       0.0799         0.71       0.0111\n",
      "! Validation         46   64.642    0.002      0.00766     8.61e-06      0.00767       0.0622       0.0789        0.143      0.00224\n",
      "Wall time: 64.64215036099995\n",
      "! Best model       46    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     47    10       0.0114       0.0113      0.00012       0.0712       0.0959        0.632      0.00987\n",
      "     47    20      0.00783      0.00778     5.06e-05       0.0618       0.0795         0.41      0.00641\n",
      "     47    30      0.00657      0.00651     5.64e-05       0.0577       0.0728        0.433      0.00676\n",
      "     47    40      0.00747      0.00736     0.000109        0.061       0.0773        0.604      0.00943\n",
      "     47    50      0.00627      0.00624     3.52e-05       0.0536       0.0712        0.343      0.00536\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     47     2      0.00709      0.00709     6.37e-06       0.0597       0.0759        0.114      0.00179\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              47   65.773    0.002      0.00802     0.000273      0.00829       0.0631       0.0807        0.749       0.0117\n",
      "! Validation         47   65.773    0.002      0.00761     8.58e-06      0.00762        0.062       0.0787        0.143      0.00223\n",
      "Wall time: 65.77364593799939\n",
      "! Best model       47    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     48    10      0.00829      0.00817     0.000111       0.0641       0.0815        0.609      0.00952\n",
      "     48    20       0.0112       0.0112     9.22e-06       0.0708       0.0954        0.175      0.00273\n",
      "     48    30      0.00873      0.00872     1.08e-05       0.0664       0.0842        0.189      0.00296\n",
      "     48    40      0.00786      0.00781     5.82e-05       0.0638       0.0797         0.44      0.00688\n",
      "     48    50      0.00647       0.0064     6.92e-05        0.058       0.0722         0.48      0.00751\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     48     2      0.00705      0.00704     6.73e-06       0.0595       0.0756        0.116      0.00181\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              48   66.931    0.002      0.00785     0.000102      0.00795       0.0625       0.0799        0.489      0.00764\n",
      "! Validation         48   66.931    0.002      0.00756     8.55e-06      0.00757       0.0618       0.0784        0.142      0.00222\n",
      "Wall time: 66.93102571599957\n",
      "! Best model       48    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     49    10        0.008      0.00781     0.000189       0.0626       0.0797        0.793       0.0124\n",
      "     49    20       0.0061      0.00577     0.000337       0.0544       0.0685         1.06       0.0165\n",
      "     49    30      0.00873      0.00864     9.07e-05       0.0665       0.0838         0.55      0.00859\n",
      "     49    40      0.00786      0.00773     0.000127       0.0634       0.0793        0.652       0.0102\n",
      "     49    50      0.00919      0.00913     6.52e-05       0.0674       0.0861        0.466      0.00728\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     49     2      0.00699      0.00699     6.74e-06       0.0592       0.0754        0.117      0.00183\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              49   68.050    0.002      0.00778     0.000191      0.00797       0.0622       0.0795        0.687       0.0107\n",
      "! Validation         49   68.050    0.002       0.0075     8.47e-06      0.00751       0.0616       0.0781        0.142      0.00222\n",
      "Wall time: 68.05069922299936\n",
      "! Best model       49    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     50    10      0.00662      0.00657     4.63e-05       0.0576       0.0731        0.393      0.00613\n",
      "     50    20      0.00548      0.00522     0.000267       0.0515       0.0651        0.942       0.0147\n",
      "     50    30      0.00911      0.00911     1.76e-06       0.0675       0.0861       0.0762      0.00119\n",
      "     50    40      0.00796      0.00748     0.000479       0.0617        0.078         1.26       0.0197\n",
      "     50    50      0.00724      0.00723     4.37e-06       0.0608       0.0767        0.121      0.00189\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     50     2      0.00694      0.00693     6.73e-06        0.059       0.0751        0.117      0.00182\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              50   69.748    0.002      0.00768     0.000238      0.00791       0.0618        0.079        0.759       0.0119\n",
      "! Validation         50   69.748    0.002      0.00744     8.38e-06      0.00744       0.0613       0.0777        0.141      0.00221\n",
      "Wall time: 69.74891563399979\n",
      "! Best model       50    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     51    10      0.00882      0.00873     9.09e-05       0.0654       0.0842         0.55      0.00859\n",
      "     51    20      0.00532      0.00526     5.98e-05        0.051       0.0654        0.447      0.00699\n",
      "     51    30      0.00747      0.00719     0.000277       0.0601       0.0765         0.96        0.015\n",
      "     51    40      0.00681       0.0068     1.08e-05       0.0589       0.0744        0.189      0.00296\n",
      "     51    50      0.00665      0.00659     6.01e-05       0.0589       0.0732        0.447      0.00699\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     51     2      0.00688      0.00687     6.82e-06       0.0587       0.0748        0.117      0.00182\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              51   71.400    0.002      0.00762     8.87e-05      0.00771       0.0615       0.0787        0.463      0.00723\n",
      "! Validation         51   71.400    0.002      0.00738     8.31e-06      0.00739       0.0611       0.0774        0.141       0.0022\n",
      "Wall time: 71.40067893299965\n",
      "! Best model       51    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     52    10       0.0108       0.0105     0.000357       0.0691       0.0924         1.09        0.017\n",
      "     52    20      0.00519      0.00519      2.1e-07       0.0508        0.065       0.0264     0.000412\n",
      "     52    30      0.00623      0.00612     0.000102       0.0564       0.0706        0.582      0.00909\n",
      "     52    40      0.00796      0.00792     3.74e-05       0.0646       0.0803        0.354      0.00552\n",
      "     52    50      0.00849      0.00848     7.38e-06       0.0672        0.083        0.156      0.00244\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     52     2      0.00683      0.00682     7.01e-06       0.0585       0.0745        0.117      0.00183\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              52   73.147    0.002      0.00751     0.000165      0.00768       0.0611       0.0781        0.599      0.00936\n",
      "! Validation         52   73.147    0.002      0.00732     8.24e-06      0.00733       0.0609       0.0772        0.139      0.00218\n",
      "Wall time: 73.14820718699957\n",
      "! Best model       52    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     53    10      0.00743      0.00742     1.17e-05        0.063       0.0776        0.197      0.00308\n",
      "     53    20      0.00855      0.00851     3.19e-05       0.0649       0.0832        0.326       0.0051\n",
      "     53    30       0.0072      0.00715     4.17e-05       0.0611       0.0763        0.373      0.00583\n",
      "     53    40       0.0088      0.00873     6.61e-05       0.0672       0.0842         0.47      0.00734\n",
      "     53    50       0.0066      0.00659     2.57e-06       0.0593       0.0732       0.0918      0.00143\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     53     2      0.00677      0.00676     7.01e-06       0.0583       0.0741        0.118      0.00184\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              53   75.053    0.002      0.00752     8.35e-05       0.0076       0.0612       0.0782        0.422       0.0066\n",
      "! Validation         53   75.053    0.002      0.00726     8.28e-06      0.00727       0.0606       0.0768         0.14      0.00218\n",
      "Wall time: 75.05320371899961\n",
      "! Best model       53    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     54    10      0.00751      0.00745     6.15e-05       0.0597       0.0778        0.452      0.00706\n",
      "     54    20      0.00699      0.00694     4.84e-05       0.0584       0.0751        0.401      0.00627\n",
      "     54    30      0.00654      0.00633     0.000202       0.0586       0.0718        0.821       0.0128\n",
      "     54    40      0.00711      0.00709     1.57e-05       0.0612       0.0759        0.229      0.00359\n",
      "     54    50      0.00682       0.0068     1.77e-05       0.0588       0.0744        0.243       0.0038\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     54     2      0.00672      0.00671     6.87e-06       0.0581       0.0739        0.116      0.00182\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              54   77.084    0.002      0.00743     0.000154      0.00758       0.0608       0.0777        0.604      0.00944\n",
      "! Validation         54   77.084    0.002      0.00721      8.2e-06      0.00721       0.0604       0.0765        0.139      0.00217\n",
      "Wall time: 77.08413192399985\n",
      "! Best model       54    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     55    10       0.0072       0.0072     3.73e-07       0.0605       0.0765       0.0352     0.000549\n",
      "     55    20      0.00812      0.00771     0.000409       0.0628       0.0792         1.17       0.0182\n",
      "     55    30      0.00499      0.00496     3.22e-05       0.0511       0.0635        0.327      0.00511\n",
      "     55    40      0.00746      0.00738     8.52e-05       0.0605       0.0774        0.533      0.00833\n",
      "     55    50         0.01      0.00993     8.64e-05       0.0675       0.0899        0.536      0.00838\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     55     2      0.00666      0.00666     6.62e-06       0.0578       0.0736        0.115       0.0018\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              55   78.509    0.002      0.00742      6.2e-05      0.00748       0.0607       0.0777        0.365       0.0057\n",
      "! Validation         55   78.509    0.002      0.00715      8.1e-06      0.00715       0.0601       0.0762        0.138      0.00216\n",
      "Wall time: 78.50983859699954\n",
      "! Best model       55    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     56    10        0.011        0.011     8.85e-06       0.0757       0.0945        0.172      0.00269\n",
      "     56    20      0.00784      0.00782     2.57e-05       0.0652       0.0797        0.292      0.00456\n",
      "     56    30      0.00726      0.00718     7.97e-05       0.0613       0.0764        0.515      0.00804\n",
      "     56    40      0.00834      0.00832     1.88e-05       0.0665       0.0822         0.25      0.00391\n",
      "     56    50      0.00618      0.00588     0.000298       0.0561       0.0692        0.996       0.0156\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     56     2      0.00662      0.00662     6.56e-06       0.0576       0.0733        0.115       0.0018\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              56   79.938    0.002      0.00729     7.82e-05      0.00736       0.0603        0.077        0.411      0.00642\n",
      "! Validation         56   79.938    0.002      0.00709     8.05e-06       0.0071       0.0599       0.0759        0.138      0.00216\n",
      "Wall time: 79.93869522999921\n",
      "! Best model       56    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     57    10      0.00612      0.00604     8.47e-05       0.0568       0.0701        0.531       0.0083\n",
      "     57    20      0.00746      0.00728     0.000173       0.0621       0.0769        0.759       0.0119\n",
      "     57    30      0.00761      0.00759     2.28e-05        0.062       0.0785        0.275       0.0043\n",
      "     57    40      0.00828       0.0081     0.000181       0.0633       0.0812        0.775       0.0121\n",
      "     57    50      0.00509      0.00509     5.96e-08        0.052       0.0643       0.0137     0.000214\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     57     2      0.00657      0.00657     6.64e-06       0.0574       0.0731        0.115      0.00179\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              57   81.293    0.002      0.00725     0.000129      0.00738       0.0601       0.0768        0.556      0.00869\n",
      "! Validation         57   81.293    0.002      0.00705     7.95e-06      0.00706       0.0597       0.0757        0.136      0.00213\n",
      "Wall time: 81.29292829599945\n",
      "! Best model       57    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     58    10      0.00526      0.00476     0.000503       0.0501       0.0622         1.29       0.0202\n",
      "     58    20      0.00756      0.00753     3.07e-05       0.0597       0.0782        0.319      0.00499\n",
      "     58    30      0.00679      0.00676     2.82e-05       0.0601       0.0741        0.307      0.00479\n",
      "     58    40      0.00833      0.00818     0.000157       0.0655       0.0815        0.724       0.0113\n",
      "     58    50      0.00922      0.00868     0.000542       0.0687        0.084         1.34        0.021\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     58     2      0.00653      0.00652     6.35e-06       0.0572       0.0728        0.113      0.00177\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              58   82.568    0.002      0.00725     9.59e-05      0.00735       0.0601       0.0768        0.445      0.00695\n",
      "! Validation         58   82.568    0.002        0.007     7.93e-06        0.007       0.0595       0.0754        0.137      0.00213\n",
      "Wall time: 82.568144201\n",
      "! Best model       58    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     59    10      0.00724      0.00685      0.00039       0.0612       0.0746         1.14       0.0178\n",
      "     59    20      0.00934      0.00916     0.000177        0.068       0.0863        0.767        0.012\n",
      "     59    30      0.00951      0.00945     5.17e-05       0.0661       0.0877        0.415      0.00648\n",
      "     59    40      0.00533      0.00531     2.55e-05       0.0497       0.0657        0.292      0.00456\n",
      "     59    50      0.00857      0.00856     1.42e-05       0.0663       0.0834        0.218       0.0034\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     59     2      0.00648      0.00648     6.66e-06        0.057       0.0726        0.113      0.00177\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              59   84.482    0.002      0.00713      0.00015      0.00728       0.0595       0.0761        0.558      0.00872\n",
      "! Validation         59   84.482    0.002      0.00695     7.81e-06      0.00695       0.0593       0.0751        0.134       0.0021\n",
      "Wall time: 84.4824912869999\n",
      "! Best model       59    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     60    10      0.00808      0.00807     1.59e-05       0.0654        0.081        0.229      0.00359\n",
      "     60    20      0.00714      0.00711     3.26e-05       0.0623        0.076        0.329      0.00514\n",
      "     60    30      0.00474      0.00474     8.41e-08       0.0488       0.0621       0.0166     0.000259\n",
      "     60    40      0.00671      0.00665     6.37e-05       0.0552       0.0735         0.46      0.00719\n",
      "     60    50      0.00629      0.00627     1.62e-05       0.0576       0.0714        0.232      0.00363\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     60     2      0.00644      0.00644     6.33e-06       0.0568       0.0723        0.111      0.00174\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              60   85.864    0.002      0.00711     0.000104      0.00722       0.0595        0.076         0.46      0.00719\n",
      "! Validation         60   85.864    0.002       0.0069     7.74e-06      0.00691       0.0591       0.0749        0.134      0.00209\n",
      "Wall time: 85.86420218799958\n",
      "! Best model       60    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     61    10      0.00464       0.0046     4.74e-05       0.0492       0.0611        0.397      0.00621\n",
      "     61    20      0.00729      0.00718     0.000111       0.0613       0.0764        0.609      0.00952\n",
      "     61    30       0.0076      0.00759     1.23e-05       0.0624       0.0785        0.202      0.00316\n",
      "     61    40      0.00588      0.00588     2.67e-06       0.0547       0.0691       0.0947      0.00148\n",
      "     61    50      0.00804        0.008     3.99e-05       0.0636       0.0806        0.364      0.00569\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     61     2       0.0064      0.00639     6.06e-06       0.0566       0.0721         0.11      0.00171\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              61   87.153    0.002      0.00715     0.000254       0.0074       0.0596       0.0762        0.768        0.012\n",
      "! Validation         61   87.153    0.002      0.00686     7.58e-06      0.00687       0.0589       0.0747        0.133      0.00208\n",
      "Wall time: 87.15313894199971\n",
      "! Best model       61    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     62    10      0.00675      0.00667     7.43e-05       0.0573       0.0737        0.498      0.00778\n",
      "     62    20      0.00967      0.00951     0.000163       0.0665       0.0879        0.735       0.0115\n",
      "     62    30      0.00767      0.00735     0.000321       0.0628       0.0773         1.03       0.0162\n",
      "     62    40      0.00628      0.00574     0.000539        0.055       0.0683         1.34        0.021\n",
      "     62    50      0.00717      0.00703     0.000136       0.0612       0.0756        0.672       0.0105\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     62     2      0.00636      0.00636     6.24e-06       0.0565       0.0719        0.111      0.00173\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              62   88.378    0.002      0.00705     0.000146       0.0072       0.0591       0.0757        0.595       0.0093\n",
      "! Validation         62   88.378    0.002      0.00682     7.57e-06      0.00682       0.0587       0.0744        0.132      0.00207\n",
      "Wall time: 88.3788098949999\n",
      "! Best model       62    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     63    10      0.00769      0.00758      0.00011       0.0617       0.0785        0.605      0.00946\n",
      "     63    20      0.00776      0.00765     0.000113       0.0642       0.0789        0.614       0.0096\n",
      "     63    30      0.00666      0.00666     3.35e-08       0.0603       0.0736      0.00977     0.000153\n",
      "     63    40      0.00513      0.00473       0.0004       0.0496        0.062         1.15        0.018\n",
      "     63    50      0.00973      0.00973     1.49e-08       0.0686        0.089      0.00684     0.000107\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     63     2      0.00633      0.00633     5.95e-06       0.0563       0.0717        0.108      0.00169\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              63   89.730    0.002      0.00708     0.000113       0.0072       0.0593       0.0759          0.5      0.00781\n",
      "! Validation         63   89.730    0.002      0.00678     7.51e-06      0.00679       0.0586       0.0742        0.131      0.00205\n",
      "Wall time: 89.73046930500004\n",
      "! Best model       63    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     64    10      0.00839      0.00838     1.05e-05       0.0654       0.0825        0.187      0.00291\n",
      "     64    20     5.43e-05     1.37e-12     5.43e-05     8.65e-07     1.05e-06        0.425      0.00664\n",
      "     64    30       0.0071      0.00648     0.000618       0.0581       0.0726         1.43       0.0224\n",
      "     64    40      0.00824      0.00824     1.42e-06       0.0636       0.0818       0.0693      0.00108\n",
      "     64    50      0.00704      0.00704     8.66e-07        0.058       0.0756       0.0537     0.000839\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     64     2       0.0063      0.00629     5.92e-06       0.0562       0.0715        0.108      0.00169\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              64   91.448    0.002      0.00704     6.07e-05       0.0071       0.0591       0.0757        0.342      0.00534\n",
      "! Validation         64   91.448    0.002      0.00674      7.5e-06      0.00675       0.0584        0.074        0.132      0.00206\n",
      "Wall time: 91.44840429899978\n",
      "! Best model       64    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     65    10      0.00476      0.00465     0.000112       0.0489       0.0615         0.61      0.00954\n",
      "     65    20      0.00757       0.0075     7.33e-05       0.0624       0.0781        0.494      0.00772\n",
      "     65    30      0.00877      0.00877     2.33e-06       0.0659       0.0844       0.0879      0.00137\n",
      "     65    40       0.0092       0.0092     4.83e-06       0.0665       0.0865        0.127      0.00198\n",
      "     65    50      0.00658      0.00658     8.95e-07       0.0577       0.0732       0.0547     0.000854\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     65     2      0.00627      0.00626     5.99e-06        0.056       0.0713        0.108      0.00169\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              65   93.516    0.002      0.00705      3.5e-05      0.00709       0.0591       0.0757        0.275       0.0043\n",
      "! Validation         65   93.516    0.002       0.0067     7.41e-06      0.00671       0.0582       0.0738         0.13      0.00204\n",
      "Wall time: 93.51666130399917\n",
      "! Best model       65    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     66    10       0.0089       0.0088     0.000106       0.0672       0.0846        0.594      0.00928\n",
      "     66    20      0.00772      0.00726     0.000457       0.0626       0.0768         1.23       0.0193\n",
      "     66    30      0.00485      0.00467      0.00018         0.05       0.0616        0.773       0.0121\n",
      "     66    40      0.00706      0.00702     4.15e-05       0.0601       0.0755        0.372      0.00581\n",
      "     66    50      0.00674      0.00674     4.56e-08         0.06        0.074       0.0117     0.000183\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     66     2      0.00624      0.00623     5.92e-06       0.0559       0.0712        0.108      0.00168\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              66   95.116    0.002      0.00715     0.000143      0.00729       0.0596       0.0762        0.541      0.00845\n",
      "! Validation         66   95.116    0.002      0.00667     7.32e-06      0.00668       0.0581       0.0736        0.129      0.00202\n",
      "Wall time: 95.11639980699965\n",
      "! Best model       66    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     67    10      0.00463      0.00462     1.22e-05       0.0493       0.0613        0.201      0.00314\n",
      "     67    20      0.00854      0.00849     5.91e-05       0.0656       0.0831        0.443      0.00693\n",
      "     67    30      0.00603      0.00599     4.03e-05        0.056       0.0698        0.366      0.00572\n",
      "     67    40      0.00513      0.00506     6.76e-05       0.0509       0.0642        0.475      0.00742\n",
      "     67    50      0.00603      0.00601     2.07e-05       0.0567       0.0699        0.263       0.0041\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     67     2      0.00622      0.00621     5.77e-06       0.0558       0.0711        0.107      0.00167\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              67   97.135    0.002      0.00685     6.26e-05      0.00691       0.0582       0.0746        0.372      0.00581\n",
      "! Validation         67   97.135    0.002      0.00664     7.24e-06      0.00665        0.058       0.0735        0.129      0.00201\n",
      "Wall time: 97.1360307469995\n",
      "! Best model       67    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     68    10      0.00651      0.00637     0.000133       0.0583        0.072        0.666       0.0104\n",
      "     68    20       0.0066      0.00648      0.00012       0.0567       0.0726        0.634       0.0099\n",
      "     68    30      0.00982      0.00952     0.000305       0.0687        0.088         1.01       0.0157\n",
      "     68    40      0.00972      0.00972     4.06e-06        0.069       0.0889        0.116      0.00182\n",
      "     68    50      0.00585      0.00581     4.74e-05       0.0555       0.0687        0.396       0.0062\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     68     2      0.00618      0.00617     5.72e-06       0.0557       0.0708        0.106      0.00165\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              68   98.865    0.002      0.00685     7.54e-05      0.00693       0.0583       0.0746        0.406      0.00634\n",
      "! Validation         68   98.865    0.002       0.0066     7.19e-06      0.00661       0.0578       0.0733        0.127      0.00199\n",
      "Wall time: 98.8650922199995\n",
      "! Best model       68    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     69    10      0.00607      0.00603     3.61e-05       0.0568         0.07        0.348      0.00543\n",
      "     69    20      0.00659      0.00657     1.77e-05       0.0563       0.0731        0.243       0.0038\n",
      "     69    30       0.0059      0.00586     4.27e-05       0.0544        0.069        0.377      0.00589\n",
      "     69    40      0.00828      0.00792     0.000356        0.062       0.0802         1.09        0.017\n",
      "     69    50     0.000201     1.29e-12     0.000201     8.58e-07     1.03e-06        0.818       0.0128\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     69     2      0.00616      0.00615     5.39e-06       0.0556       0.0707        0.104      0.00163\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              69  100.906    0.002      0.00698     0.000189      0.00717       0.0587       0.0753        0.594      0.00927\n",
      "! Validation         69  100.906    0.002      0.00657     7.16e-06      0.00658       0.0577       0.0731        0.128        0.002\n",
      "Wall time: 100.90611342800003\n",
      "! Best model       69    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     70    10      0.00695      0.00675     0.000201       0.0582       0.0741        0.817       0.0128\n",
      "     70    20      0.00902      0.00885     0.000166       0.0654       0.0848        0.744       0.0116\n",
      "     70    30      0.00652       0.0064     0.000118       0.0592       0.0722        0.628      0.00981\n",
      "     70    40      0.00637      0.00635     2.46e-05       0.0581       0.0718        0.286      0.00447\n",
      "     70    50       0.0111       0.0111     4.12e-06       0.0771        0.095        0.117      0.00183\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     70     2      0.00614      0.00614     5.79e-06       0.0555       0.0706        0.106      0.00166\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              70  102.589    0.002      0.00694     0.000208      0.00715       0.0587       0.0751        0.621       0.0097\n",
      "! Validation         70  102.589    0.002      0.00654     7.16e-06      0.00655       0.0576       0.0729        0.127      0.00199\n",
      "Wall time: 102.58955849999984\n",
      "! Best model       70    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     71    10      0.00671       0.0067     3.73e-07       0.0605       0.0738       0.0352     0.000549\n",
      "     71    20      0.00967      0.00965     2.41e-05       0.0703       0.0886        0.284      0.00444\n",
      "     71    30      0.00728      0.00672     0.000558       0.0592       0.0739         1.36       0.0213\n",
      "     71    40      0.00639      0.00616      0.00023        0.056       0.0708        0.874       0.0137\n",
      "     71    50       0.0065      0.00599     0.000513       0.0552       0.0698         1.31       0.0204\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     71     2      0.00612      0.00611     5.52e-06       0.0554       0.0705        0.103      0.00161\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              71  103.894    0.002      0.00698     0.000186      0.00717       0.0589       0.0753        0.671       0.0105\n",
      "! Validation         71  103.894    0.002      0.00652     7.07e-06      0.00653       0.0575       0.0728        0.126      0.00196\n",
      "Wall time: 103.89435115299966\n",
      "! Best model       71    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     72    10      0.00567      0.00565     2.22e-05       0.0546       0.0678        0.271      0.00424\n",
      "     72    20      0.00997      0.00994     2.84e-05       0.0687       0.0899        0.308      0.00481\n",
      "     72    30       0.0103      0.00963     0.000642       0.0705       0.0885         1.46       0.0228\n",
      "     72    40      0.00676      0.00667     8.44e-05       0.0594       0.0737         0.53      0.00829\n",
      "     72    50      0.00489      0.00489     2.52e-06       0.0498        0.063       0.0918      0.00143\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     72     2      0.00609      0.00608     5.67e-06       0.0553       0.0703        0.105      0.00164\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              72  105.145    0.002      0.00684     9.52e-05      0.00693       0.0581       0.0745        0.446      0.00698\n",
      "! Validation         72  105.145    0.002      0.00649     7.04e-06       0.0065       0.0574       0.0726        0.125      0.00196\n",
      "Wall time: 105.14480448599988\n",
      "! Best model       72    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     73    10      0.00568      0.00562      5.1e-05       0.0533       0.0676        0.412      0.00644\n",
      "     73    20      0.00624      0.00622     1.46e-05        0.053       0.0711         0.22      0.00343\n",
      "     73    30      0.00679      0.00679     1.13e-07       0.0584       0.0743       0.0186      0.00029\n",
      "     73    40       0.0102      0.00987     0.000317       0.0691       0.0896         1.03       0.0161\n",
      "     73    50     4.25e-05     1.29e-12     4.25e-05     8.41e-07     1.03e-06        0.376      0.00587\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     73     2      0.00607      0.00606     5.42e-06       0.0552       0.0702        0.103      0.00161\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              73  106.397    0.002      0.00691     0.000145      0.00705       0.0585       0.0749        0.609      0.00952\n",
      "! Validation         73  106.397    0.002      0.00646     6.92e-06      0.00647       0.0573       0.0725        0.124      0.00194\n",
      "Wall time: 106.39691561999916\n",
      "! Best model       73    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     74    10      0.00509      0.00508     7.13e-06       0.0528       0.0643        0.153       0.0024\n",
      "     74    20      0.00622      0.00621      3.7e-06       0.0581       0.0711         0.11      0.00172\n",
      "     74    30      0.00539      0.00534     5.45e-05       0.0524       0.0659        0.427      0.00667\n",
      "     74    40       0.0112        0.011     0.000147       0.0769       0.0948          0.7       0.0109\n",
      "     74    50     0.000115     1.25e-12     0.000115     8.41e-07     1.01e-06         0.62      0.00969\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     74     2      0.00605      0.00604     5.42e-06       0.0551       0.0701        0.103      0.00161\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              74  107.660    0.002      0.00669     5.67e-05      0.00675       0.0575       0.0738        0.372      0.00581\n",
      "! Validation         74  107.660    0.002      0.00644     6.89e-06      0.00645       0.0571       0.0723        0.124      0.00194\n",
      "Wall time: 107.66001303199937\n",
      "! Best model       74    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     75    10      0.00556      0.00555     1.43e-05        0.053       0.0672        0.219      0.00342\n",
      "     75    20      0.00753      0.00741     0.000122       0.0626       0.0776        0.639      0.00998\n",
      "     75    30      0.00681      0.00657     0.000238       0.0564       0.0731        0.889       0.0139\n",
      "     75    40       0.0106         0.01     0.000567       0.0685       0.0903         1.37       0.0215\n",
      "     75    50      0.00737      0.00734     3.08e-05       0.0612       0.0772         0.32        0.005\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     75     2      0.00603      0.00602     5.72e-06        0.055         0.07        0.103      0.00161\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              75  108.869    0.002      0.00669     0.000328      0.00702       0.0576       0.0737        0.901       0.0141\n",
      "! Validation         75  108.869    0.002      0.00641     6.94e-06      0.00642        0.057       0.0722        0.123      0.00192\n",
      "Wall time: 108.86928820799949\n",
      "! Best model       75    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     76    10      0.00613      0.00612     9.04e-06       0.0578       0.0706        0.174      0.00272\n",
      "     76    20      0.00778      0.00777     2.97e-06       0.0641       0.0795       0.0996      0.00156\n",
      "     76    30      0.00645      0.00636     8.95e-05       0.0573       0.0719        0.546      0.00853\n",
      "     76    40       0.0068      0.00654     0.000261       0.0566       0.0729        0.932       0.0146\n",
      "     76    50      0.00574      0.00574      1.8e-06       0.0534       0.0683       0.0781      0.00122\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     76     2      0.00601        0.006     5.25e-06       0.0549       0.0699        0.101      0.00157\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              76  110.155    0.002      0.00674     0.000109      0.00685       0.0576        0.074        0.494      0.00771\n",
      "! Validation         76  110.155    0.002      0.00639     6.79e-06       0.0064       0.0569       0.0721        0.122      0.00191\n",
      "Wall time: 110.15563859699978\n",
      "! Best model       76    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     77    10      0.00757      0.00756     1.04e-05       0.0616       0.0784        0.187      0.00291\n",
      "     77    20      0.00655      0.00654     1.01e-05       0.0589       0.0729        0.184      0.00287\n",
      "     77    30      0.00808      0.00807     1.11e-05       0.0652        0.081        0.191      0.00299\n",
      "     77    40      0.00452      0.00423     0.000286       0.0469       0.0586        0.977       0.0153\n",
      "     77    50      0.00634      0.00601     0.000333       0.0556       0.0699         1.05       0.0164\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     77     2      0.00598      0.00598     5.66e-06       0.0548       0.0697        0.103       0.0016\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              77  111.441    0.002       0.0067     0.000163      0.00686       0.0576       0.0738        0.604      0.00944\n",
      "! Validation         77  111.441    0.002      0.00637     6.88e-06      0.00637       0.0568       0.0719        0.122      0.00191\n",
      "Wall time: 111.44152566699995\n",
      "! Best model       77    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     78    10      0.00705      0.00691     0.000134       0.0559        0.075         0.67       0.0105\n",
      "     78    20      0.00609      0.00588     0.000213       0.0555       0.0691        0.842       0.0132\n",
      "     78    30        0.005      0.00458     0.000426       0.0479        0.061         1.19       0.0186\n",
      "     78    40      0.00906      0.00904     1.51e-05       0.0686       0.0857        0.225      0.00351\n",
      "     78    50      0.00652       0.0055      0.00102       0.0534       0.0668         1.85       0.0289\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     78     2      0.00597      0.00597      5.3e-06       0.0547       0.0696        0.101      0.00157\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              78  112.681    0.002       0.0067     0.000393      0.00709       0.0575       0.0738         1.03       0.0161\n",
      "! Validation         78  112.681    0.002      0.00634     6.81e-06      0.00635       0.0567       0.0718        0.122      0.00191\n",
      "Wall time: 112.6811042469999\n",
      "! Best model       78    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     79    10      0.00589      0.00571      0.00018       0.0545       0.0682        0.775       0.0121\n",
      "     79    20      0.00634      0.00631     3.12e-05       0.0569       0.0716        0.322      0.00504\n",
      "     79    30      0.00842      0.00841     9.98e-06       0.0646       0.0827        0.183      0.00285\n",
      "     79    40      0.00718      0.00716     2.77e-05       0.0585       0.0763        0.304      0.00475\n",
      "     79    50      0.00644      0.00639      5.8e-05       0.0577        0.072        0.439      0.00687\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     79     2      0.00595      0.00594     5.18e-06       0.0546       0.0695        0.099      0.00155\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              79  114.516    0.002      0.00663     0.000142      0.00677       0.0573       0.0734        0.563       0.0088\n",
      "! Validation         79  114.516    0.002      0.00633     6.68e-06      0.00633       0.0567       0.0717        0.121      0.00188\n",
      "Wall time: 114.51609589599957\n",
      "! Best model       79    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     80    10      0.00856      0.00856     4.93e-07       0.0637       0.0834        0.041     0.000641\n",
      "     80    20      0.00549      0.00548     1.39e-05       0.0532       0.0667        0.215      0.00336\n",
      "     80    30       0.0068      0.00643     0.000364       0.0588       0.0723          1.1       0.0172\n",
      "     80    40       0.0106       0.0104      0.00016       0.0693       0.0919        0.729       0.0114\n",
      "     80    50      0.00643      0.00637     6.03e-05        0.059        0.072        0.448        0.007\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     80     2      0.00594      0.00593      5.2e-06       0.0546       0.0694       0.0988      0.00154\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              80  116.308    0.002      0.00665     0.000101      0.00675       0.0572       0.0735        0.489      0.00764\n",
      "! Validation         80  116.308    0.002      0.00631     6.71e-06      0.00632       0.0566       0.0716         0.12      0.00188\n",
      "Wall time: 116.30850699999974\n",
      "! Best model       80    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     81    10      0.00909      0.00904     4.41e-05       0.0667       0.0857        0.383      0.00598\n",
      "     81    20      0.00676      0.00667     9.12e-05       0.0592       0.0736        0.552      0.00862\n",
      "     81    30      0.00567      0.00567     4.31e-06       0.0543       0.0679        0.119      0.00186\n",
      "     81    40      0.00876      0.00876     9.24e-07       0.0661       0.0844       0.0547     0.000854\n",
      "     81    50      0.00572      0.00572     1.97e-06       0.0553       0.0682       0.0811      0.00127\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     81     2      0.00593      0.00592     5.04e-06       0.0545       0.0694       0.0986      0.00154\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              81  118.120    0.002      0.00684     5.86e-05       0.0069        0.058       0.0746        0.367      0.00573\n",
      "! Validation         81  118.120    0.002       0.0063     6.62e-06       0.0063       0.0565       0.0715         0.12      0.00188\n",
      "Wall time: 118.12041474599937\n",
      "! Best model       81    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     82    10      0.00745      0.00745     4.69e-06       0.0639       0.0778        0.125      0.00195\n",
      "     82    20      0.00831      0.00825     6.59e-05       0.0645       0.0819        0.469      0.00732\n",
      "     82    30       0.0051      0.00509     1.61e-05       0.0515       0.0643        0.231      0.00362\n",
      "     82    40      0.00748      0.00748     1.89e-06       0.0623        0.078       0.0791      0.00124\n",
      "     82    50      0.00625      0.00622     3.52e-05       0.0573       0.0711        0.342      0.00534\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     82     2      0.00592      0.00591     4.89e-06       0.0545       0.0693       0.0977      0.00153\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              82  119.364    0.002      0.00672     6.57e-05      0.00679       0.0576       0.0739        0.364      0.00568\n",
      "! Validation         82  119.364    0.002      0.00627     6.59e-06      0.00628       0.0564       0.0714         0.12      0.00188\n",
      "Wall time: 119.36425498599965\n",
      "! Best model       82    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     83    10      0.00651      0.00651     2.87e-06       0.0584       0.0728       0.0977      0.00153\n",
      "     83    20      0.00483      0.00483     9.04e-06         0.05       0.0626        0.173       0.0027\n",
      "     83    30      0.00762      0.00754      8.3e-05       0.0636       0.0783        0.525      0.00821\n",
      "     83    40      0.00912      0.00912     7.56e-07       0.0672       0.0861       0.0498     0.000778\n",
      "     83    50      0.00676      0.00668     7.49e-05       0.0594       0.0737          0.5      0.00781\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     83     2       0.0059       0.0059     4.81e-06       0.0544       0.0693       0.0967      0.00151\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              83  120.618    0.002      0.00666     7.16e-05      0.00674       0.0573       0.0736        0.378       0.0059\n",
      "! Validation         83  120.618    0.002      0.00626     6.56e-06      0.00626       0.0564       0.0713        0.119      0.00186\n",
      "Wall time: 120.61882621899986\n",
      "! Best model       83    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     84    10       0.0058      0.00574     6.44e-05       0.0552       0.0683        0.463      0.00723\n",
      "     84    20        0.007      0.00699     1.37e-05        0.059       0.0754        0.214      0.00334\n",
      "     84    30      0.00791       0.0075     0.000412       0.0627       0.0781         1.17       0.0183\n",
      "     84    40      0.00737      0.00609      0.00128       0.0567       0.0704         2.06       0.0323\n",
      "     84    50      0.00813      0.00788     0.000245       0.0624       0.0801        0.902       0.0141\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     84     2      0.00589      0.00589      5.4e-06       0.0543       0.0692       0.0996      0.00156\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              84  121.927    0.002      0.00685     0.000331      0.00718       0.0581       0.0746        0.889       0.0139\n",
      "! Validation         84  121.927    0.002      0.00624     6.71e-06      0.00625       0.0563       0.0712        0.119      0.00186\n",
      "Wall time: 121.92718673599938\n",
      "! Best model       84    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     85    10      0.00742      0.00738     4.09e-05       0.0616       0.0774        0.368      0.00575\n",
      "     85    20      0.00887       0.0088     6.61e-05       0.0664       0.0846         0.47      0.00734\n",
      "     85    30      0.00536      0.00535     9.41e-06       0.0528        0.066        0.177      0.00276\n",
      "     85    40      0.00846      0.00808     0.000381       0.0634        0.081         1.13       0.0176\n",
      "     85    50      0.00624      0.00569     0.000549       0.0541        0.068         1.35       0.0211\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     85     2      0.00589      0.00588     4.79e-06       0.0543       0.0691       0.0961       0.0015\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              85  123.179    0.002      0.00657     0.000114      0.00669       0.0569       0.0731        0.483      0.00755\n",
      "! Validation         85  123.179    0.002      0.00623     6.57e-06      0.00624       0.0562       0.0712        0.119      0.00186\n",
      "Wall time: 123.17983192299926\n",
      "! Best model       85    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     86    10      0.00479      0.00479     1.34e-06       0.0495       0.0624       0.0664      0.00104\n",
      "     86    20      0.00683      0.00663       0.0002       0.0593       0.0734        0.815       0.0127\n",
      "     86    30      0.00656      0.00652     3.86e-05       0.0569       0.0728        0.358       0.0056\n",
      "     86    40       0.0084      0.00837     3.27e-05       0.0645       0.0825         0.33      0.00516\n",
      "     86    50      0.00656      0.00656     9.31e-08       0.0565        0.073       0.0176     0.000275\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     86     2      0.00586      0.00586     4.97e-06       0.0542        0.069       0.0967      0.00151\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              86  124.455    0.002      0.00669     0.000232      0.00692       0.0575       0.0737        0.728       0.0114\n",
      "! Validation         86  124.455    0.002      0.00622     6.47e-06      0.00622       0.0562       0.0711        0.117      0.00183\n",
      "Wall time: 124.45560369699979\n",
      "! Best model       86    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     87    10       0.0044      0.00439     6.65e-06       0.0478       0.0598        0.149      0.00233\n",
      "     87    20        0.007      0.00692     7.49e-05       0.0602        0.075          0.5      0.00781\n",
      "     87    30      0.00919      0.00919     5.96e-08       0.0687       0.0864       0.0137     0.000214\n",
      "     87    40      0.00714      0.00713     4.06e-06       0.0594       0.0761        0.116      0.00182\n",
      "     87    50      0.00567      0.00549     0.000182       0.0543       0.0668        0.779       0.0122\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     87     2      0.00586      0.00586     4.72e-06       0.0542        0.069       0.0955      0.00149\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              87  125.741    0.002       0.0066     8.25e-05      0.00668        0.057       0.0732         0.43      0.00672\n",
      "! Validation         87  125.741    0.002      0.00621     6.47e-06      0.00621       0.0561        0.071        0.118      0.00184\n",
      "Wall time: 125.74141569199946\n",
      "! Best model       87    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     88    10      0.00642      0.00642     6.73e-08        0.058       0.0723       0.0146     0.000229\n",
      "     88    20      0.00799      0.00789     0.000101       0.0641       0.0801        0.578      0.00903\n",
      "     88    30      0.00466      0.00446     0.000204       0.0496       0.0602        0.823       0.0129\n",
      "     88    40      0.00609      0.00609     6.81e-06       0.0523       0.0703         0.15      0.00235\n",
      "     88    50      0.00999      0.00999     3.47e-06       0.0687       0.0901        0.107      0.00168\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     88     2      0.00585      0.00585     4.66e-06       0.0542        0.069       0.0947      0.00148\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              88  127.008    0.002      0.00662     7.72e-05       0.0067       0.0571       0.0734        0.401      0.00627\n",
      "! Validation         88  127.008    0.002      0.00619     6.45e-06       0.0062       0.0561       0.0709        0.118      0.00184\n",
      "Wall time: 127.00863164399925\n",
      "! Best model       88    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     89    10      0.00819      0.00796     0.000232       0.0627       0.0804        0.879       0.0137\n",
      "     89    20      0.00614      0.00614     2.38e-07        0.055       0.0706       0.0283     0.000443\n",
      "     89    30      0.00687      0.00681     6.01e-05       0.0579       0.0744        0.447      0.00699\n",
      "     89    40      0.00507      0.00506     1.97e-06       0.0505       0.0642       0.0811      0.00127\n",
      "     89    50      0.00694      0.00685     8.98e-05       0.0597       0.0746        0.547      0.00854\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     89     2      0.00584      0.00583     4.79e-06       0.0541       0.0689       0.0951      0.00149\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              89  128.244    0.002      0.00659     8.97e-05      0.00668       0.0571       0.0732        0.453      0.00707\n",
      "! Validation         89  128.244    0.002      0.00618     6.43e-06      0.00618        0.056       0.0709        0.117      0.00182\n",
      "Wall time: 128.24446281699966\n",
      "! Best model       89    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     90    10      0.00567      0.00566     1.04e-05       0.0556       0.0678        0.186       0.0029\n",
      "     90    20      0.00863      0.00863     1.57e-07       0.0647       0.0838       0.0225     0.000351\n",
      "     90    30      0.00773      0.00765     8.38e-05        0.061       0.0788        0.528      0.00826\n",
      "     90    40      0.00731      0.00728     3.29e-05       0.0606       0.0769        0.331      0.00517\n",
      "     90    50      0.00632      0.00617     0.000159       0.0552       0.0708        0.728       0.0114\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     90     2      0.00582      0.00582     4.89e-06        0.054       0.0688       0.0951      0.00149\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              90  129.513    0.002      0.00669     7.27e-05      0.00676       0.0573       0.0737        0.388      0.00607\n",
      "! Validation         90  129.513    0.002      0.00616     6.45e-06      0.00617       0.0559       0.0708        0.117      0.00182\n",
      "Wall time: 129.5137823339992\n",
      "! Best model       90    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     91    10      0.00482      0.00475     6.69e-05       0.0513       0.0622        0.473      0.00739\n",
      "     91    20      0.00863      0.00862     6.19e-06       0.0657       0.0837        0.144      0.00224\n",
      "     91    30      0.00799      0.00795     4.15e-05       0.0647       0.0804        0.371       0.0058\n",
      "     91    40       0.0065      0.00621     0.000294       0.0561        0.071        0.989       0.0155\n",
      "     91    50      0.00679      0.00668     0.000112        0.059       0.0737        0.612      0.00957\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     91     2      0.00583      0.00582     4.87e-06        0.054       0.0688       0.0955      0.00149\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              91  131.345    0.002      0.00647     0.000105      0.00658       0.0565       0.0725        0.466      0.00727\n",
      "! Validation         91  131.345    0.002      0.00616     6.48e-06      0.00616       0.0559       0.0708        0.117      0.00182\n",
      "Wall time: 131.34516409799926\n",
      "! Best model       91    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     92    10      0.00878      0.00868     0.000103       0.0657        0.084        0.585      0.00914\n",
      "     92    20      0.00876      0.00874      1.5e-05        0.065       0.0843        0.224      0.00349\n",
      "     92    30      0.00537      0.00532     5.68e-05       0.0531       0.0658        0.435      0.00679\n",
      "     92    40      0.00829      0.00815     0.000136       0.0634       0.0814        0.672       0.0105\n",
      "     92    50      0.00562      0.00543     0.000187        0.053       0.0665        0.788       0.0123\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     92     2      0.00582      0.00582     5.04e-06        0.054       0.0688       0.0959       0.0015\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              92  132.604    0.002      0.00658     7.16e-05      0.00665       0.0568       0.0731        0.405      0.00633\n",
      "! Validation         92  132.604    0.002      0.00615      6.5e-06      0.00615       0.0559       0.0707        0.116      0.00182\n",
      "Wall time: 132.6046388649993\n",
      "! Best model       92    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     93    10      0.00624      0.00606     0.000172       0.0551       0.0702        0.757       0.0118\n",
      "     93    20      0.00442      0.00439     3.82e-05       0.0478       0.0597        0.356      0.00557\n",
      "     93    30      0.00555      0.00534     0.000213        0.053       0.0659        0.843       0.0132\n",
      "     93    40      0.00619      0.00616     2.85e-05       0.0568       0.0707        0.309      0.00482\n",
      "     93    50      0.00606      0.00606     9.24e-07       0.0554       0.0702       0.0557      0.00087\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     93     2      0.00581      0.00581     4.85e-06       0.0539       0.0687       0.0951      0.00149\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              93  133.830    0.002      0.00656      0.00013      0.00669       0.0568        0.073        0.552      0.00863\n",
      "! Validation         93  133.830    0.002      0.00613     6.43e-06      0.00614       0.0558       0.0706        0.116      0.00182\n",
      "Wall time: 133.83032001899937\n",
      "! Best model       93    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     94    10       0.0064      0.00637     2.57e-05       0.0554        0.072        0.292      0.00456\n",
      "     94    20      0.00704      0.00704     2.69e-07       0.0617       0.0757       0.0303     0.000473\n",
      "     94    30      0.00585      0.00537     0.000481       0.0532        0.066         1.27       0.0198\n",
      "     94    40      0.00751      0.00693     0.000572       0.0589       0.0751         1.38       0.0216\n",
      "     94    50       0.0063      0.00622     8.35e-05       0.0557       0.0711        0.527      0.00824\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     94     2       0.0058       0.0058     4.74e-06       0.0539       0.0687       0.0939      0.00147\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              94  135.199    0.002      0.00651     0.000152      0.00666       0.0566       0.0727         0.59      0.00921\n",
      "! Validation         94  135.199    0.002      0.00612     6.38e-06      0.00613       0.0558       0.0705        0.115       0.0018\n",
      "Wall time: 135.19960651899964\n",
      "! Best model       94    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     95    10      0.00765      0.00761     4.13e-05       0.0613       0.0787        0.371       0.0058\n",
      "     95    20      0.00616      0.00615     3.47e-06       0.0565       0.0707        0.107      0.00168\n",
      "     95    30      0.00578      0.00578     4.11e-07       0.0553       0.0686       0.0371      0.00058\n",
      "     95    40      0.00628      0.00624     4.71e-05       0.0569       0.0712        0.396       0.0062\n",
      "     95    50      0.00455      0.00454     8.23e-06       0.0495       0.0608        0.165      0.00258\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     95     2       0.0058      0.00579     4.58e-06       0.0539       0.0686       0.0932      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              95  136.720    0.002      0.00655     4.26e-05      0.00659       0.0567        0.073        0.298      0.00465\n",
      "! Validation         95  136.720    0.002      0.00612     6.35e-06      0.00612       0.0557       0.0705        0.115       0.0018\n",
      "Wall time: 136.7208320769996\n",
      "! Best model       95    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     96    10      0.00498      0.00457     0.000408       0.0503       0.0609         1.17       0.0182\n",
      "     96    20      0.00755      0.00754     7.88e-06       0.0606       0.0783        0.162      0.00253\n",
      "     96    30      0.00574      0.00574     2.82e-06       0.0521       0.0683       0.0977      0.00153\n",
      "     96    40       0.0078      0.00766     0.000149       0.0637       0.0789        0.704        0.011\n",
      "     96    50      0.00521      0.00508     0.000137       0.0514       0.0642        0.676       0.0106\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     96     2      0.00579      0.00579      4.6e-06       0.0538       0.0686       0.0938      0.00146\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              96  138.169    0.002      0.00648     0.000129      0.00661       0.0564       0.0726        0.553      0.00864\n",
      "! Validation         96  138.169    0.002      0.00611     6.36e-06      0.00611       0.0557       0.0705        0.116      0.00181\n",
      "Wall time: 138.16966684899944\n",
      "! Best model       96    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     97    10      0.00701      0.00688     0.000129       0.0602       0.0748        0.656       0.0103\n",
      "     97    20      0.00934      0.00932     1.61e-05       0.0679       0.0871        0.231      0.00362\n",
      "     97    30      0.00739      0.00723     0.000159       0.0606       0.0767        0.729       0.0114\n",
      "     97    40      0.00518      0.00516      2.4e-05       0.0514       0.0647        0.282      0.00441\n",
      "     97    50      0.00529        0.005     0.000295       0.0509       0.0638        0.991       0.0155\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     97     2      0.00578      0.00577     4.52e-06       0.0538       0.0685       0.0926      0.00145\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              97  140.180    0.002       0.0065     6.93e-05      0.00657       0.0566       0.0727        0.372      0.00581\n",
      "! Validation         97  140.180    0.002       0.0061     6.28e-06       0.0061       0.0556       0.0704        0.115      0.00179\n",
      "Wall time: 140.1808504769997\n",
      "! Best model       97    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     98    10      0.00724      0.00708     0.000162       0.0599       0.0759        0.733       0.0115\n",
      "     98    20      0.00545      0.00541     3.34e-05       0.0531       0.0663        0.333       0.0052\n",
      "     98    30      0.00608      0.00605     2.82e-05       0.0566       0.0702        0.307      0.00479\n",
      "     98    40      0.00618      0.00617     4.83e-06        0.057       0.0708        0.127      0.00198\n",
      "     98    50      0.00632      0.00631     2.47e-06        0.056       0.0716       0.0908      0.00142\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     98     2      0.00577      0.00577     4.54e-06       0.0538       0.0685        0.093      0.00145\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              98  141.481    0.002       0.0065      6.5e-05      0.00657       0.0565       0.0727        0.398      0.00623\n",
      "! Validation         98  141.481    0.002      0.00609     6.25e-06      0.00609       0.0556       0.0703        0.115      0.00179\n",
      "Wall time: 141.48165519199938\n",
      "! Best model       98    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     99    10      0.00776       0.0077     6.25e-05       0.0627       0.0791        0.456      0.00713\n",
      "     99    20      0.00713      0.00713     2.33e-10       0.0599       0.0761     0.000977     1.53e-05\n",
      "     99    30      0.00659      0.00655     3.89e-05       0.0571        0.073         0.36      0.00563\n",
      "     99    40      0.00543      0.00516     0.000276       0.0518       0.0648        0.958        0.015\n",
      "     99    50      0.00662      0.00621     0.000411       0.0545        0.071         1.17       0.0183\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "     99     2      0.00577      0.00577     4.39e-06       0.0537       0.0685        0.092      0.00144\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train              99  143.854    0.002      0.00641     9.05e-05       0.0065       0.0562       0.0722        0.427      0.00667\n",
      "! Validation         99  143.854    0.002      0.00608     6.23e-06      0.00609       0.0556       0.0703        0.115      0.00179\n",
      "Wall time: 143.85466619499948\n",
      "! Best model       99    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "    100    10       0.0044      0.00395     0.000451       0.0452       0.0567         1.23       0.0191\n",
      "    100    20      0.00768      0.00733     0.000353       0.0626       0.0772         1.08        0.017\n",
      "    100    30      0.00744      0.00708     0.000353       0.0597       0.0759         1.08       0.0169\n",
      "    100    40      0.00894      0.00887     6.99e-05       0.0663       0.0849        0.482      0.00754\n",
      "    100    50      0.00705      0.00705     5.14e-07        0.061       0.0757        0.041     0.000641\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "    100     2      0.00577      0.00577     4.48e-06       0.0537       0.0685       0.0918      0.00143\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train             100  145.691    0.002      0.00651     0.000153      0.00666       0.0566       0.0727        0.605      0.00946\n",
      "! Validation        100  145.691    0.002      0.00607     6.22e-06      0.00608       0.0555       0.0703        0.114      0.00178\n",
      "Wall time: 145.69144666800003\n",
      "! Best model      100    0.006\n",
      "! Stop training: max epochs\n",
      "Wall time: 145.7060018709999\n",
      "Cumulative wall time: 145.7060018709999\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msi\u001b[0m at: \u001b[34mhttps://wandb.ai/chirag17-indian-institute-of-technology/allegro-tutorial/runs/5GPZYqL31nE6JUnpWkvBvrnc-Iix0NLTI8rZmaBIfSY\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241007_012330-5GPZYqL31nE6JUnpWkvBvrnc-Iix0NLTI8rZmaBIfSY/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./results\n",
    "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b27050b-ae56-4fc1-899d-e6622a2c69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-11.8'\n",
      "Using device: cpu\n",
      "Loading dataset... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
      "    loading dataset took 0.0066s\n",
      "    loaded dataset of size 110 and sampled --n-data=2 frames\n",
      "    benchmark frames statistics:\n",
      "         number of atoms: 64\n",
      "         number of types: 1\n",
      "          avg. num edges: 1800.0\n",
      "         avg. neigh/atom: 28.125\n",
      "Building model... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "    building model took 0.1801s\n",
      "    model has 37352 weights\n",
      "    model has 37352 trainable weights\n",
      "    model weights and buffers take 0.14 MB\n",
      "Compile...\n",
      "    compilation took 0.0855s\n",
      "Warmup...\n",
      "    6 calls of warmup took 0.2208s\n",
      "Benchmarking...\n",
      " -- Results --\n",
      "PLEASE NOTE: these are speeds for the MODEL, evaluated on --n-data=2 configurations kept in memory.\n",
      "A variety of factors affect the performance in real molecular dynamics calculations:\n",
      "!!! Molecular dynamics speeds should be measured in LAMMPS; speeds from nequip-benchmark should only be used as an estimate of RELATIVE speed among different hyperparameters.\n",
      "Please further note that relative speed ordering of hyperparameters is NOT NECESSARILY CONSISTENT across different classes of GPUs (i.e. A100 vs V100 vs consumer) or GPUs vs CPUs.\n",
      "\n",
      "The average call took 8.602ms\n"
     ]
    }
   ],
   "source": [
    "!nequip-benchmark allegro/configs/tutorial.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7f5c26-3197-4b13-9b43-53aa096d76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.1+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trainer = torch.load(\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
      "    loaded model\n",
      "Loading original dataset...\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/chirag/anaconda3/envs/nequip_tt/lib/python3.10/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
      "Loaded dataset specified in config.yaml.\n",
      "Using origial training dataset (110 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 50 frames.\n",
      "Starting...\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "f_mae = 0.0461 | f_rmse = 0.0589 | e_mae = 0.0801 | e/N_mae = 0.0013\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.25it/s]\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.40it/s]\n",
      "f_mae = 0.0742 | f_rmse = 0.1016 | e_mae = 0.1099 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0748 | f_rmse = 0.1011 | e_mae = 0.1014 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0704 | f_rmse = 0.0957 | e_mae = 0.1027 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0670 | f_rmse = 0.0912 | e_mae = 0.1009 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0663 | f_rmse = 0.0899 | e_mae = 0.1053 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0660 | f_rmse = 0.0895 | e_mae = 0.1007 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0648 | f_rmse = 0.0878 | e_mae = 0.1104 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0635 | f_rmse = 0.0860 | e_mae = 0.1085 | e/N_mae = 0.0017\u001b[A\n",
      "f_mae = 0.0636 | f_rmse = 0.0858 | e_mae = 0.1165 | e/N_mae = 0.0018\u001b[A\n",
      "f_mae = 0.0627 | f_rmse = 0.0844 | e_mae = 0.1076 | e/N_mae = 0.0017\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 34.34it/s]\n",
      "f_mae = 0.0622 | f_rmse = 0.0830 | e_mae = 0.1015 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0614 | f_rmse = 0.0818 | e_mae = 0.1000 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0615 | f_rmse = 0.0818 | e_mae = 0.0963 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0608 | f_rmse = 0.0809 | e_mae = 0.0992 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0602 | f_rmse = 0.0800 | e_mae = 0.0941 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0791 | e_mae = 0.0928 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0591 | f_rmse = 0.0784 | e_mae = 0.0899 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0590 | f_rmse = 0.0780 | e_mae = 0.0882 | e/N_mae = 0.0014\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:00, 48.67it/s]\n",
      "f_mae = 0.0597 | f_rmse = 0.0790 | e_mae = 0.0947 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0786 | e_mae = 0.0938 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0599 | f_rmse = 0.0788 | e_mae = 0.0988 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0600 | f_rmse = 0.0788 | e_mae = 0.0975 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0784 | e_mae = 0.1017 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0598 | f_rmse = 0.0784 | e_mae = 0.1003 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0783 | e_mae = 0.1019 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0598 | f_rmse = 0.0783 | e_mae = 0.1035 | e/N_mae = 0.0016\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 59.15it/s]\n",
      "f_mae = 0.0597 | f_rmse = 0.0779 | e_mae = 0.1030 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0599 | f_rmse = 0.0780 | e_mae = 0.1006 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0598 | f_rmse = 0.0779 | e_mae = 0.0988 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0777 | e_mae = 0.1000 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0778 | e_mae = 0.0996 | e/N_mae = 0.0016\u001b[A\n",
      "f_mae = 0.0596 | f_rmse = 0.0776 | e_mae = 0.0978 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0597 | f_rmse = 0.0776 | e_mae = 0.0954 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0595 | f_rmse = 0.0773 | e_mae = 0.0937 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0593 | f_rmse = 0.0771 | e_mae = 0.0932 | e/N_mae = 0.0015\u001b[A\n",
      "f_mae = 0.0592 | f_rmse = 0.0769 | e_mae = 0.0912 | e/N_mae = 0.0014\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:00<00:00, 72.14it/s]\n",
      "f_mae = 0.0589 | f_rmse = 0.0764 | e_mae = 0.0884 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0588 | f_rmse = 0.0762 | e_mae = 0.0881 | e/N_mae = 0.0014\u001b[A\n",
      "f_mae = 0.0588 | f_rmse = 0.0761 | e_mae = 0.0861 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0587 | f_rmse = 0.0760 | e_mae = 0.0852 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0587 | f_rmse = 0.0759 | e_mae = 0.0835 | e/N_mae = 0.0013\u001b[A\n",
      "f_mae = 0.0586 | f_rmse = 0.0757 | e_mae = 0.0835 | e/N_mae = 0.0013\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [00:00<00:00, 51.50it/s]\n",
      "f_mae = 0.0586 | f_rmse = 0.0757 | e_mae = 0.0844 | e/N_mae = 0.0013\n",
      "\n",
      "--- Final result: ---\n",
      "               f_mae =  0.058591           \n",
      "              f_rmse =  0.075679           \n",
      "               e_mae =  0.084434           \n",
      "             e/N_mae =  0.001319           \n",
      "               f_mae =  0.058591           \n",
      "              f_rmse =  0.075679           \n",
      "               e_mae =  0.084434           \n",
      "             e/N_mae =  0.001319           \n"
     ]
    }
   ],
   "source": [
    "!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fb977-a62a-4fd7-8043-067c7667d13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nequip_tt",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
